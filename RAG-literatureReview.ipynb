{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYlHfVh9LIDeZFha3AbVZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/RAG-literatureReview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI in Research: using Large Language Models (LLMs) to enhance and streamline the academic literature review process.\n",
        "  \n",
        "    Techniques for summarizing articles, identifying connections across papers (authors, references, methods), and uncovering key themes.\n",
        "    Constructing a knowledge graph to visualize and interpret findings."
      ],
      "metadata": {
        "id": "vq3zIOCofySQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "# Document processing\n",
        "import fitz  # PyMuPDF\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector database\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.embeddings import Embeddings\n",
        "\n",
        "# LLM\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import create_extraction_chain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Knowledge graph\n",
        "import pyvis\n",
        "from pyvis.network import Network\n"
      ],
      "metadata": {
        "id": "SwEF3FuigcZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Document Processing & Chunking\n",
        "The system first loads PDF documents using PyMuPDF (the fitz library) and extracts their text content. This is crucial because PDFs often contain formatting that needs to be properly parsed.\n",
        "Each document is then split into smaller chunks (default 1000 characters with 200 character overlap) using LangChain's RecursiveCharacterTextSplitter. This chunking is essential because:\n",
        "\n",
        "Most language models have context window limitations (typically 8K-128K tokens)\n",
        "Smaller chunks allow for more precise retrieval\n",
        "Overlapping chunks preserve context across chunk boundaries"
      ],
      "metadata": {
        "id": "E9Kwlftug5Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Vector Store Creation & Embedding\n",
        "The system uses SentenceTransformer to create embeddings for each document chunk. These embeddings capture the semantic meaning of each chunk as dense vectors.\n",
        "The embeddings are stored in a FAISS vector database, which enables efficient similarity search. When you ask a question, the system can quickly retrieve the most relevant chunks by comparing your query's embedding with the stored document embeddings."
      ],
      "metadata": {
        "id": "h-Sata6Ig_hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Metadata Extraction & Knowledge Graph\n",
        "The system uses an LLM to extract structured metadata from each paper, including:\n",
        "\n",
        "Title, authors, publication year\n",
        "Keywords and methodologies\n",
        "Abstract\n",
        "Cited papers\n",
        "\n",
        "This metadata forms the basis of a knowledge graph built with NetworkX, where:\n",
        "\n",
        "Papers, authors, journals, methodologies, and keywords are nodes\n",
        "Relationships (authored, cites, uses) are edges\n",
        "\n",
        "This graph representation allows visualization of relationships between papers and helps identify key authors, methodologies, and research themes across multiple papers."
      ],
      "metadata": {
        "id": "jhA6cB-LhA9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Analysis Capabilities\n",
        "\n",
        "Paper Summarization\n",
        "For each paper, the system generates a comprehensive summary covering:\n",
        "\n",
        "Main research questions\n",
        "Methodology\n",
        "Key findings\n",
        "Limitations and future work\n",
        "\n",
        "This helps quickly understand individual papers without reading the full text.\n",
        "\n",
        "\n",
        "Paper Comparison\n",
        "The system can compare multiple papers to identify:\n",
        "\n",
        "Common themes\n",
        "Differences in methodology\n",
        "Complementary or contradictory findings\n",
        "Research gaps\n",
        "\n",
        "\n",
        "Theme Extraction\n",
        "The system analyzes all papers to identify common themes, showing which papers address each theme and how they contribute to it.\n",
        "\n",
        "\n",
        "Question Answering\n",
        "When you ask a question:\n",
        "\n",
        "The system retrieves the most relevant chunks from across all papers\n",
        "It provides the chunks as context to the LLM\n",
        "The LLM synthesizes an answer based on this context, citing the relevant papers\n",
        "\n"
      ],
      "metadata": {
        "id": "rmkO0O2IhEJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Overcoming Document Length Limitations\n",
        "The system addresses document length challenges through:\n",
        "\n",
        "Chunking documents intelligently (with overlap)\n",
        "Using metadata extraction to capture key information regardless of document length\n",
        "Building a knowledge graph to represent relationships that might span across documents\n",
        "Using the LLM to synthesize information from multiple chunks"
      ],
      "metadata": {
        "id": "Fk75Fg3RhaZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies: The system requires several Python libraries:\n",
        "\n",
        "PyMuPDF for PDF processing\n",
        "LangChain for document processing and LLM chains\n",
        "SentenceTransformer for embeddings\n",
        "FAISS for vector storage\n",
        "NetworkX and PyVis for knowledge graph creation and visualization\n",
        "OpenAI for the LLM"
      ],
      "metadata": {
        "id": "EikMfiVWhdvl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OdUrxM1mgcpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQ1smJHWgcsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6zUaAwjfvNy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AcademicRAG:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-4o\"):\n",
        "        \"\"\"\n",
        "        Initialize the Academic RAG system.\n",
        "\n",
        "        Args:\n",
        "            api_key: OpenAI API key\n",
        "            model_name: LLM model to use\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=model_name,\n",
        "            openai_api_key=api_key,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Initialize document storage\n",
        "        self.documents = {}\n",
        "        self.chunks = {}\n",
        "        self.metadata = {}\n",
        "        self.knowledge_graph = nx.DiGraph()\n",
        "\n",
        "    def load_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to PDF file\n",
        "\n",
        "        Returns:\n",
        "            Full text content of the PDF\n",
        "        \"\"\"\n",
        "        doc_id = Path(file_path).stem\n",
        "        pdf_document = fitz.open(file_path)\n",
        "        text = \"\"\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "\n",
        "        # Store the full document\n",
        "        self.documents[doc_id] = text\n",
        "\n",
        "        print(f\"Loaded document: {doc_id} ({len(text)} characters)\")\n",
        "        return text\n",
        "\n",
        "    def load_multiple_pdfs(self, directory: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Load multiple PDFs from a directory.\n",
        "\n",
        "        Args:\n",
        "            directory: Directory containing PDF files\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to full text content\n",
        "        \"\"\"\n",
        "        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            file_path = os.path.join(directory, pdf_file)\n",
        "            self.load_pdf(file_path)\n",
        "\n",
        "        return self.documents\n",
        "\n",
        "    def chunk_document(self, doc_id: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Split document into chunks.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            List of document chunks with metadata\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found. Load it first.\")\n",
        "\n",
        "        text = self.documents[doc_id]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.create_documents([text])\n",
        "\n",
        "        # Add metadata to chunks\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata = {\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"source\": doc_id\n",
        "            }\n",
        "\n",
        "        self.chunks[doc_id] = chunks\n",
        "        print(f\"Split {doc_id} into {len(chunks)} chunks\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_all_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict[str, List[Document]]:\n",
        "        \"\"\"\n",
        "        Split all loaded documents into chunks.\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to lists of chunks\n",
        "        \"\"\"\n",
        "        for doc_id in self.documents:\n",
        "            self.chunk_document(doc_id, chunk_size, chunk_overlap)\n",
        "\n",
        "        return self.chunks\n",
        "\n",
        "    def build_vector_store(self):\n",
        "        \"\"\"\n",
        "        Build vector store from all document chunks.\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        for doc_chunks in self.chunks.values():\n",
        "            all_chunks.extend(doc_chunks)\n",
        "\n",
        "        # Create embedding class that adapts SentenceTransformer to LangChain\n",
        "        class STEmbeddings(Embeddings):\n",
        "            def __init__(self, model):\n",
        "                self.model = model\n",
        "\n",
        "            def embed_documents(self, texts):\n",
        "                return self.model.encode(texts).tolist()\n",
        "\n",
        "            def embed_query(self, text):\n",
        "                return self.model.encode(text).tolist()\n",
        "\n",
        "        embeddings = STEmbeddings(self.embedding_model)\n",
        "\n",
        "        # Create FAISS vector store\n",
        "        self.vector_store = FAISS.from_documents(all_chunks, embeddings)\n",
        "        print(f\"Built vector store with {len(all_chunks)} chunks\")\n",
        "\n",
        "    def extract_metadata(self):\n",
        "        \"\"\"\n",
        "        Extract metadata from papers using LLM.\n",
        "        \"\"\"\n",
        "        schema = {\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"string\"},\n",
        "                \"authors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"publication_year\": {\"type\": \"integer\"},\n",
        "                \"journal\": {\"type\": \"string\"},\n",
        "                \"abstract\": {\"type\": \"string\"},\n",
        "                \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"methodology\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"cited_papers\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "            },\n",
        "            \"required\": [\"title\", \"authors\"],\n",
        "        }\n",
        "\n",
        "        extraction_chain = create_extraction_chain(schema, self.llm)\n",
        "\n",
        "        for doc_id, text in self.documents.items():\n",
        "            # Use the first chunk plus any detected abstract section for metadata extraction\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", text, re.DOTALL)\n",
        "            abstract_text = abstract_match.group(1) if abstract_match else \"\"\n",
        "\n",
        "            input_text = first_chunk + \"\\n\\n\" + abstract_text\n",
        "\n",
        "            # Truncate if too long\n",
        "            if len(input_text) > 5000:\n",
        "                input_text = input_text[:5000]\n",
        "\n",
        "            try:\n",
        "                result = extraction_chain.invoke({\"input\": input_text})\n",
        "                metadata = result[\"extracted\"][0] if result[\"extracted\"] else {}\n",
        "                self.metadata[doc_id] = metadata\n",
        "                print(f\"Extracted metadata for {doc_id}: {metadata['title'] if 'title' in metadata else 'Unknown'}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting metadata for {doc_id}: {e}\")\n",
        "                self.metadata[doc_id] = {\"title\": doc_id, \"authors\": [\"Unknown\"]}\n",
        "\n",
        "    def build_knowledge_graph(self):\n",
        "        \"\"\"\n",
        "        Build knowledge graph from extracted paper metadata.\n",
        "        \"\"\"\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add papers as nodes\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = metadata.get(\"authors\", [\"Unknown\"])\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "            journal = metadata.get(\"journal\", \"Unknown\")\n",
        "\n",
        "            # Add paper node\n",
        "            G.add_node(title,\n",
        "                       type=\"paper\",\n",
        "                       authors=authors,\n",
        "                       year=year,\n",
        "                       journal=journal,\n",
        "                       doc_id=doc_id)\n",
        "\n",
        "            # Add author nodes and connections\n",
        "            for author in authors:\n",
        "                if not G.has_node(author):\n",
        "                    G.add_node(author, type=\"author\")\n",
        "                G.add_edge(author, title, type=\"authored\")\n",
        "\n",
        "            # Add journal node and connection\n",
        "            if journal != \"Unknown\":\n",
        "                if not G.has_node(journal):\n",
        "                    G.add_node(journal, type=\"journal\")\n",
        "                G.add_edge(title, journal, type=\"published_in\")\n",
        "\n",
        "            # Add methodology nodes\n",
        "            methods = metadata.get(\"methodology\", [])\n",
        "            for method in methods:\n",
        "                if not G.has_node(method):\n",
        "                    G.add_node(method, type=\"methodology\")\n",
        "                G.add_edge(title, method, type=\"uses\")\n",
        "\n",
        "            # Add keyword nodes\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "            for keyword in keywords:\n",
        "                if not G.has_node(keyword):\n",
        "                    G.add_node(keyword, type=\"keyword\")\n",
        "                G.add_edge(title, keyword, type=\"contains\")\n",
        "\n",
        "        # Add citation links\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            source_title = metadata.get(\"title\", doc_id)\n",
        "            cited_papers = metadata.get(\"cited_papers\", [])\n",
        "\n",
        "            for cited_paper in cited_papers:\n",
        "                # Try to match with existing paper nodes\n",
        "                matching_papers = [node for node in G.nodes() if G.nodes[node].get(\"type\") == \"paper\" and cited_paper.lower() in node.lower()]\n",
        "\n",
        "                if matching_papers:\n",
        "                    G.add_edge(source_title, matching_papers[0], type=\"cites\")\n",
        "                else:\n",
        "                    # Add as a new node if not found\n",
        "                    G.add_node(cited_paper, type=\"external_paper\")\n",
        "                    G.add_edge(source_title, cited_paper, type=\"cites\")\n",
        "\n",
        "        self.knowledge_graph = G\n",
        "        print(f\"Built knowledge graph with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "\n",
        "        return G\n",
        "\n",
        "    def visualize_knowledge_graph(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        \"\"\"\n",
        "        Visualize the knowledge graph.\n",
        "\n",
        "        Args:\n",
        "            output_file: Path to save the HTML visualization\n",
        "        \"\"\"\n",
        "        G = self.knowledge_graph\n",
        "\n",
        "        # Create pyvis network\n",
        "        net = Network(height=\"750px\", width=\"100%\", notebook=False, directed=True)\n",
        "\n",
        "        # Node colors by type\n",
        "        color_map = {\n",
        "            \"paper\": \"#4285F4\",  # blue\n",
        "            \"author\": \"#EA4335\",  # red\n",
        "            \"journal\": \"#FBBC05\",  # yellow\n",
        "            \"methodology\": \"#34A853\",  # green\n",
        "            \"keyword\": \"#800080\",  # purple\n",
        "            \"external_paper\": \"#A0A0A0\"  # gray\n",
        "        }\n",
        "\n",
        "        # Add nodes\n",
        "        for node in G.nodes():\n",
        "            node_type = G.nodes[node].get(\"type\", \"unknown\")\n",
        "            title = f\"Type: {node_type}\"\n",
        "\n",
        "            if node_type == \"paper\":\n",
        "                authors = \", \".join(G.nodes[node].get(\"authors\", [\"Unknown\"]))\n",
        "                year = G.nodes[node].get(\"year\", \"Unknown\")\n",
        "                journal = G.nodes[node].get(\"journal\", \"Unknown\")\n",
        "                title = f\"Paper: {node}\\nAuthors: {authors}\\nYear: {year}\\nJournal: {journal}\"\n",
        "\n",
        "            net.add_node(node,\n",
        "                         title=title,\n",
        "                         color=color_map.get(node_type, \"#000000\"),\n",
        "                         size=20 if node_type == \"paper\" else 10)\n",
        "\n",
        "        # Add edges\n",
        "        edge_colors = {\n",
        "            \"authored\": \"#EA4335\",\n",
        "            \"published_in\": \"#FBBC05\",\n",
        "            \"uses\": \"#34A853\",\n",
        "            \"contains\": \"#800080\",\n",
        "            \"cites\": \"#4285F4\"\n",
        "        }\n",
        "\n",
        "        for source, target, data in G.edges(data=True):\n",
        "            edge_type = data.get(\"type\", \"unknown\")\n",
        "            net.add_edge(source, target,\n",
        "                         title=edge_type,\n",
        "                         color=edge_colors.get(edge_type, \"#000000\"),\n",
        "                         arrows=\"to\")\n",
        "\n",
        "        # Set physics layout\n",
        "        net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08)\n",
        "        net.toggle_physics(True)\n",
        "\n",
        "        # Save visualization\n",
        "        net.show(output_file)\n",
        "        print(f\"Knowledge graph visualization saved to {output_file}\")\n",
        "\n",
        "    def retrieve_similar_chunks(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve similar chunks for a query.\n",
        "\n",
        "        Args:\n",
        "            query: Query text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            List of (document, similarity) tuples\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not built. Call build_vector_store() first.\")\n",
        "\n",
        "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_paper_summary(self, doc_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a summary for a specific paper.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "\n",
        "        Returns:\n",
        "            Summary text\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Summarize the following academic paper excerpt:\n",
        "\n",
        "            {text}\n",
        "\n",
        "            Provide a comprehensive summary that includes:\n",
        "            1. The main research question/problem\n",
        "            2. The methodology used\n",
        "            3. Key findings and contributions\n",
        "            4. Limitations and future work\n",
        "\n",
        "            Summary:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Use the first few chunks (up to 10,000 characters)\n",
        "        chunks = self.chunks.get(doc_id, [])\n",
        "        combined_text = \"\"\n",
        "        for chunk in chunks:\n",
        "            combined_text += chunk.page_content + \"\\n\\n\"\n",
        "            if len(combined_text) > 10000:\n",
        "                break\n",
        "        combined_text = combined_text[:10000]\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        summary = chain.invoke({\"text\": combined_text})\n",
        "\n",
        "        return summary[\"text\"]\n",
        "\n",
        "    def compare_papers(self, doc_ids: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Compare multiple papers.\n",
        "\n",
        "        Args:\n",
        "            doc_ids: List of document identifiers\n",
        "\n",
        "        Returns:\n",
        "            Comparison text\n",
        "        \"\"\"\n",
        "        # Validate all doc_ids\n",
        "        for doc_id in doc_ids:\n",
        "            if doc_id not in self.documents:\n",
        "                raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        # Get metadata for each paper\n",
        "        paper_info = []\n",
        "        for doc_id in doc_ids:\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "\n",
        "            # Get a brief summary by using first chunk\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            paper_info.append(f\"Title: {title}\\nAuthors: {authors}\\nYear: {year}\\nExcerpt: {first_chunk[:500]}...\")\n",
        "\n",
        "        # Generate comparison using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Compare and contrast the following academic papers:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Provide a detailed comparison including:\n",
        "            1. Common themes and research questions\n",
        "            2. Differences in methodology\n",
        "            3. Complementary or contradictory findings\n",
        "            4. How they build upon each other's work\n",
        "            5. Potential gaps or areas for future research\n",
        "\n",
        "            Comparison:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        comparison = chain.invoke({\"papers\": \"\\n\\n\".join(paper_info)})\n",
        "\n",
        "        return comparison[\"text\"]\n",
        "\n",
        "    def answer_question(self, question: str, k: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Answer a question based on the papers.\n",
        "\n",
        "        Args:\n",
        "            question: Question text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Answer text\n",
        "        \"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = self.retrieve_similar_chunks(question, k=k)\n",
        "\n",
        "        # Prepare context from chunks\n",
        "        context = []\n",
        "        for chunk, score in relevant_chunks:\n",
        "            doc_id = chunk.metadata[\"doc_id\"]\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            context.append(f\"From '{title}' by {authors}:\\n{chunk.page_content}\")\n",
        "\n",
        "        # Generate answer using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"question\", \"context\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant with expertise in synthesizing information from academic papers.\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Here are relevant excerpts from academic papers:\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Please provide a comprehensive answer to the question based on the given context.\n",
        "            Cite the papers you're referencing in your response.\n",
        "            If the information provided is insufficient to answer the question, clearly state what's missing.\n",
        "\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        answer = chain.invoke({\"question\": question, \"context\": \"\\n\\n\".join(context)})\n",
        "\n",
        "        return answer[\"text\"]\n",
        "\n",
        "    def extract_themes(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract common themes across papers.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of themes with supporting evidence\n",
        "        \"\"\"\n",
        "        # Prepare paper information\n",
        "        papers_info = []\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            abstract = metadata.get(\"abstract\", \"\")\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "\n",
        "            if not abstract:\n",
        "                # Try to find abstract in the document\n",
        "                doc_text = self.documents[doc_id]\n",
        "                abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", doc_text, re.DOTALL)\n",
        "                abstract = abstract_match.group(1) if abstract_match else \"No abstract found\"\n",
        "\n",
        "            papers_info.append(f\"Title: {title}\\nAbstract: {abstract}\\nKeywords: {', '.join(keywords)}\")\n",
        "\n",
        "        # Generate themes using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Analyze the following academic papers and extract common themes:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Identify 3-5 major themes across these papers. For each theme:\n",
        "            1. Provide a clear name and description\n",
        "            2. List which papers address this theme\n",
        "            3. Describe how each paper contributes to or approaches this theme\n",
        "            4. Note any contradictions or complementary findings within the theme\n",
        "\n",
        "            Format your response as a JSON structure where each theme is a key, and the value contains the description and paper relationships.\n",
        "\n",
        "            Themes:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        themes_result = chain.invoke({\"papers\": \"\\n\\n\".join(papers_info)})\n",
        "\n",
        "        # Extract themes from the text response\n",
        "        themes_text = themes_result[\"text\"]\n",
        "\n",
        "        # Use LLM to convert to structured format\n",
        "        extraction_prompt = PromptTemplate(\n",
        "            input_variables=[\"themes_text\"],\n",
        "            template=\"\"\"\n",
        "            Convert the following themes analysis into a proper JSON structure:\n",
        "\n",
        "            {themes_text}\n",
        "\n",
        "            JSON format:\n",
        "            ```\n",
        "            {{\n",
        "                \"Theme 1 Name\": {{\n",
        "                    \"description\": \"Description of theme 1\",\n",
        "                    \"papers\": [\n",
        "                        {{\n",
        "                            \"title\": \"Paper Title 1\",\n",
        "                            \"contribution\": \"How Paper 1 contributes to this theme\"\n",
        "                        }},\n",
        "                        ...\n",
        "                    ]\n",
        "                }},\n",
        "                ...\n",
        "            }}\n",
        "            ```\n",
        "\n",
        "            Only respond with the valid JSON, nothing else.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=extraction_prompt)\n",
        "        json_result = chain.invoke({\"themes_text\": themes_text})\n",
        "\n",
        "        try:\n",
        "            # Extract JSON from the response (it might be wrapped in backticks)\n",
        "            json_text = re.search(r'```json\\n(.*?)\\n```', json_result[\"text\"], re.DOTALL)\n",
        "            if json_text:\n",
        "                json_str = json_text.group(1)\n",
        "            else:\n",
        "                json_str = json_result[\"text\"]\n",
        "\n",
        "            # Clean up any extra backticks\n",
        "            json_str = json_str.replace('```', '').strip()\n",
        "\n",
        "            import json\n",
        "            themes = json.loads(json_str)\n",
        "            return themes\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing themes JSON: {e}\")\n",
        "            return {\"error\": \"Could not parse themes\", \"raw_text\": themes_text}\n",
        "\n",
        "    def run_full_analysis(self, pdf_directory: str, output_dir: str = \"./output\"):\n",
        "        \"\"\"\n",
        "        Run full analysis pipeline on a directory of PDFs.\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_dir: Directory to save output files\n",
        "        \"\"\"\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Load PDFs\n",
        "        print(\"Loading PDFs...\")\n",
        "        self.load_multiple_pdfs(pdf_directory)\n",
        "\n",
        "        # 2. Chunk documents\n",
        "        print(\"Chunking documents...\")\n",
        "        self.chunk_all_documents()\n",
        "\n",
        "        # 3. Build vector store\n",
        "        print(\"Building vector store...\")\n",
        "        self.build_vector_store()\n",
        "\n",
        "        # 4. Extract metadata\n",
        "        print(\"Extracting metadata...\")\n",
        "        self.extract_metadata()\n",
        "\n",
        "        # 5. Build knowledge graph\n",
        "        print(\"Building knowledge graph...\")\n",
        "        self.build_knowledge_graph()\n",
        "\n",
        "        # 6. Visualize knowledge graph\n",
        "        print(\"Visualizing knowledge graph...\")\n",
        "        self.visualize_knowledge_graph(os.path.join(output_dir, \"knowledge_graph.html\"))\n",
        "\n",
        "        # 7. Generate summaries for each paper\n",
        "        print(\"Generating summaries...\")\n",
        "        summaries = {}\n",
        "        for doc_id in self.documents:\n",
        "            summaries[doc_id] = self.generate_paper_summary(doc_id)\n",
        "\n",
        "            # Save individual summaries\n",
        "            with open(os.path.join(output_dir, f\"{doc_id}_summary.txt\"), \"w\") as f:\n",
        "                f.write(summaries[doc_id])\n",
        "\n",
        "        # 8. Compare all papers\n",
        "        print(\"Comparing papers...\")\n",
        "        comparison = self.compare_papers(list(self.documents.keys()))\n",
        "        with open(os.path.join(output_dir, \"paper_comparison.txt\"), \"w\") as f:\n",
        "            f.write(comparison)\n",
        "\n",
        "        # 9. Extract themes\n",
        "        print(\"Extracting themes...\")\n",
        "        themes = self.extract_themes()\n",
        "\n",
        "        import json\n",
        "        with open(os.path.join(output_dir, \"themes.json\"), \"w\") as f:\n",
        "            json.dump(themes, f, indent=2)\n",
        "\n",
        "        print(f\"Analysis complete! Results saved in {output_dir}\")\n",
        "\n",
        "        return {\n",
        "            \"summaries\": summaries,\n",
        "            \"comparison\": comparison,\n",
        "            \"themes\": themes\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your OpenAI API key\n",
        "    OPENAI_API_KEY = \"your-api-key-here\"\n",
        "\n",
        "    # Initialize the RAG system\n",
        "    rag = AcademicRAG(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # Run full analysis\n",
        "    results = rag.run_full_analysis(\"./papers\")\n",
        "\n",
        "    # Ask questions\n",
        "    question = \"What are the main methodologies used across these papers and how do they compare?\"\n",
        "    answer = rag.answer_question(question)\n",
        "    print(f\"Q: {question}\\nA: {answer}\")\n",
        "\n",
        "    # Get more paper relationships\n",
        "    print(\"Paper relationships:\")\n",
        "    for source, target, data in rag.knowledge_graph.edges(data=True):\n",
        "        if data.get(\"type\") == \"cites\":\n",
        "            print(f\"  {source} cites {target}\")"
      ],
      "metadata": {
        "id": "rGFG6eGPgmQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}