{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNw8XYg39wY5lf2bp5ShRB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70c9e53a42334d7fba2f30741b7c75be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de340c0db90c4301a49b0c1dcce1b648",
              "IPY_MODEL_c363c95105644794842e606de9fd2993",
              "IPY_MODEL_f55599e3b5d5443f86697333ac9cc1b9"
            ],
            "layout": "IPY_MODEL_420e71a8cb0a4076b994984b20077cc4"
          }
        },
        "de340c0db90c4301a49b0c1dcce1b648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a9dcbc054b348ff8d5ea10f8d393f0a",
            "placeholder": "​",
            "style": "IPY_MODEL_6c5fe19fccb04666b3bb7a029ec1933e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c363c95105644794842e606de9fd2993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839742f43e394854894da2e80afe41f2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f821d053132643c8a0e509cf198d78c2",
            "value": 2
          }
        },
        "f55599e3b5d5443f86697333ac9cc1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9263351a9940c68c55b28244b6c7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_ac408326a72c41eea1909b36eb3247b3",
            "value": " 2/2 [00:00&lt;00:00,  4.43it/s]"
          }
        },
        "420e71a8cb0a4076b994984b20077cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a9dcbc054b348ff8d5ea10f8d393f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5fe19fccb04666b3bb7a029ec1933e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "839742f43e394854894da2e80afe41f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f821d053132643c8a0e509cf198d78c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd9263351a9940c68c55b28244b6c7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac408326a72c41eea1909b36eb3247b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/RAG-literatureReview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI in Research: using Large Language Models (LLMs) to enhance and streamline the academic literature review process."
      ],
      "metadata": {
        "id": "vq3zIOCofySQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "leverage RAG Techniques for summarizing papers, identifying connections across papers (authors, references, methods), uncovering key themes in them.\n",
        "\n",
        "Download 2 papers (related to diffusion model) and convert them to .txt files in a directory named \"data\". Use these .txt files as input papers and evaluate if the RAG technique is giving good results."
      ],
      "metadata": {
        "id": "S_-IHuLTAh-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf requests"
      ],
      "metadata": {
        "id": "OUPE-P03hm6L",
        "outputId": "1ff19b47-6063-4fc1-e711-ce9f4f145c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download any 2 papers of diffusion model and convert their pdf to .txt files"
      ],
      "metadata": {
        "id": "MbRT1J6xiGMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r data"
      ],
      "metadata": {
        "id": "2eWwSdyRMPPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c9c988-a01a-43cd-e77b-af4b5c96d178"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Sample arXiv paper IDs related to diffusion models\n",
        "arxiv_ids = [\n",
        "    \"2006.11239\",  # Denoising Diffusion Probabilistic Models\n",
        "    \"2105.05233\",  # Improved Denoising Diffusion Probabilistic Models\n",
        "]\n",
        "\n",
        "def download_pdf(arxiv_id, output_folder):\n",
        "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "    pdf_path = os.path.join(output_folder, f\"{arxiv_id}.pdf\")\n",
        "    response = requests.get(url)\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {arxiv_id}\")\n",
        "    return pdf_path\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"Converted to text: {txt_path}\")\n",
        "\n",
        "def main():\n",
        "    data_dir = \"data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for arxiv_id in arxiv_ids:\n",
        "        pdf_path = download_pdf(arxiv_id, data_dir)\n",
        "        txt_path = os.path.join(data_dir, f\"{arxiv_id}.txt\")\n",
        "        pdf_to_text(pdf_path, txt_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Yq0vtxjXhZcm",
        "outputId": "e3b89d10-06a4-405c-924a-11d132bd710b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 2006.11239\n",
            "Converted to text: data/2006.11239.txt\n",
            "Downloaded 2105.05233\n",
            "Converted to text: data/2105.05233.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm data/*.pdf"
      ],
      "metadata": {
        "id": "mieCFJzFAOuw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain_community faiss-cpu sentence-transformers transformers networkx matplotlib spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "4f4wA_teK4xy",
        "outputId": "f46d9e29-2fa8-46bb-cd20-2b00a137c6d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
            "  Downloading langchain_core-0.3.55-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.55-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, langchain-core, langchain, langchain_community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.52\n",
            "    Uninstalling langchain-core-0.3.52:\n",
            "      Successfully uninstalled langchain-core-0.3.52\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.23\n",
            "    Uninstalling langchain-0.3.23:\n",
            "      Successfully uninstalled langchain-0.3.23\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.10.0 httpx-sse-0.4.0 langchain-0.3.24 langchain-core-0.3.55 langchain_community-0.3.22 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import gc\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import re\n",
        "gc.collect()\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def clean_text(text):\n",
        "    import re\n",
        "    # Remove inline citations like [14], [14, 27]\n",
        "    text = re.sub(r\"\\[[0-9,\\s]+\\]\", \"\", text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
        "    # Remove LaTeX math expressions\n",
        "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
        "    # Remove repeated words\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "    # Remove special characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,;:?!\\s]\", \"\", text)\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# Load a document and return its content\n",
        "def load_document(file_path):\n",
        "    loader = TextLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    for doc in docs:\n",
        "        doc.page_content = clean_text(doc.page_content)  # Apply cleaning here\n",
        "    return docs\n",
        "\n",
        "# Split the document into chunks ensuring each chunk is under the token limit\n",
        "def split_document(docs, chunk_size=1500, chunk_overlap=100):\n",
        "    # Make sure docs is always a list\n",
        "    if not isinstance(docs, list):\n",
        "        docs = [docs]\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "# Vector store (use sentence embeddings)\n",
        "def create_faiss_index(docs):\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    return FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Load HuggingFace LLM\n",
        "def load_llm():\n",
        "    model_id = \"google/flan-t5-xl\"  # Better GPU utilization, faster than flan-t5-large\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\")  # Move model to GPU\n",
        "    #pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0, max_new_tokens=1024)\n",
        "\n",
        "    return HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "# Build RAG chain\n",
        "def build_qa_chain(llm, vectorstore):\n",
        "    return RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever(), chain_type=\"stuff\")\n",
        "\n",
        "# Analyze a single document in chunks and store results with a chunk limit\n",
        "def analyze_document(llm, docs, batch_size=1, max_chunks=10):\n",
        "    results = {\"Summary\": [], \"Connections\": [], \"Themes\": []}\n",
        "    chunks = split_document(docs)\n",
        "    chunks = chunks[:max_chunks]\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i + batch_size]\n",
        "\n",
        "        for label, q in [\n",
        "            (\"Summary\", \"Summarize the following scientific paper text in concise bullet points. Include: Main contribution, Dataset used, method, Evaluation metrics and Key results.\"),\n",
        "            (\"Connections\", \"You are reading several research papers. Based on the passage below, what connections or similarities can you identify with other papers on diffusion models? Mention common techniques, models, datasets, or evaluation strategies.\"),\n",
        "            (\"Themes\", \"What are the central *research themes* in the following paper? List them as concise topics.\")\n",
        "        ]:\n",
        "            prompts = [f\"{q}\\n\\n---\\n{chunk.page_content.strip()}\" for chunk in batch]\n",
        "            try:\n",
        "                responses = llm.pipeline(prompts)\n",
        "                for response in responses:\n",
        "                    text = response['generated_text'].strip()\n",
        "                    results[label].append(text)\n",
        "                    print(f\"\\n🔍 {label}:\\n{text}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during {label} batch: {e}\")\n",
        "                results[label].extend([\"Error\"] * len(batch))\n",
        "\n",
        "        del batch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Main pipeline\n",
        "def process_all_documents(data_dir=\"data\", max_chunks=10):\n",
        "    files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
        "    results = {\"Summary\": [], \"Connections\": [], \"Themes\": []}\n",
        "\n",
        "    llm = load_llm()\n",
        "\n",
        "    for file_path in files:\n",
        "        print(f\"\\n📄 Processing: {file_path}\")\n",
        "        # Load and clean the document\n",
        "        doc = load_document(file_path)\n",
        "\n",
        "        # Process the document in chunks\n",
        "        doc_results = analyze_document(llm, docs=doc, max_chunks=max_chunks)\n",
        "\n",
        "        # Collect results\n",
        "        for label in results:\n",
        "            results[label].extend(doc_results.get(label, []))\n",
        "\n",
        "        # Free up GPU memory after processing each document\n",
        "        del doc\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()  # Run garbage collection to free memory\n",
        "\n",
        "    # Clean-up results (e.g., remove empty strings or redundant entries)\n",
        "    for label in results:\n",
        "        flat = [str(item).strip() for sublist in results[label] for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "        cleaned = [s for s in flat if s and s.lower() != \"error\"]\n",
        "        combined_text = \"\\n\".join(cleaned)\n",
        "\n",
        "        if label == \"Themes\":\n",
        "          final_theme = summarize_combined_output(llm, combined_text, label) # Remove duplicates\n",
        "          themes = list(dict.fromkeys([line.strip() for line in final_theme.split(\"\\n\") if line.strip()]))\n",
        "          results[label] = \"\\n\".join(themes)\n",
        "        else:\n",
        "          results[label] = summarize_combined_output(llm, combined_text, label)\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# Summarize combined chunk outputs into a single final output\n",
        "def summarize_combined_output(llm, text, label):\n",
        "    prompts = {\n",
        "        \"Summary\": \"You are a helpful scientific assistant. Based on the following combined summaries of a scientific paper, provide a single concise overall summary. Mention the main contribution, dataset used, method, evaluation metrics, and key results.\",\n",
        "        \"Connections\": \"You are reading several research papers. Based on the following notes, summarize the common connections or similarities across papers, focusing on shared techniques, datasets, or models.\",\n",
        "        \"Themes\": \"Summarize the central research themes mentioned in the combined text below. List them as concise, broad topics.\"\n",
        "    }\n",
        "    prompt = f\"{prompts[label]}\\n\\n{text}\"\n",
        "    try:\n",
        "        response = llm.pipeline(prompt)\n",
        "        return response[0][\"generated_text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating final {label}: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "\n",
        "# Main execution\n",
        "results = process_all_documents(data_dir=\"data\", max_chunks=10)  # Limit the number of chunks for faster processing\n",
        "# Final outputs\n",
        "final_connections = results.get(\"Connections\", \"\")\n",
        "final_themes = results.get(\"Themes\", \"\")\n",
        "final_summary = results.get(\"Summary\", \"\")\n",
        "\n",
        "# Display summaries\n",
        "print(\"\\nFinal Summary:\")\n",
        "print(final_summary)\n",
        "print(\"\\nFinal Connections Across Papers:\")\n",
        "print(final_connections)\n",
        "print(\"\\nFinal Themes:\")\n",
        "print(final_themes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "70c9e53a42334d7fba2f30741b7c75be",
            "de340c0db90c4301a49b0c1dcce1b648",
            "c363c95105644794842e606de9fd2993",
            "f55599e3b5d5443f86697333ac9cc1b9",
            "420e71a8cb0a4076b994984b20077cc4",
            "6a9dcbc054b348ff8d5ea10f8d393f0a",
            "6c5fe19fccb04666b3bb7a029ec1933e",
            "839742f43e394854894da2e80afe41f2",
            "f821d053132643c8a0e509cf198d78c2",
            "dd9263351a9940c68c55b28244b6c7a1",
            "ac408326a72c41eea1909b36eb3247b3"
          ]
        },
        "outputId": "3d18074b-5e00-4901-f92e-c02f2af27261",
        "id": "wPC5QP8_IRw2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c9e53a42334d7fba2f30741b7c75be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Processing: data/2105.05233.txt\n",
            "\n",
            "🔍 Summary:\n",
            "We show that diffusion models can achieve image sample quality superior to the current stateoftheart generative models. We achieve this on unconditional im age synthesis by nding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classier guidance: a simple, computeefcient method for trading off diversity for delity using gradients from a classier. We achieve an FID of 2.97 on ImageNet 128128, 4.59 on ImageNet 256256, and 7.72 on ImageNet 512512, and we match BigGANdeep even with as few as 25 forward passes per sample. Finally, we nd that classier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256256 and 3.85 on ImageNet 512512. We release our code at 1 Introduction\n",
            "\n",
            "🔍 Connections:\n",
            "ImageNet 512512 images, they are not yet capable of producing highquality images.\n",
            "\n",
            "🔍 Themes:\n",
            "Image Synthesis\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a new class of likelihoodbased models called diffusion models. We show that these models can produce highquality images while capturing more diversity than GANs. We also show that these models can be trained faster than GANs.\n",
            "\n",
            "🔍 Connections:\n",
            "---\n",
            "\n",
            "🔍 Themes:\n",
            "---\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a new model architecture and a new scheme for trading off diversity for delity. We achieve a new stateoftheart, surpassing GANs on several different metrics and datasets.\n",
            "\n",
            "🔍 Connections:\n",
            "In Section 3, we introduce our model architecture and discuss the tradeoffs between diversity and delity.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models for generating samples\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a method for using gradients from a classier to guide a diffusion model during sampling. We nd that a single hyperparameter, the scale of the classier gradients, can be tuned to trade off diversity for delity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples . We also compare our improved models to upsampling stacks, nding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256256 and 512512.\n",
            "\n",
            "🔍 Connections:\n",
            ", and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classier to guide a diffusion model during sampling. We nd that a single hyperparameter, the scale of the classier gradients, can be tuned to trade off diversity for delity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples . Finally, in Section 5 we show that models with our improved architecture achieve stateoftheart on unconditional image synthesis tasks, and with classier guidance achieve stateoftheart on conditional image synthesis. We also compare our improved models to upsampling stacks, nding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256256 and 512512.\n",
            "\n",
            "🔍 Themes:\n",
            "--- Architecture improvements --- Classier guidance --- Image synthesis\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a new method for sampling from a noise predictor. We show that this method is equivalent to the classical diffusion sampling method. We show that the proposed method is a generalization of the Ho et al. method. We show that the proposed method is a generalization of the Ho et al. method.\n",
            "\n",
            "🔍 Connections:\n",
            "Ho et al.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion sampling with noise predictors\n",
            "\n",
            "🔍 Summary:\n",
            "Models for denoising and score matching with a generative model and a neural network.\n",
            "\n",
            "🔍 Connections:\n",
            "Denoising Diffusion Model that combines the Denoising Diffusion Model with a VAE.\n",
            "\n",
            "🔍 Themes:\n",
            "Denoising Diffusion Models\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We propose a new method for estimating the latent distribution of images using a non-Markovian noising process. We adopt the DDPM model proposed by Nichol and Dhariwal . We also adopt the DDIM model proposed by Song et al. . We evaluate the models using the following metrics: Inception Score IS - Inception Score IS is a measure of how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing.\n",
            "\n",
            "🔍 Connections:\n",
            "We adopt the hybrid objective and parameterization proposed by Nichol and Dhariwal . We also adopt the hybrid objective and parameterization proposed by Song et al. .\n",
            "\n",
            "🔍 Themes:\n",
            "DDPM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We propose a new metric for evaluating deep neural networks, based on the Frchet Inception Distance (FID) metric. We use FID as our metric to evaluate the performance of our models on the full ImageNet class distribution. We use FID as our metric to evaluate the performance of our models on the full ImageNet class distribution.\n",
            "\n",
            "🔍 Connections:\n",
            "metric, and we use sFID as a spatial version of FID.\n",
            "\n",
            "🔍 Themes:\n",
            "metric for evaluating architectures\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a new architecture for diffusion models, which improves sample quality over the previous architectures used for denoising score matching. We conduct several architecture ablations to nd the model architecture that provides the best sample quality for diffusion models.\n",
            "\n",
            "🔍 Connections:\n",
            "UNet architecture for diffusion models.\n",
            "\n",
            "🔍 Themes:\n",
            "Adaptive sampling for diffusion models\n",
            "\n",
            "🔍 Summary:\n",
            "--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
            "\n",
            "🔍 Connections:\n",
            "---\n",
            "\n",
            "🔍 Themes:\n",
            "UNet architecture for image reconstruction\n",
            "\n",
            "📄 Processing: data/2006.11239.txt\n",
            "\n",
            "🔍 Summary:\n",
            "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a stateoftheart FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\n",
            "\n",
            "🔍 Connections:\n",
            "Denoising Diffusion Probabilistic Models\n",
            "\n",
            "🔍 Themes:\n",
            "Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Den\n",
            "\n",
            "🔍 Summary:\n",
            "We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models Section 4.\n",
            "\n",
            "🔍 Connections:\n",
            "We also show that the equivalence is not restricted to the case of Gaussian noise.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion Probabilistic Models\n",
            "\n",
            "🔍 Summary:\n",
            "Models are compared to annealed Langevin dynamics and score matching. We show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.\n",
            "\n",
            "🔍 Connections:\n",
            "Latent Variable Models\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models are a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Summary:\n",
            "--- Diffusion models are a generalization of the forward process model. They are characterized by the following properties: --- The forward process is xed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule 1, . . . , T: qx1:T x0 : T Y t1 qxtxt1, qxtxt1 : Nxt; p 1 txt1, tI 2 Training is performed by optimizing the usual variational bound on negative log likelihood: E log px0 Eq log px0:T qx1:T x0 Eq log pxT X t1 log pxt1xt qxtxt1 : L 3 The forward process variances t can be learned by reparameterization or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in pxtxt; tx0, 1 tI 4 2 Efficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L 3 as: Eq DKLqxT x0 pxT z LT X t1 DKLqxt1xt, x0 pxt1xt z Lt1 log px0x1 z L0\n",
            "\n",
            "🔍 Connections:\n",
            "KL divergence.\n",
            "\n",
            "🔍 Themes:\n",
            "--- Diffusion models with a pxt1xt : Nxt1; xt, t, xt, t\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a new explicit connection between diffusion models and denoising score matching. We use this connection to guide our model design. We discuss our choices in the following sections.\n",
            "\n",
            "🔍 Connections:\n",
            "Diffusion models and denoising autoencoders\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models and denoising autoencoders\n",
            "\n",
            "🔍 Summary:\n",
            "We propose a new model for the reverse process entropy, Lt, that is a model that predicts t, the forward process posterior mean.\n",
            "\n",
            "🔍 Connections:\n",
            "N0, I, we can use the forward process posterior as a parameter.\n",
            "\n",
            "🔍 Themes:\n",
            "txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt, txt\n",
            "\n",
            "🔍 Summary:\n",
            "We train a reverse process mean function approximator to predict t. We also show that the parameterization 11 is equivalent to a variational bound for the Langevinlike reverse process 11.\n",
            "\n",
            "🔍 Connections:\n",
            "---\n",
            "\n",
            "🔍 Themes:\n",
            "--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
            "\n",
            "🔍 Summary:\n",
            "Main contribution: We introduce a new method for predicting the pixel coordinates of an image using a neural network reverse process. We use a Gaussian decoder to obtain discrete log likelihoods. We use a Gaussian decoder to obtain discrete log likelihoods. We use a Gaussian decoder to obtain discrete log likelihoods.\n",
            "\n",
            "🔍 Connections:\n",
            "We use the pxt1xt model to train the decoder.\n",
            "\n",
            "🔍 Themes:\n",
            "Variational bounds for a Gaussian decoder\n",
            "\n",
            "🔍 Summary:\n",
            "--- Variational bound for a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with\n",
            "\n",
            "🔍 Connections:\n",
            "L0.\n",
            "\n",
            "🔍 Themes:\n",
            "Variational bound for a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a recurrent neural network with a\n",
            "\n",
            "🔍 Summary:\n",
            "--- the bin width, ignoring 2 1 and edge effects. The t 1 cases correspond to an unweighted version of Eq. 12, analogous to the loss weighting used by the NCSN denoising score matching model . LT does not appear because the forward process variances t are xed. Algorithm 1 displays the complete training procedure with this simplied objective. Since our simplied objective 14 discards the weighting in Eq. 12, it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound . In particular, our diffusion process setup in Section 4 causes the simplied objective to downweight loss terms corresponding to small t. These terms train the network to denoise data with very small amounts of noise, so it is benecial to downweight them so that the network can focus on more difcult denoising tasks at larger t terms. 4 Experiments We set T 1000 for all experiments so that the number of neural network evaluations needed during sampling matches previous work . We set the forward process variances to constants increasing linearly from 1 104 to T 0.02. These constants were chosen to be small relative to data scaled to 1, 1, ensuring that reverse and forward processes have approximately the same functional form while keeping the signaltonoise ratio at xT as small as possible LT DKLqxT x0 N0, I 105 bits per dimension in our experiments. To represent the\n",
            "\n",
            "🔍 Connections:\n",
            "t1 cases, we use the t1 cases as a weighted variational bound.\n",
            "\n",
            "🔍 Themes:\n",
            "Adaptive sampling for denoising with a simple forward process\n",
            "Error generating final Summary: CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.68 GiB is free. Process 3439 has 13.06 GiB memory in use. Of the allocated memory 12.92 GiB is allocated by PyTorch, and 14.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Error generating final Themes: CUDA out of memory. Tried to allocate 3.39 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 3439 has 12.90 GiB memory in use. Of the allocated memory 10.83 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Final Summary:\n",
            "Error\n",
            "\n",
            "Final Connections Across Papers:\n",
            "Denoising Diffusion Probabilistic Models KL divergence. Denoising Diffusion Probabilistic Models ---\n",
            "\n",
            "Final Themes:\n",
            "Error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4opMFxDGS8gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdhMO9TKS8i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mv4Fl3s4_jc3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Metadata Extraction & Knowledge Graph\n",
        "The system uses an LLM to extract structured metadata from each paper, including:\n",
        "\n",
        "Title, authors, publication year\n",
        "Keywords and methodologies\n",
        "Abstract\n",
        "Cited papers\n",
        "\n",
        "This metadata forms the basis of a knowledge graph built with NetworkX, where:\n",
        "\n",
        "Papers, authors, journals, methodologies, and keywords are nodes\n",
        "Relationships (authored, cites, uses) are edges\n",
        "\n",
        "This graph representation allows visualization of relationships between papers and helps identify key authors, methodologies, and research themes across multiple papers."
      ],
      "metadata": {
        "id": "jhA6cB-LhA9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Analysis Capabilities\n",
        "\n",
        "Paper Summarization\n",
        "For each paper, the system generates a comprehensive summary covering:\n",
        "\n",
        "Main research questions\n",
        "Methodology\n",
        "Key findings\n",
        "Limitations and future work\n",
        "\n",
        "This helps quickly understand individual papers without reading the full text.\n",
        "\n",
        "\n",
        "Paper Comparison\n",
        "The system can compare multiple papers to identify:\n",
        "\n",
        "Common themes\n",
        "Differences in methodology\n",
        "Complementary or contradictory findings\n",
        "Research gaps\n",
        "\n",
        "\n",
        "Theme Extraction\n",
        "The system analyzes all papers to identify common themes, showing which papers address each theme and how they contribute to it.\n",
        "\n",
        "\n",
        "Question Answering\n",
        "When you ask a question:\n",
        "\n",
        "The system retrieves the most relevant chunks from across all papers\n",
        "It provides the chunks as context to the LLM\n",
        "The LLM synthesizes an answer based on this context, citing the relevant papers\n",
        "\n"
      ],
      "metadata": {
        "id": "rmkO0O2IhEJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6zUaAwjfvNy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AcademicRAG:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-4o\"):\n",
        "        \"\"\"\n",
        "        Initialize the Academic RAG system.\n",
        "\n",
        "        Args:\n",
        "            api_key: OpenAI API key\n",
        "            model_name: LLM model to use\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=model_name,\n",
        "            openai_api_key=api_key,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Initialize document storage\n",
        "        self.documents = {}\n",
        "        self.chunks = {}\n",
        "        self.metadata = {}\n",
        "        self.knowledge_graph = nx.DiGraph()\n",
        "\n",
        "    def load_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to PDF file\n",
        "\n",
        "        Returns:\n",
        "            Full text content of the PDF\n",
        "        \"\"\"\n",
        "        doc_id = Path(file_path).stem\n",
        "        pdf_document = fitz.open(file_path)\n",
        "        text = \"\"\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "\n",
        "        # Store the full document\n",
        "        self.documents[doc_id] = text\n",
        "\n",
        "        print(f\"Loaded document: {doc_id} ({len(text)} characters)\")\n",
        "        return text\n",
        "\n",
        "    def load_multiple_pdfs(self, directory: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Load multiple PDFs from a directory.\n",
        "\n",
        "        Args:\n",
        "            directory: Directory containing PDF files\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to full text content\n",
        "        \"\"\"\n",
        "        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            file_path = os.path.join(directory, pdf_file)\n",
        "            self.load_pdf(file_path)\n",
        "\n",
        "        return self.documents\n",
        "\n",
        "    def chunk_document(self, doc_id: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Split document into chunks.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            List of document chunks with metadata\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found. Load it first.\")\n",
        "\n",
        "        text = self.documents[doc_id]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.create_documents([text])\n",
        "\n",
        "        # Add metadata to chunks\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata = {\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"source\": doc_id\n",
        "            }\n",
        "\n",
        "        self.chunks[doc_id] = chunks\n",
        "        print(f\"Split {doc_id} into {len(chunks)} chunks\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_all_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict[str, List[Document]]:\n",
        "        \"\"\"\n",
        "        Split all loaded documents into chunks.\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to lists of chunks\n",
        "        \"\"\"\n",
        "        for doc_id in self.documents:\n",
        "            self.chunk_document(doc_id, chunk_size, chunk_overlap)\n",
        "\n",
        "        return self.chunks\n",
        "\n",
        "    def build_vector_store(self):\n",
        "        \"\"\"\n",
        "        Build vector store from all document chunks.\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        for doc_chunks in self.chunks.values():\n",
        "            all_chunks.extend(doc_chunks)\n",
        "\n",
        "        # Create embedding class that adapts SentenceTransformer to LangChain\n",
        "        class STEmbeddings(Embeddings):\n",
        "            def __init__(self, model):\n",
        "                self.model = model\n",
        "\n",
        "            def embed_documents(self, texts):\n",
        "                return self.model.encode(texts).tolist()\n",
        "\n",
        "            def embed_query(self, text):\n",
        "                return self.model.encode(text).tolist()\n",
        "\n",
        "        embeddings = STEmbeddings(self.embedding_model)\n",
        "\n",
        "        # Create FAISS vector store\n",
        "        self.vector_store = FAISS.from_documents(all_chunks, embeddings)\n",
        "        print(f\"Built vector store with {len(all_chunks)} chunks\")\n",
        "\n",
        "    def extract_metadata(self):\n",
        "        \"\"\"\n",
        "        Extract metadata from papers using LLM.\n",
        "        \"\"\"\n",
        "        schema = {\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"string\"},\n",
        "                \"authors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"publication_year\": {\"type\": \"integer\"},\n",
        "                \"journal\": {\"type\": \"string\"},\n",
        "                \"abstract\": {\"type\": \"string\"},\n",
        "                \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"methodology\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"cited_papers\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "            },\n",
        "            \"required\": [\"title\", \"authors\"],\n",
        "        }\n",
        "\n",
        "        extraction_chain = create_extraction_chain(schema, self.llm)\n",
        "\n",
        "        for doc_id, text in self.documents.items():\n",
        "            # Use the first chunk plus any detected abstract section for metadata extraction\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", text, re.DOTALL)\n",
        "            abstract_text = abstract_match.group(1) if abstract_match else \"\"\n",
        "\n",
        "            input_text = first_chunk + \"\\n\\n\" + abstract_text\n",
        "\n",
        "            # Truncate if too long\n",
        "            if len(input_text) > 5000:\n",
        "                input_text = input_text[:5000]\n",
        "\n",
        "            try:\n",
        "                result = extraction_chain.invoke({\"input\": input_text})\n",
        "                metadata = result[\"extracted\"][0] if result[\"extracted\"] else {}\n",
        "                self.metadata[doc_id] = metadata\n",
        "                print(f\"Extracted metadata for {doc_id}: {metadata['title'] if 'title' in metadata else 'Unknown'}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting metadata for {doc_id}: {e}\")\n",
        "                self.metadata[doc_id] = {\"title\": doc_id, \"authors\": [\"Unknown\"]}\n",
        "\n",
        "    def build_knowledge_graph(self):\n",
        "        \"\"\"\n",
        "        Build knowledge graph from extracted paper metadata.\n",
        "        \"\"\"\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add papers as nodes\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = metadata.get(\"authors\", [\"Unknown\"])\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "            journal = metadata.get(\"journal\", \"Unknown\")\n",
        "\n",
        "            # Add paper node\n",
        "            G.add_node(title,\n",
        "                       type=\"paper\",\n",
        "                       authors=authors,\n",
        "                       year=year,\n",
        "                       journal=journal,\n",
        "                       doc_id=doc_id)\n",
        "\n",
        "            # Add author nodes and connections\n",
        "            for author in authors:\n",
        "                if not G.has_node(author):\n",
        "                    G.add_node(author, type=\"author\")\n",
        "                G.add_edge(author, title, type=\"authored\")\n",
        "\n",
        "            # Add journal node and connection\n",
        "            if journal != \"Unknown\":\n",
        "                if not G.has_node(journal):\n",
        "                    G.add_node(journal, type=\"journal\")\n",
        "                G.add_edge(title, journal, type=\"published_in\")\n",
        "\n",
        "            # Add methodology nodes\n",
        "            methods = metadata.get(\"methodology\", [])\n",
        "            for method in methods:\n",
        "                if not G.has_node(method):\n",
        "                    G.add_node(method, type=\"methodology\")\n",
        "                G.add_edge(title, method, type=\"uses\")\n",
        "\n",
        "            # Add keyword nodes\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "            for keyword in keywords:\n",
        "                if not G.has_node(keyword):\n",
        "                    G.add_node(keyword, type=\"keyword\")\n",
        "                G.add_edge(title, keyword, type=\"contains\")\n",
        "\n",
        "        # Add citation links\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            source_title = metadata.get(\"title\", doc_id)\n",
        "            cited_papers = metadata.get(\"cited_papers\", [])\n",
        "\n",
        "            for cited_paper in cited_papers:\n",
        "                # Try to match with existing paper nodes\n",
        "                matching_papers = [node for node in G.nodes() if G.nodes[node].get(\"type\") == \"paper\" and cited_paper.lower() in node.lower()]\n",
        "\n",
        "                if matching_papers:\n",
        "                    G.add_edge(source_title, matching_papers[0], type=\"cites\")\n",
        "                else:\n",
        "                    # Add as a new node if not found\n",
        "                    G.add_node(cited_paper, type=\"external_paper\")\n",
        "                    G.add_edge(source_title, cited_paper, type=\"cites\")\n",
        "\n",
        "        self.knowledge_graph = G\n",
        "        print(f\"Built knowledge graph with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "\n",
        "        return G\n",
        "\n",
        "    def visualize_knowledge_graph(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        \"\"\"\n",
        "        Visualize the knowledge graph.\n",
        "\n",
        "        Args:\n",
        "            output_file: Path to save the HTML visualization\n",
        "        \"\"\"\n",
        "        G = self.knowledge_graph\n",
        "\n",
        "        # Create pyvis network\n",
        "        net = Network(height=\"750px\", width=\"100%\", notebook=False, directed=True)\n",
        "\n",
        "        # Node colors by type\n",
        "        color_map = {\n",
        "            \"paper\": \"#4285F4\",  # blue\n",
        "            \"author\": \"#EA4335\",  # red\n",
        "            \"journal\": \"#FBBC05\",  # yellow\n",
        "            \"methodology\": \"#34A853\",  # green\n",
        "            \"keyword\": \"#800080\",  # purple\n",
        "            \"external_paper\": \"#A0A0A0\"  # gray\n",
        "        }\n",
        "\n",
        "        # Add nodes\n",
        "        for node in G.nodes():\n",
        "            node_type = G.nodes[node].get(\"type\", \"unknown\")\n",
        "            title = f\"Type: {node_type}\"\n",
        "\n",
        "            if node_type == \"paper\":\n",
        "                authors = \", \".join(G.nodes[node].get(\"authors\", [\"Unknown\"]))\n",
        "                year = G.nodes[node].get(\"year\", \"Unknown\")\n",
        "                journal = G.nodes[node].get(\"journal\", \"Unknown\")\n",
        "                title = f\"Paper: {node}\\nAuthors: {authors}\\nYear: {year}\\nJournal: {journal}\"\n",
        "\n",
        "            net.add_node(node,\n",
        "                         title=title,\n",
        "                         color=color_map.get(node_type, \"#000000\"),\n",
        "                         size=20 if node_type == \"paper\" else 10)\n",
        "\n",
        "        # Add edges\n",
        "        edge_colors = {\n",
        "            \"authored\": \"#EA4335\",\n",
        "            \"published_in\": \"#FBBC05\",\n",
        "            \"uses\": \"#34A853\",\n",
        "            \"contains\": \"#800080\",\n",
        "            \"cites\": \"#4285F4\"\n",
        "        }\n",
        "\n",
        "        for source, target, data in G.edges(data=True):\n",
        "            edge_type = data.get(\"type\", \"unknown\")\n",
        "            net.add_edge(source, target,\n",
        "                         title=edge_type,\n",
        "                         color=edge_colors.get(edge_type, \"#000000\"),\n",
        "                         arrows=\"to\")\n",
        "\n",
        "        # Set physics layout\n",
        "        net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08)\n",
        "        net.toggle_physics(True)\n",
        "\n",
        "        # Save visualization\n",
        "        net.show(output_file)\n",
        "        print(f\"Knowledge graph visualization saved to {output_file}\")\n",
        "\n",
        "    def retrieve_similar_chunks(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve similar chunks for a query.\n",
        "\n",
        "        Args:\n",
        "            query: Query text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            List of (document, similarity) tuples\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not built. Call build_vector_store() first.\")\n",
        "\n",
        "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_paper_summary(self, doc_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a summary for a specific paper.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "\n",
        "        Returns:\n",
        "            Summary text\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Summarize the following academic paper excerpt:\n",
        "\n",
        "            {text}\n",
        "\n",
        "            Provide a comprehensive summary that includes:\n",
        "            1. The main research question/problem\n",
        "            2. The methodology used\n",
        "            3. Key findings and contributions\n",
        "            4. Limitations and future work\n",
        "\n",
        "            Summary:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Use the first few chunks (up to 10,000 characters)\n",
        "        chunks = self.chunks.get(doc_id, [])\n",
        "        combined_text = \"\"\n",
        "        for chunk in chunks:\n",
        "            combined_text += chunk.page_content + \"\\n\\n\"\n",
        "            if len(combined_text) > 10000:\n",
        "                break\n",
        "        combined_text = combined_text[:10000]\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        summary = chain.invoke({\"text\": combined_text})\n",
        "\n",
        "        return summary[\"text\"]\n",
        "\n",
        "    def compare_papers(self, doc_ids: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Compare multiple papers.\n",
        "\n",
        "        Args:\n",
        "            doc_ids: List of document identifiers\n",
        "\n",
        "        Returns:\n",
        "            Comparison text\n",
        "        \"\"\"\n",
        "        # Validate all doc_ids\n",
        "        for doc_id in doc_ids:\n",
        "            if doc_id not in self.documents:\n",
        "                raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        # Get metadata for each paper\n",
        "        paper_info = []\n",
        "        for doc_id in doc_ids:\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "\n",
        "            # Get a brief summary by using first chunk\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            paper_info.append(f\"Title: {title}\\nAuthors: {authors}\\nYear: {year}\\nExcerpt: {first_chunk[:500]}...\")\n",
        "\n",
        "        # Generate comparison using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Compare and contrast the following academic papers:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Provide a detailed comparison including:\n",
        "            1. Common themes and research questions\n",
        "            2. Differences in methodology\n",
        "            3. Complementary or contradictory findings\n",
        "            4. How they build upon each other's work\n",
        "            5. Potential gaps or areas for future research\n",
        "\n",
        "            Comparison:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        comparison = chain.invoke({\"papers\": \"\\n\\n\".join(paper_info)})\n",
        "\n",
        "        return comparison[\"text\"]\n",
        "\n",
        "    def answer_question(self, question: str, k: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Answer a question based on the papers.\n",
        "\n",
        "        Args:\n",
        "            question: Question text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Answer text\n",
        "        \"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = self.retrieve_similar_chunks(question, k=k)\n",
        "\n",
        "        # Prepare context from chunks\n",
        "        context = []\n",
        "        for chunk, score in relevant_chunks:\n",
        "            doc_id = chunk.metadata[\"doc_id\"]\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            context.append(f\"From '{title}' by {authors}:\\n{chunk.page_content}\")\n",
        "\n",
        "        # Generate answer using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"question\", \"context\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant with expertise in synthesizing information from academic papers.\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Here are relevant excerpts from academic papers:\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Please provide a comprehensive answer to the question based on the given context.\n",
        "            Cite the papers you're referencing in your response.\n",
        "            If the information provided is insufficient to answer the question, clearly state what's missing.\n",
        "\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        answer = chain.invoke({\"question\": question, \"context\": \"\\n\\n\".join(context)})\n",
        "\n",
        "        return answer[\"text\"]\n",
        "\n",
        "    def extract_themes(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract common themes across papers.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of themes with supporting evidence\n",
        "        \"\"\"\n",
        "        # Prepare paper information\n",
        "        papers_info = []\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            abstract = metadata.get(\"abstract\", \"\")\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "\n",
        "            if not abstract:\n",
        "                # Try to find abstract in the document\n",
        "                doc_text = self.documents[doc_id]\n",
        "                abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", doc_text, re.DOTALL)\n",
        "                abstract = abstract_match.group(1) if abstract_match else \"No abstract found\"\n",
        "\n",
        "            papers_info.append(f\"Title: {title}\\nAbstract: {abstract}\\nKeywords: {', '.join(keywords)}\")\n",
        "\n",
        "        # Generate themes using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Analyze the following academic papers and extract common themes:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Identify 3-5 major themes across these papers. For each theme:\n",
        "            1. Provide a clear name and description\n",
        "            2. List which papers address this theme\n",
        "            3. Describe how each paper contributes to or approaches this theme\n",
        "            4. Note any contradictions or complementary findings within the theme\n",
        "\n",
        "            Format your response as a JSON structure where each theme is a key, and the value contains the description and paper relationships.\n",
        "\n",
        "            Themes:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        themes_result = chain.invoke({\"papers\": \"\\n\\n\".join(papers_info)})\n",
        "\n",
        "        # Extract themes from the text response\n",
        "        themes_text = themes_result[\"text\"]\n",
        "\n",
        "        # Use LLM to convert to structured format\n",
        "        extraction_prompt = PromptTemplate(\n",
        "            input_variables=[\"themes_text\"],\n",
        "            template=\"\"\"\n",
        "            Convert the following themes analysis into a proper JSON structure:\n",
        "\n",
        "            {themes_text}\n",
        "\n",
        "            JSON format:\n",
        "            ```\n",
        "            {{\n",
        "                \"Theme 1 Name\": {{\n",
        "                    \"description\": \"Description of theme 1\",\n",
        "                    \"papers\": [\n",
        "                        {{\n",
        "                            \"title\": \"Paper Title 1\",\n",
        "                            \"contribution\": \"How Paper 1 contributes to this theme\"\n",
        "                        }},\n",
        "                        ...\n",
        "                    ]\n",
        "                }},\n",
        "                ...\n",
        "            }}\n",
        "            ```\n",
        "\n",
        "            Only respond with the valid JSON, nothing else.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=extraction_prompt)\n",
        "        json_result = chain.invoke({\"themes_text\": themes_text})\n",
        "\n",
        "        try:\n",
        "            # Extract JSON from the response (it might be wrapped in backticks)\n",
        "            json_text = re.search(r'```json\\n(.*?)\\n```', json_result[\"text\"], re.DOTALL)\n",
        "            if json_text:\n",
        "                json_str = json_text.group(1)\n",
        "            else:\n",
        "                json_str = json_result[\"text\"]\n",
        "\n",
        "            # Clean up any extra backticks\n",
        "            json_str = json_str.replace('```', '').strip()\n",
        "\n",
        "            import json\n",
        "            themes = json.loads(json_str)\n",
        "            return themes\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing themes JSON: {e}\")\n",
        "            return {\"error\": \"Could not parse themes\", \"raw_text\": themes_text}\n",
        "\n",
        "    def run_full_analysis(self, pdf_directory: str, output_dir: str = \"./output\"):\n",
        "        \"\"\"\n",
        "        Run full analysis pipeline on a directory of PDFs.\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_dir: Directory to save output files\n",
        "        \"\"\"\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Load PDFs\n",
        "        print(\"Loading PDFs...\")\n",
        "        self.load_multiple_pdfs(pdf_directory)\n",
        "\n",
        "        # 2. Chunk documents\n",
        "        print(\"Chunking documents...\")\n",
        "        self.chunk_all_documents()\n",
        "\n",
        "        # 3. Build vector store\n",
        "        print(\"Building vector store...\")\n",
        "        self.build_vector_store()\n",
        "\n",
        "        # 4. Extract metadata\n",
        "        print(\"Extracting metadata...\")\n",
        "        self.extract_metadata()\n",
        "\n",
        "        # 5. Build knowledge graph\n",
        "        print(\"Building knowledge graph...\")\n",
        "        self.build_knowledge_graph()\n",
        "\n",
        "        # 6. Visualize knowledge graph\n",
        "        print(\"Visualizing knowledge graph...\")\n",
        "        self.visualize_knowledge_graph(os.path.join(output_dir, \"knowledge_graph.html\"))\n",
        "\n",
        "        # 7. Generate summaries for each paper\n",
        "        print(\"Generating summaries...\")\n",
        "        summaries = {}\n",
        "        for doc_id in self.documents:\n",
        "            summaries[doc_id] = self.generate_paper_summary(doc_id)\n",
        "\n",
        "            # Save individual summaries\n",
        "            with open(os.path.join(output_dir, f\"{doc_id}_summary.txt\"), \"w\") as f:\n",
        "                f.write(summaries[doc_id])\n",
        "\n",
        "        # 8. Compare all papers\n",
        "        print(\"Comparing papers...\")\n",
        "        comparison = self.compare_papers(list(self.documents.keys()))\n",
        "        with open(os.path.join(output_dir, \"paper_comparison.txt\"), \"w\") as f:\n",
        "            f.write(comparison)\n",
        "\n",
        "        # 9. Extract themes\n",
        "        print(\"Extracting themes...\")\n",
        "        themes = self.extract_themes()\n",
        "\n",
        "        import json\n",
        "        with open(os.path.join(output_dir, \"themes.json\"), \"w\") as f:\n",
        "            json.dump(themes, f, indent=2)\n",
        "\n",
        "        print(f\"Analysis complete! Results saved in {output_dir}\")\n",
        "\n",
        "        return {\n",
        "            \"summaries\": summaries,\n",
        "            \"comparison\": comparison,\n",
        "            \"themes\": themes\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your OpenAI API key\n",
        "    OPENAI_API_KEY = \"your-api-key-here\"\n",
        "\n",
        "    # Initialize the RAG system\n",
        "    rag = AcademicRAG(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # Run full analysis\n",
        "    results = rag.run_full_analysis(\"./papers\")\n",
        "\n",
        "    # Ask questions\n",
        "    question = \"What are the main methodologies used across these papers and how do they compare?\"\n",
        "    answer = rag.answer_question(question)\n",
        "    print(f\"Q: {question}\\nA: {answer}\")\n",
        "\n",
        "    # Get more paper relationships\n",
        "    print(\"Paper relationships:\")\n",
        "    for source, target, data in rag.knowledge_graph.edges(data=True):\n",
        "        if data.get(\"type\") == \"cites\":\n",
        "            print(f\"  {source} cites {target}\")"
      ],
      "metadata": {
        "id": "rGFG6eGPgmQw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "36ef0202-cc9a-41dc-96c6-268155a5c3db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AcademicRAG' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-67231c360bc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Initialize the RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAcademicRAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPENAI_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Run full analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AcademicRAG' is not defined"
          ]
        }
      ]
    }
  ]
}