{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1wZS5VtSq7Jp6IwkEpeS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/RAG-literatureReview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI in Research: using Large Language Models (LLMs) to enhance and streamline the academic literature review process."
      ],
      "metadata": {
        "id": "vq3zIOCofySQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "leverage RAG Techniques for summarizing papers, identifying connections across papers (authors, references, methods), uncovering key themes in them, and Constructing a knowledge graph to visualize and interpret findings.\n",
        "\n",
        "Download 5 papers (related to diffusion model) and convert them to 5 .txt files in a directory named \"data\". Use these .txt files as input papers and evaluate if the RAG technique is giving good results."
      ],
      "metadata": {
        "id": "S_-IHuLTAh-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf requests"
      ],
      "metadata": {
        "id": "OUPE-P03hm6L",
        "outputId": "0a83031b-b489-4b5e-ab38-d04f5aba6c51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download any 2 papers of diffusion model and convert their pdf to .txt files"
      ],
      "metadata": {
        "id": "MbRT1J6xiGMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r data"
      ],
      "metadata": {
        "id": "2eWwSdyRMPPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Sample arXiv paper IDs related to diffusion models\n",
        "arxiv_ids = [\n",
        "    \"2006.11239\",  # Denoising Diffusion Probabilistic Models\n",
        "    \"2105.05233\",  # Improved Denoising Diffusion Probabilistic Models\n",
        "]\n",
        "\n",
        "def download_pdf(arxiv_id, output_folder):\n",
        "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "    pdf_path = os.path.join(output_folder, f\"{arxiv_id}.pdf\")\n",
        "    response = requests.get(url)\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {arxiv_id}\")\n",
        "    return pdf_path\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"Converted to text: {txt_path}\")\n",
        "\n",
        "def main():\n",
        "    data_dir = \"data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for arxiv_id in arxiv_ids:\n",
        "        pdf_path = download_pdf(arxiv_id, data_dir)\n",
        "        txt_path = os.path.join(data_dir, f\"{arxiv_id}.txt\")\n",
        "        pdf_to_text(pdf_path, txt_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Yq0vtxjXhZcm",
        "outputId": "0a0cf782-5e5f-44f1-b12a-024eb72b77b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 2006.11239\n",
            "Converted to text: data/2006.11239.txt\n",
            "Downloaded 2105.05233\n",
            "Converted to text: data/2105.05233.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm data/*.pdf"
      ],
      "metadata": {
        "id": "mieCFJzFAOuw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install langchain faiss-cpu sentence-transformers transformers networkx matplotlib spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "4f4wA_teK4xy",
        "outputId": "273f36bb-0968-4ef5-e1da-251a4eae4398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import gc\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "# Load a document and return its content\n",
        "def load_document(file_path):\n",
        "    loader = TextLoader(file_path)\n",
        "    return loader.load()  # This returns a list of documents, but we only need one document per file\n",
        "\n",
        "# Split the document into chunks ensuring each chunk is under the token limit\n",
        "def split_document(doc, chunk_size=512, chunk_overlap=100):\n",
        "    # Ensure the document passed is in the correct format\n",
        "    if isinstance(doc, list):  # Handle list of documents (from TextLoader)\n",
        "        doc = doc[0]  # Take the first document (we assume one per file)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.split_documents([doc])  # Convert doc to a list of one document\n",
        "\n",
        "# Vector store (use sentence embeddings)\n",
        "def create_faiss_index(docs):\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    return FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Load HuggingFace LLM\n",
        "def load_llm():\n",
        "    model_id = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
        "    return HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Build RAG chain\n",
        "def build_qa_chain(llm, vectorstore):\n",
        "    return RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever(), chain_type=\"stuff\")\n",
        "\n",
        "# Analyze a single document in chunks and store results\n",
        "def analyze_document(llm, qa, doc, batch_size=2):\n",
        "    results = {}\n",
        "    # Split the document into chunks\n",
        "    chunks = split_document(doc)\n",
        "\n",
        "    # Initialize vectorstore for the document\n",
        "    vectorstore = create_faiss_index(chunks)\n",
        "    qa_chain = build_qa_chain(llm, vectorstore)\n",
        "\n",
        "    # Process each chunk in batches\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "\n",
        "        for chunk in batch:\n",
        "            print(f\"\\n🔍 Analyzing Chunk...\")\n",
        "            questions = [\n",
        "                (\"Summary\", \"Summarize the key contributions of each paper.\"),\n",
        "                (\"Connections\", \"Identify connections between authors, shared references or methods across the papers.\"),\n",
        "                (\"Themes\", \"What are the key research themes in these papers on diffusion models?\")\n",
        "            ]\n",
        "\n",
        "            chunk_results = {}\n",
        "            for label, q in questions:\n",
        "                print(f\"\\n🔍 {label}:\")\n",
        "                try:\n",
        "                    result = qa_chain.run(q)  # Here, only the query (q) is passed\n",
        "                    print(result)\n",
        "                    chunk_results[label] = result\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during processing question {label}: {e}\")\n",
        "                    chunk_results[label] = \"Error processing the question\"\n",
        "\n",
        "            # Combine chunk results for each task\n",
        "            for label, result in chunk_results.items():\n",
        "                if label not in results:\n",
        "                    results[label] = []\n",
        "                results[label].append(result)\n",
        "\n",
        "        # Explicit garbage collection after processing each batch to free memory\n",
        "        del batch\n",
        "        gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Extract entities and relations using spaCy\n",
        "def build_knowledge_graph(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    G = nx.Graph()\n",
        "    for ent in doc.ents:\n",
        "        G.add_node(ent.text, label=ent.label_)\n",
        "    for i in range(len(doc.ents)-1):\n",
        "        G.add_edge(doc.ents[i].text, doc.ents[i+1].text)\n",
        "    return G\n",
        "\n",
        "# Visualize the graph\n",
        "def plot_graph(G):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pos = nx.spring_layout(G, k=0.5)\n",
        "    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=2000, font_size=10, font_weight='bold', edge_color='gray')\n",
        "    plt.title(\"Knowledge Graph from Diffusion Model Literature\")\n",
        "    plt.show()\n",
        "\n",
        "# Main pipeline\n",
        "def process_all_documents(data_dir=\"data\"):\n",
        "    files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
        "    results = {}\n",
        "\n",
        "    # Initialize LLM\n",
        "    llm = load_llm()\n",
        "\n",
        "    # Process each document one by one\n",
        "    for file_path in files:\n",
        "        print(f\"\\n🔍 Processing Document: {file_path}\")\n",
        "\n",
        "        # Load and analyze the document\n",
        "        doc = load_document(file_path)\n",
        "        doc_results = analyze_document(llm, qa=None, doc=doc, batch_size=2)\n",
        "\n",
        "        # Combine document results\n",
        "        for key, value in doc_results.items():\n",
        "            if key not in results:\n",
        "                results[key] = []\n",
        "            results[key].append(value)\n",
        "\n",
        "        # Explicit garbage collection after processing each document\n",
        "        del doc\n",
        "        gc.collect()\n",
        "\n",
        "    # Combine the results into one string for each task\n",
        "    for label in results:\n",
        "        results[label] = \" \".join(results[label])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Main execution\n",
        "results = process_all_documents(data_dir=\"data\")\n",
        "\n",
        "# Build and show knowledge graph from combined text\n",
        "all_text = \"\\n\".join(results.values())\n",
        "G = build_knowledge_graph(all_text)\n",
        "plot_graph(G)\n"
      ],
      "metadata": {
        "id": "Zbv3cIzljaPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c677625f-50c8-4e03-80b8-016ce53ed866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Processing Document: data/2006.11239.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e6a1b0bd10e6>:71: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain.run(q)  # Here, only the query (q) is passed\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Analyzing Chunk...\n",
            "\n",
            "🔍 Summary:\n",
            "[10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\n",
            "\n",
            "🔍 Connections:\n",
            "[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011. [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In International Conference on Learning Representations, 2020. [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models and denoising autoencoders\n",
            "\n",
            "🔍 Analyzing Chunk...\n",
            "\n",
            "🔍 Summary:\n",
            "[10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\n",
            "\n",
            "🔍 Connections:\n",
            "[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011. [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In International Conference on Learning Representations, 2020. [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models and denoising autoencoders\n",
            "\n",
            "🔍 Analyzing Chunk...\n",
            "\n",
            "🔍 Summary:\n",
            "[10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\n",
            "\n",
            "🔍 Connections:\n",
            "[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011. [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In International Conference on Learning Representations, 2020. [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models and denoising autoencoders\n",
            "\n",
            "🔍 Analyzing Chunk...\n",
            "\n",
            "🔍 Summary:\n",
            "[10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\n",
            "\n",
            "🔍 Connections:\n",
            "[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011. [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In International Conference on Learning Representations, 2020. [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.\n",
            "\n",
            "🔍 Themes:\n",
            "Diffusion models and denoising autoencoders\n",
            "\n",
            "🔍 Analyzing Chunk...\n",
            "\n",
            "🔍 Summary:\n",
            "[10] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1410.8516, 2014. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\n",
            "\n",
            "🔍 Connections:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CeFmZpQijaS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLCFWoIPjaVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "# Document processing\n",
        "import fitz  # PyMuPDF\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector database\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.embeddings import Embeddings\n",
        "\n",
        "# LLM\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import create_extraction_chain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Knowledge graph\n",
        "import pyvis\n",
        "from pyvis.network import Network\n"
      ],
      "metadata": {
        "id": "SwEF3FuigcZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Document Processing & Chunking\n",
        "The system first loads PDF documents using PyMuPDF (the fitz library) and extracts their text content. This is crucial because PDFs often contain formatting that needs to be properly parsed.\n",
        "Each document is then split into smaller chunks (default 1000 characters with 200 character overlap) using LangChain's RecursiveCharacterTextSplitter. This chunking is essential because:\n",
        "\n",
        "Most language models have context window limitations (typically 8K-128K tokens)\n",
        "Smaller chunks allow for more precise retrieval\n",
        "Overlapping chunks preserve context across chunk boundaries"
      ],
      "metadata": {
        "id": "E9Kwlftug5Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Vector Store Creation & Embedding\n",
        "The system uses SentenceTransformer to create embeddings for each document chunk. These embeddings capture the semantic meaning of each chunk as dense vectors.\n",
        "The embeddings are stored in a FAISS vector database, which enables efficient similarity search. When you ask a question, the system can quickly retrieve the most relevant chunks by comparing your query's embedding with the stored document embeddings."
      ],
      "metadata": {
        "id": "h-Sata6Ig_hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Metadata Extraction & Knowledge Graph\n",
        "The system uses an LLM to extract structured metadata from each paper, including:\n",
        "\n",
        "Title, authors, publication year\n",
        "Keywords and methodologies\n",
        "Abstract\n",
        "Cited papers\n",
        "\n",
        "This metadata forms the basis of a knowledge graph built with NetworkX, where:\n",
        "\n",
        "Papers, authors, journals, methodologies, and keywords are nodes\n",
        "Relationships (authored, cites, uses) are edges\n",
        "\n",
        "This graph representation allows visualization of relationships between papers and helps identify key authors, methodologies, and research themes across multiple papers."
      ],
      "metadata": {
        "id": "jhA6cB-LhA9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Analysis Capabilities\n",
        "\n",
        "Paper Summarization\n",
        "For each paper, the system generates a comprehensive summary covering:\n",
        "\n",
        "Main research questions\n",
        "Methodology\n",
        "Key findings\n",
        "Limitations and future work\n",
        "\n",
        "This helps quickly understand individual papers without reading the full text.\n",
        "\n",
        "\n",
        "Paper Comparison\n",
        "The system can compare multiple papers to identify:\n",
        "\n",
        "Common themes\n",
        "Differences in methodology\n",
        "Complementary or contradictory findings\n",
        "Research gaps\n",
        "\n",
        "\n",
        "Theme Extraction\n",
        "The system analyzes all papers to identify common themes, showing which papers address each theme and how they contribute to it.\n",
        "\n",
        "\n",
        "Question Answering\n",
        "When you ask a question:\n",
        "\n",
        "The system retrieves the most relevant chunks from across all papers\n",
        "It provides the chunks as context to the LLM\n",
        "The LLM synthesizes an answer based on this context, citing the relevant papers\n",
        "\n"
      ],
      "metadata": {
        "id": "rmkO0O2IhEJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Overcoming Document Length Limitations\n",
        "The system addresses document length challenges through:\n",
        "\n",
        "Chunking documents intelligently (with overlap)\n",
        "Using metadata extraction to capture key information regardless of document length\n",
        "Building a knowledge graph to represent relationships that might span across documents\n",
        "Using the LLM to synthesize information from multiple chunks"
      ],
      "metadata": {
        "id": "Fk75Fg3RhaZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies: The system requires several Python libraries:\n",
        "\n",
        "PyMuPDF for PDF processing\n",
        "LangChain for document processing and LLM chains\n",
        "SentenceTransformer for embeddings\n",
        "FAISS for vector storage\n",
        "NetworkX and PyVis for knowledge graph creation and visualization\n",
        "OpenAI for the LLM"
      ],
      "metadata": {
        "id": "EikMfiVWhdvl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OdUrxM1mgcpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQ1smJHWgcsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6zUaAwjfvNy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AcademicRAG:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-4o\"):\n",
        "        \"\"\"\n",
        "        Initialize the Academic RAG system.\n",
        "\n",
        "        Args:\n",
        "            api_key: OpenAI API key\n",
        "            model_name: LLM model to use\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=model_name,\n",
        "            openai_api_key=api_key,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Initialize document storage\n",
        "        self.documents = {}\n",
        "        self.chunks = {}\n",
        "        self.metadata = {}\n",
        "        self.knowledge_graph = nx.DiGraph()\n",
        "\n",
        "    def load_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to PDF file\n",
        "\n",
        "        Returns:\n",
        "            Full text content of the PDF\n",
        "        \"\"\"\n",
        "        doc_id = Path(file_path).stem\n",
        "        pdf_document = fitz.open(file_path)\n",
        "        text = \"\"\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "\n",
        "        # Store the full document\n",
        "        self.documents[doc_id] = text\n",
        "\n",
        "        print(f\"Loaded document: {doc_id} ({len(text)} characters)\")\n",
        "        return text\n",
        "\n",
        "    def load_multiple_pdfs(self, directory: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Load multiple PDFs from a directory.\n",
        "\n",
        "        Args:\n",
        "            directory: Directory containing PDF files\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to full text content\n",
        "        \"\"\"\n",
        "        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            file_path = os.path.join(directory, pdf_file)\n",
        "            self.load_pdf(file_path)\n",
        "\n",
        "        return self.documents\n",
        "\n",
        "    def chunk_document(self, doc_id: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Split document into chunks.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            List of document chunks with metadata\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found. Load it first.\")\n",
        "\n",
        "        text = self.documents[doc_id]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.create_documents([text])\n",
        "\n",
        "        # Add metadata to chunks\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata = {\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"source\": doc_id\n",
        "            }\n",
        "\n",
        "        self.chunks[doc_id] = chunks\n",
        "        print(f\"Split {doc_id} into {len(chunks)} chunks\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_all_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict[str, List[Document]]:\n",
        "        \"\"\"\n",
        "        Split all loaded documents into chunks.\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to lists of chunks\n",
        "        \"\"\"\n",
        "        for doc_id in self.documents:\n",
        "            self.chunk_document(doc_id, chunk_size, chunk_overlap)\n",
        "\n",
        "        return self.chunks\n",
        "\n",
        "    def build_vector_store(self):\n",
        "        \"\"\"\n",
        "        Build vector store from all document chunks.\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        for doc_chunks in self.chunks.values():\n",
        "            all_chunks.extend(doc_chunks)\n",
        "\n",
        "        # Create embedding class that adapts SentenceTransformer to LangChain\n",
        "        class STEmbeddings(Embeddings):\n",
        "            def __init__(self, model):\n",
        "                self.model = model\n",
        "\n",
        "            def embed_documents(self, texts):\n",
        "                return self.model.encode(texts).tolist()\n",
        "\n",
        "            def embed_query(self, text):\n",
        "                return self.model.encode(text).tolist()\n",
        "\n",
        "        embeddings = STEmbeddings(self.embedding_model)\n",
        "\n",
        "        # Create FAISS vector store\n",
        "        self.vector_store = FAISS.from_documents(all_chunks, embeddings)\n",
        "        print(f\"Built vector store with {len(all_chunks)} chunks\")\n",
        "\n",
        "    def extract_metadata(self):\n",
        "        \"\"\"\n",
        "        Extract metadata from papers using LLM.\n",
        "        \"\"\"\n",
        "        schema = {\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"string\"},\n",
        "                \"authors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"publication_year\": {\"type\": \"integer\"},\n",
        "                \"journal\": {\"type\": \"string\"},\n",
        "                \"abstract\": {\"type\": \"string\"},\n",
        "                \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"methodology\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"cited_papers\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "            },\n",
        "            \"required\": [\"title\", \"authors\"],\n",
        "        }\n",
        "\n",
        "        extraction_chain = create_extraction_chain(schema, self.llm)\n",
        "\n",
        "        for doc_id, text in self.documents.items():\n",
        "            # Use the first chunk plus any detected abstract section for metadata extraction\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", text, re.DOTALL)\n",
        "            abstract_text = abstract_match.group(1) if abstract_match else \"\"\n",
        "\n",
        "            input_text = first_chunk + \"\\n\\n\" + abstract_text\n",
        "\n",
        "            # Truncate if too long\n",
        "            if len(input_text) > 5000:\n",
        "                input_text = input_text[:5000]\n",
        "\n",
        "            try:\n",
        "                result = extraction_chain.invoke({\"input\": input_text})\n",
        "                metadata = result[\"extracted\"][0] if result[\"extracted\"] else {}\n",
        "                self.metadata[doc_id] = metadata\n",
        "                print(f\"Extracted metadata for {doc_id}: {metadata['title'] if 'title' in metadata else 'Unknown'}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting metadata for {doc_id}: {e}\")\n",
        "                self.metadata[doc_id] = {\"title\": doc_id, \"authors\": [\"Unknown\"]}\n",
        "\n",
        "    def build_knowledge_graph(self):\n",
        "        \"\"\"\n",
        "        Build knowledge graph from extracted paper metadata.\n",
        "        \"\"\"\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add papers as nodes\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = metadata.get(\"authors\", [\"Unknown\"])\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "            journal = metadata.get(\"journal\", \"Unknown\")\n",
        "\n",
        "            # Add paper node\n",
        "            G.add_node(title,\n",
        "                       type=\"paper\",\n",
        "                       authors=authors,\n",
        "                       year=year,\n",
        "                       journal=journal,\n",
        "                       doc_id=doc_id)\n",
        "\n",
        "            # Add author nodes and connections\n",
        "            for author in authors:\n",
        "                if not G.has_node(author):\n",
        "                    G.add_node(author, type=\"author\")\n",
        "                G.add_edge(author, title, type=\"authored\")\n",
        "\n",
        "            # Add journal node and connection\n",
        "            if journal != \"Unknown\":\n",
        "                if not G.has_node(journal):\n",
        "                    G.add_node(journal, type=\"journal\")\n",
        "                G.add_edge(title, journal, type=\"published_in\")\n",
        "\n",
        "            # Add methodology nodes\n",
        "            methods = metadata.get(\"methodology\", [])\n",
        "            for method in methods:\n",
        "                if not G.has_node(method):\n",
        "                    G.add_node(method, type=\"methodology\")\n",
        "                G.add_edge(title, method, type=\"uses\")\n",
        "\n",
        "            # Add keyword nodes\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "            for keyword in keywords:\n",
        "                if not G.has_node(keyword):\n",
        "                    G.add_node(keyword, type=\"keyword\")\n",
        "                G.add_edge(title, keyword, type=\"contains\")\n",
        "\n",
        "        # Add citation links\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            source_title = metadata.get(\"title\", doc_id)\n",
        "            cited_papers = metadata.get(\"cited_papers\", [])\n",
        "\n",
        "            for cited_paper in cited_papers:\n",
        "                # Try to match with existing paper nodes\n",
        "                matching_papers = [node for node in G.nodes() if G.nodes[node].get(\"type\") == \"paper\" and cited_paper.lower() in node.lower()]\n",
        "\n",
        "                if matching_papers:\n",
        "                    G.add_edge(source_title, matching_papers[0], type=\"cites\")\n",
        "                else:\n",
        "                    # Add as a new node if not found\n",
        "                    G.add_node(cited_paper, type=\"external_paper\")\n",
        "                    G.add_edge(source_title, cited_paper, type=\"cites\")\n",
        "\n",
        "        self.knowledge_graph = G\n",
        "        print(f\"Built knowledge graph with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "\n",
        "        return G\n",
        "\n",
        "    def visualize_knowledge_graph(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        \"\"\"\n",
        "        Visualize the knowledge graph.\n",
        "\n",
        "        Args:\n",
        "            output_file: Path to save the HTML visualization\n",
        "        \"\"\"\n",
        "        G = self.knowledge_graph\n",
        "\n",
        "        # Create pyvis network\n",
        "        net = Network(height=\"750px\", width=\"100%\", notebook=False, directed=True)\n",
        "\n",
        "        # Node colors by type\n",
        "        color_map = {\n",
        "            \"paper\": \"#4285F4\",  # blue\n",
        "            \"author\": \"#EA4335\",  # red\n",
        "            \"journal\": \"#FBBC05\",  # yellow\n",
        "            \"methodology\": \"#34A853\",  # green\n",
        "            \"keyword\": \"#800080\",  # purple\n",
        "            \"external_paper\": \"#A0A0A0\"  # gray\n",
        "        }\n",
        "\n",
        "        # Add nodes\n",
        "        for node in G.nodes():\n",
        "            node_type = G.nodes[node].get(\"type\", \"unknown\")\n",
        "            title = f\"Type: {node_type}\"\n",
        "\n",
        "            if node_type == \"paper\":\n",
        "                authors = \", \".join(G.nodes[node].get(\"authors\", [\"Unknown\"]))\n",
        "                year = G.nodes[node].get(\"year\", \"Unknown\")\n",
        "                journal = G.nodes[node].get(\"journal\", \"Unknown\")\n",
        "                title = f\"Paper: {node}\\nAuthors: {authors}\\nYear: {year}\\nJournal: {journal}\"\n",
        "\n",
        "            net.add_node(node,\n",
        "                         title=title,\n",
        "                         color=color_map.get(node_type, \"#000000\"),\n",
        "                         size=20 if node_type == \"paper\" else 10)\n",
        "\n",
        "        # Add edges\n",
        "        edge_colors = {\n",
        "            \"authored\": \"#EA4335\",\n",
        "            \"published_in\": \"#FBBC05\",\n",
        "            \"uses\": \"#34A853\",\n",
        "            \"contains\": \"#800080\",\n",
        "            \"cites\": \"#4285F4\"\n",
        "        }\n",
        "\n",
        "        for source, target, data in G.edges(data=True):\n",
        "            edge_type = data.get(\"type\", \"unknown\")\n",
        "            net.add_edge(source, target,\n",
        "                         title=edge_type,\n",
        "                         color=edge_colors.get(edge_type, \"#000000\"),\n",
        "                         arrows=\"to\")\n",
        "\n",
        "        # Set physics layout\n",
        "        net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08)\n",
        "        net.toggle_physics(True)\n",
        "\n",
        "        # Save visualization\n",
        "        net.show(output_file)\n",
        "        print(f\"Knowledge graph visualization saved to {output_file}\")\n",
        "\n",
        "    def retrieve_similar_chunks(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve similar chunks for a query.\n",
        "\n",
        "        Args:\n",
        "            query: Query text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            List of (document, similarity) tuples\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not built. Call build_vector_store() first.\")\n",
        "\n",
        "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_paper_summary(self, doc_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a summary for a specific paper.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "\n",
        "        Returns:\n",
        "            Summary text\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Summarize the following academic paper excerpt:\n",
        "\n",
        "            {text}\n",
        "\n",
        "            Provide a comprehensive summary that includes:\n",
        "            1. The main research question/problem\n",
        "            2. The methodology used\n",
        "            3. Key findings and contributions\n",
        "            4. Limitations and future work\n",
        "\n",
        "            Summary:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Use the first few chunks (up to 10,000 characters)\n",
        "        chunks = self.chunks.get(doc_id, [])\n",
        "        combined_text = \"\"\n",
        "        for chunk in chunks:\n",
        "            combined_text += chunk.page_content + \"\\n\\n\"\n",
        "            if len(combined_text) > 10000:\n",
        "                break\n",
        "        combined_text = combined_text[:10000]\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        summary = chain.invoke({\"text\": combined_text})\n",
        "\n",
        "        return summary[\"text\"]\n",
        "\n",
        "    def compare_papers(self, doc_ids: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Compare multiple papers.\n",
        "\n",
        "        Args:\n",
        "            doc_ids: List of document identifiers\n",
        "\n",
        "        Returns:\n",
        "            Comparison text\n",
        "        \"\"\"\n",
        "        # Validate all doc_ids\n",
        "        for doc_id in doc_ids:\n",
        "            if doc_id not in self.documents:\n",
        "                raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        # Get metadata for each paper\n",
        "        paper_info = []\n",
        "        for doc_id in doc_ids:\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "\n",
        "            # Get a brief summary by using first chunk\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            paper_info.append(f\"Title: {title}\\nAuthors: {authors}\\nYear: {year}\\nExcerpt: {first_chunk[:500]}...\")\n",
        "\n",
        "        # Generate comparison using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Compare and contrast the following academic papers:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Provide a detailed comparison including:\n",
        "            1. Common themes and research questions\n",
        "            2. Differences in methodology\n",
        "            3. Complementary or contradictory findings\n",
        "            4. How they build upon each other's work\n",
        "            5. Potential gaps or areas for future research\n",
        "\n",
        "            Comparison:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        comparison = chain.invoke({\"papers\": \"\\n\\n\".join(paper_info)})\n",
        "\n",
        "        return comparison[\"text\"]\n",
        "\n",
        "    def answer_question(self, question: str, k: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Answer a question based on the papers.\n",
        "\n",
        "        Args:\n",
        "            question: Question text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Answer text\n",
        "        \"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = self.retrieve_similar_chunks(question, k=k)\n",
        "\n",
        "        # Prepare context from chunks\n",
        "        context = []\n",
        "        for chunk, score in relevant_chunks:\n",
        "            doc_id = chunk.metadata[\"doc_id\"]\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            context.append(f\"From '{title}' by {authors}:\\n{chunk.page_content}\")\n",
        "\n",
        "        # Generate answer using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"question\", \"context\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant with expertise in synthesizing information from academic papers.\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Here are relevant excerpts from academic papers:\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Please provide a comprehensive answer to the question based on the given context.\n",
        "            Cite the papers you're referencing in your response.\n",
        "            If the information provided is insufficient to answer the question, clearly state what's missing.\n",
        "\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        answer = chain.invoke({\"question\": question, \"context\": \"\\n\\n\".join(context)})\n",
        "\n",
        "        return answer[\"text\"]\n",
        "\n",
        "    def extract_themes(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract common themes across papers.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of themes with supporting evidence\n",
        "        \"\"\"\n",
        "        # Prepare paper information\n",
        "        papers_info = []\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            abstract = metadata.get(\"abstract\", \"\")\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "\n",
        "            if not abstract:\n",
        "                # Try to find abstract in the document\n",
        "                doc_text = self.documents[doc_id]\n",
        "                abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", doc_text, re.DOTALL)\n",
        "                abstract = abstract_match.group(1) if abstract_match else \"No abstract found\"\n",
        "\n",
        "            papers_info.append(f\"Title: {title}\\nAbstract: {abstract}\\nKeywords: {', '.join(keywords)}\")\n",
        "\n",
        "        # Generate themes using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Analyze the following academic papers and extract common themes:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Identify 3-5 major themes across these papers. For each theme:\n",
        "            1. Provide a clear name and description\n",
        "            2. List which papers address this theme\n",
        "            3. Describe how each paper contributes to or approaches this theme\n",
        "            4. Note any contradictions or complementary findings within the theme\n",
        "\n",
        "            Format your response as a JSON structure where each theme is a key, and the value contains the description and paper relationships.\n",
        "\n",
        "            Themes:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        themes_result = chain.invoke({\"papers\": \"\\n\\n\".join(papers_info)})\n",
        "\n",
        "        # Extract themes from the text response\n",
        "        themes_text = themes_result[\"text\"]\n",
        "\n",
        "        # Use LLM to convert to structured format\n",
        "        extraction_prompt = PromptTemplate(\n",
        "            input_variables=[\"themes_text\"],\n",
        "            template=\"\"\"\n",
        "            Convert the following themes analysis into a proper JSON structure:\n",
        "\n",
        "            {themes_text}\n",
        "\n",
        "            JSON format:\n",
        "            ```\n",
        "            {{\n",
        "                \"Theme 1 Name\": {{\n",
        "                    \"description\": \"Description of theme 1\",\n",
        "                    \"papers\": [\n",
        "                        {{\n",
        "                            \"title\": \"Paper Title 1\",\n",
        "                            \"contribution\": \"How Paper 1 contributes to this theme\"\n",
        "                        }},\n",
        "                        ...\n",
        "                    ]\n",
        "                }},\n",
        "                ...\n",
        "            }}\n",
        "            ```\n",
        "\n",
        "            Only respond with the valid JSON, nothing else.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=extraction_prompt)\n",
        "        json_result = chain.invoke({\"themes_text\": themes_text})\n",
        "\n",
        "        try:\n",
        "            # Extract JSON from the response (it might be wrapped in backticks)\n",
        "            json_text = re.search(r'```json\\n(.*?)\\n```', json_result[\"text\"], re.DOTALL)\n",
        "            if json_text:\n",
        "                json_str = json_text.group(1)\n",
        "            else:\n",
        "                json_str = json_result[\"text\"]\n",
        "\n",
        "            # Clean up any extra backticks\n",
        "            json_str = json_str.replace('```', '').strip()\n",
        "\n",
        "            import json\n",
        "            themes = json.loads(json_str)\n",
        "            return themes\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing themes JSON: {e}\")\n",
        "            return {\"error\": \"Could not parse themes\", \"raw_text\": themes_text}\n",
        "\n",
        "    def run_full_analysis(self, pdf_directory: str, output_dir: str = \"./output\"):\n",
        "        \"\"\"\n",
        "        Run full analysis pipeline on a directory of PDFs.\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_dir: Directory to save output files\n",
        "        \"\"\"\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Load PDFs\n",
        "        print(\"Loading PDFs...\")\n",
        "        self.load_multiple_pdfs(pdf_directory)\n",
        "\n",
        "        # 2. Chunk documents\n",
        "        print(\"Chunking documents...\")\n",
        "        self.chunk_all_documents()\n",
        "\n",
        "        # 3. Build vector store\n",
        "        print(\"Building vector store...\")\n",
        "        self.build_vector_store()\n",
        "\n",
        "        # 4. Extract metadata\n",
        "        print(\"Extracting metadata...\")\n",
        "        self.extract_metadata()\n",
        "\n",
        "        # 5. Build knowledge graph\n",
        "        print(\"Building knowledge graph...\")\n",
        "        self.build_knowledge_graph()\n",
        "\n",
        "        # 6. Visualize knowledge graph\n",
        "        print(\"Visualizing knowledge graph...\")\n",
        "        self.visualize_knowledge_graph(os.path.join(output_dir, \"knowledge_graph.html\"))\n",
        "\n",
        "        # 7. Generate summaries for each paper\n",
        "        print(\"Generating summaries...\")\n",
        "        summaries = {}\n",
        "        for doc_id in self.documents:\n",
        "            summaries[doc_id] = self.generate_paper_summary(doc_id)\n",
        "\n",
        "            # Save individual summaries\n",
        "            with open(os.path.join(output_dir, f\"{doc_id}_summary.txt\"), \"w\") as f:\n",
        "                f.write(summaries[doc_id])\n",
        "\n",
        "        # 8. Compare all papers\n",
        "        print(\"Comparing papers...\")\n",
        "        comparison = self.compare_papers(list(self.documents.keys()))\n",
        "        with open(os.path.join(output_dir, \"paper_comparison.txt\"), \"w\") as f:\n",
        "            f.write(comparison)\n",
        "\n",
        "        # 9. Extract themes\n",
        "        print(\"Extracting themes...\")\n",
        "        themes = self.extract_themes()\n",
        "\n",
        "        import json\n",
        "        with open(os.path.join(output_dir, \"themes.json\"), \"w\") as f:\n",
        "            json.dump(themes, f, indent=2)\n",
        "\n",
        "        print(f\"Analysis complete! Results saved in {output_dir}\")\n",
        "\n",
        "        return {\n",
        "            \"summaries\": summaries,\n",
        "            \"comparison\": comparison,\n",
        "            \"themes\": themes\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your OpenAI API key\n",
        "    OPENAI_API_KEY = \"your-api-key-here\"\n",
        "\n",
        "    # Initialize the RAG system\n",
        "    rag = AcademicRAG(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # Run full analysis\n",
        "    results = rag.run_full_analysis(\"./papers\")\n",
        "\n",
        "    # Ask questions\n",
        "    question = \"What are the main methodologies used across these papers and how do they compare?\"\n",
        "    answer = rag.answer_question(question)\n",
        "    print(f\"Q: {question}\\nA: {answer}\")\n",
        "\n",
        "    # Get more paper relationships\n",
        "    print(\"Paper relationships:\")\n",
        "    for source, target, data in rag.knowledge_graph.edges(data=True):\n",
        "        if data.get(\"type\") == \"cites\":\n",
        "            print(f\"  {source} cites {target}\")"
      ],
      "metadata": {
        "id": "rGFG6eGPgmQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}