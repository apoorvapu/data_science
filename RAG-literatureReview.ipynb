{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxL4884hbl8GxT+gmyego1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/RAG-literatureReview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI in Research: using Large Language Models (LLMs) to enhance and streamline the academic literature review process.\n",
        "  \n",
        "    Techniques for summarizing articles, identifying connections across papers (authors, references, methods), and uncovering key themes.\n",
        "    Constructing a knowledge graph to visualize and interpret findings."
      ],
      "metadata": {
        "id": "vq3zIOCofySQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf requests"
      ],
      "metadata": {
        "id": "OUPE-P03hm6L",
        "outputId": "f03b1893-cbdd-49fe-805b-f3bde7cc2c1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download any 5 papers of diffusion model and convert their pdf to .txt files"
      ],
      "metadata": {
        "id": "MbRT1J6xiGMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Sample arXiv paper IDs related to diffusion models\n",
        "arxiv_ids = [\n",
        "    \"2006.11239\",  # Denoising Diffusion Probabilistic Models\n",
        "    \"2105.05233\",  # Improved Denoising Diffusion Probabilistic Models\n",
        "    \"2204.06125\",  # Latent Diffusion Models\n",
        "    \"2112.10752\",  # Glide: Towards Photorealistic Image Generation and Editing\n",
        "    \"2305.08891\"   # Consistency Models\n",
        "]\n",
        "\n",
        "def download_pdf(arxiv_id, output_folder):\n",
        "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "    pdf_path = os.path.join(output_folder, f\"{arxiv_id}.pdf\")\n",
        "    response = requests.get(url)\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {arxiv_id}\")\n",
        "    return pdf_path\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"Converted to text: {txt_path}\")\n",
        "\n",
        "def main():\n",
        "    data_dir = \"data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for arxiv_id in arxiv_ids:\n",
        "        pdf_path = download_pdf(arxiv_id, data_dir)\n",
        "        txt_path = os.path.join(data_dir, f\"{arxiv_id}.txt\")\n",
        "        pdf_to_text(pdf_path, txt_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Yq0vtxjXhZcm",
        "outputId": "3ea41cf5-0523-4db3-deec-79f12653722e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 2006.11239\n",
            "Converted to text: data/2006.11239.txt\n",
            "Downloaded 2105.05233\n",
            "Converted to text: data/2105.05233.txt\n",
            "Downloaded 2204.06125\n",
            "Converted to text: data/2204.06125.txt\n",
            "Downloaded 2112.10752\n",
            "Converted to text: data/2112.10752.txt\n",
            "Downloaded 2305.08891\n",
            "Converted to text: data/2305.08891.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu huggingface-hub spacy networkx pyvis\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install -U langchain-community\n",
        "!pip install -U langchain"
      ],
      "metadata": {
        "id": "TZ3toKE9h_NX",
        "outputId": "b1347915-e5ce-4e8f-bb57-8d58354a78eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis) (4.0.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-huggingface"
      ],
      "metadata": {
        "id": "mPot-K9akGNq",
        "outputId": "1512a01f-33ab-4189-a00d-276112b43fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.30.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.52)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.13.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.3.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.11.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-huggingface\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed langchain-huggingface-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "52549dbf39fc48fb849f069b00f844ec"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import spacy\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub  # or OpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ---------------------------\n",
        "# Load and Embed Documents\n",
        "# ---------------------------\n",
        "def load_documents(directory):\n",
        "    filepaths = glob.glob(f\"{directory}/*.txt\")\n",
        "    docs = []\n",
        "    for path in filepaths:\n",
        "        loader = TextLoader(path)\n",
        "        docs.extend(loader.load())\n",
        "    return docs\n",
        "\n",
        "def create_vectorstore(documents):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = FAISS.from_documents(docs, embeddings)\n",
        "    return db\n",
        "\n",
        "# ---------------------------\n",
        "# Summarize & Thematic Analysis\n",
        "# ---------------------------\n",
        "def summarize_and_analyze(db, query, llm):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
        "    return qa_chain.run(query)\n",
        "\n",
        "# ---------------------------\n",
        "# Build Knowledge Graph\n",
        "# ---------------------------\n",
        "def extract_entities(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "def build_knowledge_graph(all_texts, output_path=\"output/knowledge_graph.html\"):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for text in all_texts:\n",
        "        entities = extract_entities(text, nlp)\n",
        "        for i in range(len(entities)):\n",
        "            for j in range(i+1, len(entities)):\n",
        "                e1, e2 = entities[i][0], entities[j][0]\n",
        "                G.add_edge(e1, e2)\n",
        "\n",
        "    net = Network(notebook=False)\n",
        "    net.from_nx(G)\n",
        "    net.show(output_path)"
      ],
      "metadata": {
        "id": "SRX9dj1QhA8Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and embed academic papers\n",
        "docs = load_documents(\"data\")\n",
        "db = create_vectorstore(docs)\n",
        "\n",
        "# Load your LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    #huggingfacehub_api_token=\"your_actual_token_here\",  # Or use an env var\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n",
        ")\n",
        "\n",
        "queries = {\n",
        "        \"summary\": \"Summarize this research literature in 5 bullet points.\",\n",
        "        \"methods\": \"What methods are commonly used in these papers?\",\n",
        "        \"connections\": \"Identify connections between authors and references.\",\n",
        "        \"themes\": \"What are the main research themes discussed?\"\n",
        "    }\n",
        "\n",
        "os.makedirs(\"output/summaries\", exist_ok=True)\n",
        "all_texts = []\n",
        "\n",
        "for name, question in queries.items():\n",
        "        print(f\"\\n--- {name.upper()} ---\")\n",
        "        result = summarize_and_analyze(db, question, llm)\n",
        "        print(result)\n",
        "        all_texts.append(result)\n",
        "        with open(f\"output/summaries/{name}.txt\", \"w\") as f:\n",
        "            f.write(result)\n",
        "\n",
        "# Build Knowledge Graph\n",
        "build_knowledge_graph(all_texts)"
      ],
      "metadata": {
        "id": "qefGM5XUjvm3",
        "outputId": "0a5e9017-50ed-4c30-c772-e8fb4c8c66a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2cb54da7adef>:28: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for HuggingFaceEndpoint\n  Value error, Parameters {'temperature'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'repo_id': 'google/flan-...0.5, 'max_length': 512}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4a8c9a00d25a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load your LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m llm = HuggingFaceEndpoint(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/flan-t5-large\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#huggingfacehub_api_token=\"your_actual_token_here\",  # Or use an env var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceEndpoint\n  Value error, Parameters {'temperature'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'repo_id': 'google/flan-...0.5, 'max_length': 512}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXxZ9j6Sj-xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db.similarity_search(\"What is the forward and reverse process in diffusion models?\", k=3)\n",
        "for doc in docs:\n",
        "    print(doc.page_content[:300], \"\\n---\")\n"
      ],
      "metadata": {
        "id": "rYpAApQHi6HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the role of the variational bound in training diffusion models\"\n",
        "answer = qa_chain.run(query)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "hmTjFDOuhCDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def check_similarity(answer, source_docs):\n",
        "    texts = [doc.page_content for doc in source_docs] + [answer]\n",
        "    tfidf = TfidfVectorizer().fit_transform(texts)\n",
        "    sim_matrix = cosine_similarity(tfidf[-1:], tfidf[:-1])\n",
        "    return sim_matrix.max()\n",
        "\n",
        "score = check_similarity(answer, docs)\n",
        "print(f\"Answer similarity score: {score:.2f}\")\n"
      ],
      "metadata": {
        "id": "6REN-scWjLfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = qa_chain.run(\"Summarize the key idea of the paper arXiv:2006.11239\")\n"
      ],
      "metadata": {
        "id": "1R1gCSFtjLjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "central_nodes = sorted(nx.degree_centrality(G).items(), key=lambda x: -x[1])[:10]\n",
        "print(\"Top entities:\", [node for node, score in central_nodes])\n"
      ],
      "metadata": {
        "id": "L7nCKW_TjLlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zbv3cIzljaPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CeFmZpQijaS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLCFWoIPjaVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "# Document processing\n",
        "import fitz  # PyMuPDF\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector database\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.embeddings import Embeddings\n",
        "\n",
        "# LLM\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import create_extraction_chain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Knowledge graph\n",
        "import pyvis\n",
        "from pyvis.network import Network\n"
      ],
      "metadata": {
        "id": "SwEF3FuigcZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Document Processing & Chunking\n",
        "The system first loads PDF documents using PyMuPDF (the fitz library) and extracts their text content. This is crucial because PDFs often contain formatting that needs to be properly parsed.\n",
        "Each document is then split into smaller chunks (default 1000 characters with 200 character overlap) using LangChain's RecursiveCharacterTextSplitter. This chunking is essential because:\n",
        "\n",
        "Most language models have context window limitations (typically 8K-128K tokens)\n",
        "Smaller chunks allow for more precise retrieval\n",
        "Overlapping chunks preserve context across chunk boundaries"
      ],
      "metadata": {
        "id": "E9Kwlftug5Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Vector Store Creation & Embedding\n",
        "The system uses SentenceTransformer to create embeddings for each document chunk. These embeddings capture the semantic meaning of each chunk as dense vectors.\n",
        "The embeddings are stored in a FAISS vector database, which enables efficient similarity search. When you ask a question, the system can quickly retrieve the most relevant chunks by comparing your query's embedding with the stored document embeddings."
      ],
      "metadata": {
        "id": "h-Sata6Ig_hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Metadata Extraction & Knowledge Graph\n",
        "The system uses an LLM to extract structured metadata from each paper, including:\n",
        "\n",
        "Title, authors, publication year\n",
        "Keywords and methodologies\n",
        "Abstract\n",
        "Cited papers\n",
        "\n",
        "This metadata forms the basis of a knowledge graph built with NetworkX, where:\n",
        "\n",
        "Papers, authors, journals, methodologies, and keywords are nodes\n",
        "Relationships (authored, cites, uses) are edges\n",
        "\n",
        "This graph representation allows visualization of relationships between papers and helps identify key authors, methodologies, and research themes across multiple papers."
      ],
      "metadata": {
        "id": "jhA6cB-LhA9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Analysis Capabilities\n",
        "\n",
        "Paper Summarization\n",
        "For each paper, the system generates a comprehensive summary covering:\n",
        "\n",
        "Main research questions\n",
        "Methodology\n",
        "Key findings\n",
        "Limitations and future work\n",
        "\n",
        "This helps quickly understand individual papers without reading the full text.\n",
        "\n",
        "\n",
        "Paper Comparison\n",
        "The system can compare multiple papers to identify:\n",
        "\n",
        "Common themes\n",
        "Differences in methodology\n",
        "Complementary or contradictory findings\n",
        "Research gaps\n",
        "\n",
        "\n",
        "Theme Extraction\n",
        "The system analyzes all papers to identify common themes, showing which papers address each theme and how they contribute to it.\n",
        "\n",
        "\n",
        "Question Answering\n",
        "When you ask a question:\n",
        "\n",
        "The system retrieves the most relevant chunks from across all papers\n",
        "It provides the chunks as context to the LLM\n",
        "The LLM synthesizes an answer based on this context, citing the relevant papers\n",
        "\n"
      ],
      "metadata": {
        "id": "rmkO0O2IhEJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Overcoming Document Length Limitations\n",
        "The system addresses document length challenges through:\n",
        "\n",
        "Chunking documents intelligently (with overlap)\n",
        "Using metadata extraction to capture key information regardless of document length\n",
        "Building a knowledge graph to represent relationships that might span across documents\n",
        "Using the LLM to synthesize information from multiple chunks"
      ],
      "metadata": {
        "id": "Fk75Fg3RhaZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies: The system requires several Python libraries:\n",
        "\n",
        "PyMuPDF for PDF processing\n",
        "LangChain for document processing and LLM chains\n",
        "SentenceTransformer for embeddings\n",
        "FAISS for vector storage\n",
        "NetworkX and PyVis for knowledge graph creation and visualization\n",
        "OpenAI for the LLM"
      ],
      "metadata": {
        "id": "EikMfiVWhdvl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OdUrxM1mgcpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQ1smJHWgcsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6zUaAwjfvNy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AcademicRAG:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-4o\"):\n",
        "        \"\"\"\n",
        "        Initialize the Academic RAG system.\n",
        "\n",
        "        Args:\n",
        "            api_key: OpenAI API key\n",
        "            model_name: LLM model to use\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=model_name,\n",
        "            openai_api_key=api_key,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Initialize document storage\n",
        "        self.documents = {}\n",
        "        self.chunks = {}\n",
        "        self.metadata = {}\n",
        "        self.knowledge_graph = nx.DiGraph()\n",
        "\n",
        "    def load_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to PDF file\n",
        "\n",
        "        Returns:\n",
        "            Full text content of the PDF\n",
        "        \"\"\"\n",
        "        doc_id = Path(file_path).stem\n",
        "        pdf_document = fitz.open(file_path)\n",
        "        text = \"\"\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "\n",
        "        # Store the full document\n",
        "        self.documents[doc_id] = text\n",
        "\n",
        "        print(f\"Loaded document: {doc_id} ({len(text)} characters)\")\n",
        "        return text\n",
        "\n",
        "    def load_multiple_pdfs(self, directory: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Load multiple PDFs from a directory.\n",
        "\n",
        "        Args:\n",
        "            directory: Directory containing PDF files\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to full text content\n",
        "        \"\"\"\n",
        "        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            file_path = os.path.join(directory, pdf_file)\n",
        "            self.load_pdf(file_path)\n",
        "\n",
        "        return self.documents\n",
        "\n",
        "    def chunk_document(self, doc_id: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Split document into chunks.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            List of document chunks with metadata\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found. Load it first.\")\n",
        "\n",
        "        text = self.documents[doc_id]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.create_documents([text])\n",
        "\n",
        "        # Add metadata to chunks\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata = {\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"source\": doc_id\n",
        "            }\n",
        "\n",
        "        self.chunks[doc_id] = chunks\n",
        "        print(f\"Split {doc_id} into {len(chunks)} chunks\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_all_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict[str, List[Document]]:\n",
        "        \"\"\"\n",
        "        Split all loaded documents into chunks.\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Size of each chunk\n",
        "            chunk_overlap: Overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to lists of chunks\n",
        "        \"\"\"\n",
        "        for doc_id in self.documents:\n",
        "            self.chunk_document(doc_id, chunk_size, chunk_overlap)\n",
        "\n",
        "        return self.chunks\n",
        "\n",
        "    def build_vector_store(self):\n",
        "        \"\"\"\n",
        "        Build vector store from all document chunks.\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        for doc_chunks in self.chunks.values():\n",
        "            all_chunks.extend(doc_chunks)\n",
        "\n",
        "        # Create embedding class that adapts SentenceTransformer to LangChain\n",
        "        class STEmbeddings(Embeddings):\n",
        "            def __init__(self, model):\n",
        "                self.model = model\n",
        "\n",
        "            def embed_documents(self, texts):\n",
        "                return self.model.encode(texts).tolist()\n",
        "\n",
        "            def embed_query(self, text):\n",
        "                return self.model.encode(text).tolist()\n",
        "\n",
        "        embeddings = STEmbeddings(self.embedding_model)\n",
        "\n",
        "        # Create FAISS vector store\n",
        "        self.vector_store = FAISS.from_documents(all_chunks, embeddings)\n",
        "        print(f\"Built vector store with {len(all_chunks)} chunks\")\n",
        "\n",
        "    def extract_metadata(self):\n",
        "        \"\"\"\n",
        "        Extract metadata from papers using LLM.\n",
        "        \"\"\"\n",
        "        schema = {\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"string\"},\n",
        "                \"authors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"publication_year\": {\"type\": \"integer\"},\n",
        "                \"journal\": {\"type\": \"string\"},\n",
        "                \"abstract\": {\"type\": \"string\"},\n",
        "                \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"methodology\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                \"cited_papers\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "            },\n",
        "            \"required\": [\"title\", \"authors\"],\n",
        "        }\n",
        "\n",
        "        extraction_chain = create_extraction_chain(schema, self.llm)\n",
        "\n",
        "        for doc_id, text in self.documents.items():\n",
        "            # Use the first chunk plus any detected abstract section for metadata extraction\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", text, re.DOTALL)\n",
        "            abstract_text = abstract_match.group(1) if abstract_match else \"\"\n",
        "\n",
        "            input_text = first_chunk + \"\\n\\n\" + abstract_text\n",
        "\n",
        "            # Truncate if too long\n",
        "            if len(input_text) > 5000:\n",
        "                input_text = input_text[:5000]\n",
        "\n",
        "            try:\n",
        "                result = extraction_chain.invoke({\"input\": input_text})\n",
        "                metadata = result[\"extracted\"][0] if result[\"extracted\"] else {}\n",
        "                self.metadata[doc_id] = metadata\n",
        "                print(f\"Extracted metadata for {doc_id}: {metadata['title'] if 'title' in metadata else 'Unknown'}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting metadata for {doc_id}: {e}\")\n",
        "                self.metadata[doc_id] = {\"title\": doc_id, \"authors\": [\"Unknown\"]}\n",
        "\n",
        "    def build_knowledge_graph(self):\n",
        "        \"\"\"\n",
        "        Build knowledge graph from extracted paper metadata.\n",
        "        \"\"\"\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add papers as nodes\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = metadata.get(\"authors\", [\"Unknown\"])\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "            journal = metadata.get(\"journal\", \"Unknown\")\n",
        "\n",
        "            # Add paper node\n",
        "            G.add_node(title,\n",
        "                       type=\"paper\",\n",
        "                       authors=authors,\n",
        "                       year=year,\n",
        "                       journal=journal,\n",
        "                       doc_id=doc_id)\n",
        "\n",
        "            # Add author nodes and connections\n",
        "            for author in authors:\n",
        "                if not G.has_node(author):\n",
        "                    G.add_node(author, type=\"author\")\n",
        "                G.add_edge(author, title, type=\"authored\")\n",
        "\n",
        "            # Add journal node and connection\n",
        "            if journal != \"Unknown\":\n",
        "                if not G.has_node(journal):\n",
        "                    G.add_node(journal, type=\"journal\")\n",
        "                G.add_edge(title, journal, type=\"published_in\")\n",
        "\n",
        "            # Add methodology nodes\n",
        "            methods = metadata.get(\"methodology\", [])\n",
        "            for method in methods:\n",
        "                if not G.has_node(method):\n",
        "                    G.add_node(method, type=\"methodology\")\n",
        "                G.add_edge(title, method, type=\"uses\")\n",
        "\n",
        "            # Add keyword nodes\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "            for keyword in keywords:\n",
        "                if not G.has_node(keyword):\n",
        "                    G.add_node(keyword, type=\"keyword\")\n",
        "                G.add_edge(title, keyword, type=\"contains\")\n",
        "\n",
        "        # Add citation links\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            source_title = metadata.get(\"title\", doc_id)\n",
        "            cited_papers = metadata.get(\"cited_papers\", [])\n",
        "\n",
        "            for cited_paper in cited_papers:\n",
        "                # Try to match with existing paper nodes\n",
        "                matching_papers = [node for node in G.nodes() if G.nodes[node].get(\"type\") == \"paper\" and cited_paper.lower() in node.lower()]\n",
        "\n",
        "                if matching_papers:\n",
        "                    G.add_edge(source_title, matching_papers[0], type=\"cites\")\n",
        "                else:\n",
        "                    # Add as a new node if not found\n",
        "                    G.add_node(cited_paper, type=\"external_paper\")\n",
        "                    G.add_edge(source_title, cited_paper, type=\"cites\")\n",
        "\n",
        "        self.knowledge_graph = G\n",
        "        print(f\"Built knowledge graph with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "\n",
        "        return G\n",
        "\n",
        "    def visualize_knowledge_graph(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        \"\"\"\n",
        "        Visualize the knowledge graph.\n",
        "\n",
        "        Args:\n",
        "            output_file: Path to save the HTML visualization\n",
        "        \"\"\"\n",
        "        G = self.knowledge_graph\n",
        "\n",
        "        # Create pyvis network\n",
        "        net = Network(height=\"750px\", width=\"100%\", notebook=False, directed=True)\n",
        "\n",
        "        # Node colors by type\n",
        "        color_map = {\n",
        "            \"paper\": \"#4285F4\",  # blue\n",
        "            \"author\": \"#EA4335\",  # red\n",
        "            \"journal\": \"#FBBC05\",  # yellow\n",
        "            \"methodology\": \"#34A853\",  # green\n",
        "            \"keyword\": \"#800080\",  # purple\n",
        "            \"external_paper\": \"#A0A0A0\"  # gray\n",
        "        }\n",
        "\n",
        "        # Add nodes\n",
        "        for node in G.nodes():\n",
        "            node_type = G.nodes[node].get(\"type\", \"unknown\")\n",
        "            title = f\"Type: {node_type}\"\n",
        "\n",
        "            if node_type == \"paper\":\n",
        "                authors = \", \".join(G.nodes[node].get(\"authors\", [\"Unknown\"]))\n",
        "                year = G.nodes[node].get(\"year\", \"Unknown\")\n",
        "                journal = G.nodes[node].get(\"journal\", \"Unknown\")\n",
        "                title = f\"Paper: {node}\\nAuthors: {authors}\\nYear: {year}\\nJournal: {journal}\"\n",
        "\n",
        "            net.add_node(node,\n",
        "                         title=title,\n",
        "                         color=color_map.get(node_type, \"#000000\"),\n",
        "                         size=20 if node_type == \"paper\" else 10)\n",
        "\n",
        "        # Add edges\n",
        "        edge_colors = {\n",
        "            \"authored\": \"#EA4335\",\n",
        "            \"published_in\": \"#FBBC05\",\n",
        "            \"uses\": \"#34A853\",\n",
        "            \"contains\": \"#800080\",\n",
        "            \"cites\": \"#4285F4\"\n",
        "        }\n",
        "\n",
        "        for source, target, data in G.edges(data=True):\n",
        "            edge_type = data.get(\"type\", \"unknown\")\n",
        "            net.add_edge(source, target,\n",
        "                         title=edge_type,\n",
        "                         color=edge_colors.get(edge_type, \"#000000\"),\n",
        "                         arrows=\"to\")\n",
        "\n",
        "        # Set physics layout\n",
        "        net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08)\n",
        "        net.toggle_physics(True)\n",
        "\n",
        "        # Save visualization\n",
        "        net.show(output_file)\n",
        "        print(f\"Knowledge graph visualization saved to {output_file}\")\n",
        "\n",
        "    def retrieve_similar_chunks(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve similar chunks for a query.\n",
        "\n",
        "        Args:\n",
        "            query: Query text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            List of (document, similarity) tuples\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not built. Call build_vector_store() first.\")\n",
        "\n",
        "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_paper_summary(self, doc_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a summary for a specific paper.\n",
        "\n",
        "        Args:\n",
        "            doc_id: Document identifier\n",
        "\n",
        "        Returns:\n",
        "            Summary text\n",
        "        \"\"\"\n",
        "        if doc_id not in self.documents:\n",
        "            raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Summarize the following academic paper excerpt:\n",
        "\n",
        "            {text}\n",
        "\n",
        "            Provide a comprehensive summary that includes:\n",
        "            1. The main research question/problem\n",
        "            2. The methodology used\n",
        "            3. Key findings and contributions\n",
        "            4. Limitations and future work\n",
        "\n",
        "            Summary:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Use the first few chunks (up to 10,000 characters)\n",
        "        chunks = self.chunks.get(doc_id, [])\n",
        "        combined_text = \"\"\n",
        "        for chunk in chunks:\n",
        "            combined_text += chunk.page_content + \"\\n\\n\"\n",
        "            if len(combined_text) > 10000:\n",
        "                break\n",
        "        combined_text = combined_text[:10000]\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        summary = chain.invoke({\"text\": combined_text})\n",
        "\n",
        "        return summary[\"text\"]\n",
        "\n",
        "    def compare_papers(self, doc_ids: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Compare multiple papers.\n",
        "\n",
        "        Args:\n",
        "            doc_ids: List of document identifiers\n",
        "\n",
        "        Returns:\n",
        "            Comparison text\n",
        "        \"\"\"\n",
        "        # Validate all doc_ids\n",
        "        for doc_id in doc_ids:\n",
        "            if doc_id not in self.documents:\n",
        "                raise ValueError(f\"Document {doc_id} not found.\")\n",
        "\n",
        "        # Get metadata for each paper\n",
        "        paper_info = []\n",
        "        for doc_id in doc_ids:\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            year = metadata.get(\"publication_year\", \"Unknown\")\n",
        "\n",
        "            # Get a brief summary by using first chunk\n",
        "            first_chunk = self.chunks[doc_id][0].page_content\n",
        "            paper_info.append(f\"Title: {title}\\nAuthors: {authors}\\nYear: {year}\\nExcerpt: {first_chunk[:500]}...\")\n",
        "\n",
        "        # Generate comparison using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Compare and contrast the following academic papers:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Provide a detailed comparison including:\n",
        "            1. Common themes and research questions\n",
        "            2. Differences in methodology\n",
        "            3. Complementary or contradictory findings\n",
        "            4. How they build upon each other's work\n",
        "            5. Potential gaps or areas for future research\n",
        "\n",
        "            Comparison:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        comparison = chain.invoke({\"papers\": \"\\n\\n\".join(paper_info)})\n",
        "\n",
        "        return comparison[\"text\"]\n",
        "\n",
        "    def answer_question(self, question: str, k: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Answer a question based on the papers.\n",
        "\n",
        "        Args:\n",
        "            question: Question text\n",
        "            k: Number of chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Answer text\n",
        "        \"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = self.retrieve_similar_chunks(question, k=k)\n",
        "\n",
        "        # Prepare context from chunks\n",
        "        context = []\n",
        "        for chunk, score in relevant_chunks:\n",
        "            doc_id = chunk.metadata[\"doc_id\"]\n",
        "            metadata = self.metadata.get(doc_id, {})\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            authors = \", \".join(metadata.get(\"authors\", [\"Unknown\"]))\n",
        "            context.append(f\"From '{title}' by {authors}:\\n{chunk.page_content}\")\n",
        "\n",
        "        # Generate answer using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"question\", \"context\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant with expertise in synthesizing information from academic papers.\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Here are relevant excerpts from academic papers:\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Please provide a comprehensive answer to the question based on the given context.\n",
        "            Cite the papers you're referencing in your response.\n",
        "            If the information provided is insufficient to answer the question, clearly state what's missing.\n",
        "\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        answer = chain.invoke({\"question\": question, \"context\": \"\\n\\n\".join(context)})\n",
        "\n",
        "        return answer[\"text\"]\n",
        "\n",
        "    def extract_themes(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract common themes across papers.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of themes with supporting evidence\n",
        "        \"\"\"\n",
        "        # Prepare paper information\n",
        "        papers_info = []\n",
        "        for doc_id, metadata in self.metadata.items():\n",
        "            title = metadata.get(\"title\", doc_id)\n",
        "            abstract = metadata.get(\"abstract\", \"\")\n",
        "            keywords = metadata.get(\"keywords\", [])\n",
        "\n",
        "            if not abstract:\n",
        "                # Try to find abstract in the document\n",
        "                doc_text = self.documents[doc_id]\n",
        "                abstract_match = re.search(r\"(?i)abstract(.*?)(?:introduction|keywords|background|related work)\", doc_text, re.DOTALL)\n",
        "                abstract = abstract_match.group(1) if abstract_match else \"No abstract found\"\n",
        "\n",
        "            papers_info.append(f\"Title: {title}\\nAbstract: {abstract}\\nKeywords: {', '.join(keywords)}\")\n",
        "\n",
        "        # Generate themes using LLM\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"papers\"],\n",
        "            template=\"\"\"\n",
        "            You are an academic research assistant. Analyze the following academic papers and extract common themes:\n",
        "\n",
        "            {papers}\n",
        "\n",
        "            Identify 3-5 major themes across these papers. For each theme:\n",
        "            1. Provide a clear name and description\n",
        "            2. List which papers address this theme\n",
        "            3. Describe how each paper contributes to or approaches this theme\n",
        "            4. Note any contradictions or complementary findings within the theme\n",
        "\n",
        "            Format your response as a JSON structure where each theme is a key, and the value contains the description and paper relationships.\n",
        "\n",
        "            Themes:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        themes_result = chain.invoke({\"papers\": \"\\n\\n\".join(papers_info)})\n",
        "\n",
        "        # Extract themes from the text response\n",
        "        themes_text = themes_result[\"text\"]\n",
        "\n",
        "        # Use LLM to convert to structured format\n",
        "        extraction_prompt = PromptTemplate(\n",
        "            input_variables=[\"themes_text\"],\n",
        "            template=\"\"\"\n",
        "            Convert the following themes analysis into a proper JSON structure:\n",
        "\n",
        "            {themes_text}\n",
        "\n",
        "            JSON format:\n",
        "            ```\n",
        "            {{\n",
        "                \"Theme 1 Name\": {{\n",
        "                    \"description\": \"Description of theme 1\",\n",
        "                    \"papers\": [\n",
        "                        {{\n",
        "                            \"title\": \"Paper Title 1\",\n",
        "                            \"contribution\": \"How Paper 1 contributes to this theme\"\n",
        "                        }},\n",
        "                        ...\n",
        "                    ]\n",
        "                }},\n",
        "                ...\n",
        "            }}\n",
        "            ```\n",
        "\n",
        "            Only respond with the valid JSON, nothing else.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=extraction_prompt)\n",
        "        json_result = chain.invoke({\"themes_text\": themes_text})\n",
        "\n",
        "        try:\n",
        "            # Extract JSON from the response (it might be wrapped in backticks)\n",
        "            json_text = re.search(r'```json\\n(.*?)\\n```', json_result[\"text\"], re.DOTALL)\n",
        "            if json_text:\n",
        "                json_str = json_text.group(1)\n",
        "            else:\n",
        "                json_str = json_result[\"text\"]\n",
        "\n",
        "            # Clean up any extra backticks\n",
        "            json_str = json_str.replace('```', '').strip()\n",
        "\n",
        "            import json\n",
        "            themes = json.loads(json_str)\n",
        "            return themes\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing themes JSON: {e}\")\n",
        "            return {\"error\": \"Could not parse themes\", \"raw_text\": themes_text}\n",
        "\n",
        "    def run_full_analysis(self, pdf_directory: str, output_dir: str = \"./output\"):\n",
        "        \"\"\"\n",
        "        Run full analysis pipeline on a directory of PDFs.\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_dir: Directory to save output files\n",
        "        \"\"\"\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Load PDFs\n",
        "        print(\"Loading PDFs...\")\n",
        "        self.load_multiple_pdfs(pdf_directory)\n",
        "\n",
        "        # 2. Chunk documents\n",
        "        print(\"Chunking documents...\")\n",
        "        self.chunk_all_documents()\n",
        "\n",
        "        # 3. Build vector store\n",
        "        print(\"Building vector store...\")\n",
        "        self.build_vector_store()\n",
        "\n",
        "        # 4. Extract metadata\n",
        "        print(\"Extracting metadata...\")\n",
        "        self.extract_metadata()\n",
        "\n",
        "        # 5. Build knowledge graph\n",
        "        print(\"Building knowledge graph...\")\n",
        "        self.build_knowledge_graph()\n",
        "\n",
        "        # 6. Visualize knowledge graph\n",
        "        print(\"Visualizing knowledge graph...\")\n",
        "        self.visualize_knowledge_graph(os.path.join(output_dir, \"knowledge_graph.html\"))\n",
        "\n",
        "        # 7. Generate summaries for each paper\n",
        "        print(\"Generating summaries...\")\n",
        "        summaries = {}\n",
        "        for doc_id in self.documents:\n",
        "            summaries[doc_id] = self.generate_paper_summary(doc_id)\n",
        "\n",
        "            # Save individual summaries\n",
        "            with open(os.path.join(output_dir, f\"{doc_id}_summary.txt\"), \"w\") as f:\n",
        "                f.write(summaries[doc_id])\n",
        "\n",
        "        # 8. Compare all papers\n",
        "        print(\"Comparing papers...\")\n",
        "        comparison = self.compare_papers(list(self.documents.keys()))\n",
        "        with open(os.path.join(output_dir, \"paper_comparison.txt\"), \"w\") as f:\n",
        "            f.write(comparison)\n",
        "\n",
        "        # 9. Extract themes\n",
        "        print(\"Extracting themes...\")\n",
        "        themes = self.extract_themes()\n",
        "\n",
        "        import json\n",
        "        with open(os.path.join(output_dir, \"themes.json\"), \"w\") as f:\n",
        "            json.dump(themes, f, indent=2)\n",
        "\n",
        "        print(f\"Analysis complete! Results saved in {output_dir}\")\n",
        "\n",
        "        return {\n",
        "            \"summaries\": summaries,\n",
        "            \"comparison\": comparison,\n",
        "            \"themes\": themes\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your OpenAI API key\n",
        "    OPENAI_API_KEY = \"your-api-key-here\"\n",
        "\n",
        "    # Initialize the RAG system\n",
        "    rag = AcademicRAG(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # Run full analysis\n",
        "    results = rag.run_full_analysis(\"./papers\")\n",
        "\n",
        "    # Ask questions\n",
        "    question = \"What are the main methodologies used across these papers and how do they compare?\"\n",
        "    answer = rag.answer_question(question)\n",
        "    print(f\"Q: {question}\\nA: {answer}\")\n",
        "\n",
        "    # Get more paper relationships\n",
        "    print(\"Paper relationships:\")\n",
        "    for source, target, data in rag.knowledge_graph.edges(data=True):\n",
        "        if data.get(\"type\") == \"cites\":\n",
        "            print(f\"  {source} cites {target}\")"
      ],
      "metadata": {
        "id": "rGFG6eGPgmQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}