{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d995135-6e58-4aa5-b6ae-2f8bfcd5fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549a421-50c9-4ffc-9302-b2d83e1f8c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b1e92-b7be-49bb-ac08-a71c081f12c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e81d6-8840-4d9d-8456-d533ab76e7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e04f03e-d283-43f4-ad03-94a2bca109c0",
   "metadata": {},
   "source": [
    "# code created by Claude: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af6d10-0c85-4d27-bab1-5185103bf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock Price Forecasting Pipeline with Flume, HDFS, and PySpark\n",
    "# Running on Vertex AI Workbench\n",
    "# This pipeline:\n",
    "# 1. Ingests historical stock data using Flume\n",
    "# 2. Stores data in HDFS\n",
    "# 3. Performs data cleaning and analysis with PySpark\n",
    "# 4. Builds a multivariate time series forecasting model\n",
    "# 5. Deploys the model for real-time prediction\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, date_format, to_date, window, lit, expr, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "# Prophet for time series forecasting\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "# Flask for API deployment\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Airflow for scheduling\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "###########################################\n",
    "# 1. FLUME CONFIGURATION Create Flume configuration file for stock data ingestion\n",
    "###########################################\n",
    "\n",
    "def create_flume_config():\n",
    "    \n",
    "    flume_config = \"\"\"\n",
    "    # Stock data source agent\n",
    "    stockAgent.sources = httpSource\n",
    "    stockAgent.channels = memoryChannel fileChannel\n",
    "    stockAgent.sinks = hdfsSink\n",
    "\n",
    "    # HTTP Source for API data\n",
    "    stockAgent.sources.httpSource.type = http\n",
    "    stockAgent.sources.httpSource.port = 8080\n",
    "    stockAgent.sources.httpSource.handler = org.apache.flume.source.http.JSONHandler\n",
    "    stockAgent.sources.httpSource.channels = memoryChannel fileChannel\n",
    "\n",
    "    # Memory Channel for processing\n",
    "    stockAgent.channels.memoryChannel.type = memory\n",
    "    stockAgent.channels.memoryChannel.capacity = 10000\n",
    "    stockAgent.channels.memoryChannel.transactionCapacity = 1000\n",
    "\n",
    "    # File Channel for durability\n",
    "    stockAgent.channels.fileChannel.type = file\n",
    "    stockAgent.channels.fileChannel.capacity = 1000000\n",
    "    stockAgent.channels.fileChannel.transactionCapacity = 10000\n",
    "    stockAgent.channels.fileChannel.checkpointDir = /var/flume/checkpoint\n",
    "    stockAgent.channels.fileChannel.dataDirs = /var/flume/data\n",
    "\n",
    "    # HDFS Sink\n",
    "    stockAgent.sinks.hdfsSink.type = hdfs\n",
    "    stockAgent.sinks.hdfsSink.channel = fileChannel\n",
    "    stockAgent.sinks.hdfsSink.hdfs.path = /stock_data/%Y/%m/%d\n",
    "    stockAgent.sinks.hdfsSink.hdfs.fileType = DataStream\n",
    "    stockAgent.sinks.hdfsSink.hdfs.writeFormat = Text\n",
    "    stockAgent.sinks.hdfsSink.hdfs.rollInterval = 3600\n",
    "    stockAgent.sinks.hdfsSink.hdfs.rollSize = 0\n",
    "    stockAgent.sinks.hdfsSink.hdfs.rollCount = 0\n",
    "    stockAgent.sinks.hdfsSink.hdfs.filePrefix = stock_data\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('flume_stock_config.conf', 'w') as f:\n",
    "        f.write(flume_config)\n",
    "    \n",
    "    print(\"Flume configuration created: flume_stock_config.conf\")\n",
    "    return 'flume_stock_config.conf'\n",
    "\n",
    "def setup_flume_pipeline():\n",
    "    \"\"\"\n",
    "    Set up the Flume pipeline for data ingestion\n",
    "    \"\"\"\n",
    "    # Create Flume configuration\n",
    "    flume_config_file = create_flume_config()\n",
    "    \n",
    "    # Command to start Flume agent (to be executed in shell)\n",
    "    flume_start_cmd = f\"flume-ng agent --conf /etc/flume --conf-file {flume_config_file} --name stockAgent -Dflume.root.logger=INFO,console\"\n",
    "    \n",
    "    print(f\"To start Flume agent, run: {flume_start_cmd}\")\n",
    "    return flume_start_cmd\n",
    "\n",
    "###########################################\n",
    "# 2. STOCK DATA COLLECTION\n",
    "###########################################\n",
    "\n",
    "def generate_stock_data_collection_script():\n",
    "    \"\"\"\n",
    "    Generate a Python script to collect stock data from APIs and push to Flume\n",
    "    \"\"\"\n",
    "    collection_script = \"\"\"\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "# List of stock symbols to track\n",
    "STOCK_SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'JPM', 'V', 'JNJ']\n",
    "\n",
    "def collect_historical_data():\n",
    "    \"\"\"Collect 10 years of historical data for specified stocks\"\"\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365*10)  # 10 years ago\n",
    "    \n",
    "    for symbol in STOCK_SYMBOLS:\n",
    "        print(f\"Collecting historical data for {symbol}...\")\n",
    "        stock = yf.Ticker(symbol)\n",
    "        hist_data = stock.history(start=start_date.strftime('%Y-%m-%d'), \n",
    "                                end=end_date.strftime('%Y-%m-%d'),\n",
    "                                interval='1d')\n",
    "        \n",
    "        # Save to CSV for initial ingestion\n",
    "        output_file = f\"stock_data_{symbol}_historical.csv\"\n",
    "        hist_data.to_csv(output_file)\n",
    "        \n",
    "        # Push to Flume HTTP source\n",
    "        send_to_flume(symbol, hist_data)\n",
    "        \n",
    "        print(f\"Data saved to {output_file}\")\n",
    "\n",
    "def send_to_flume(symbol, data):\n",
    "    \"\"\"Send data to Flume HTTP source\"\"\"\n",
    "    FLUME_URL = \"http://localhost:8080\"\n",
    "    \n",
    "    # Convert dataframe to list of records\n",
    "    records = []\n",
    "    for date, row in data.iterrows():\n",
    "        record = {\n",
    "            'symbol': symbol,\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'open': float(row['Open']),\n",
    "            'high': float(row['High']),\n",
    "            'low': float(row['Low']),\n",
    "            'close': float(row['Close']),\n",
    "            'volume': int(row['Volume']),\n",
    "            'timestamp': int(time.time())\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    # Send in batches to avoid overwhelming the server\n",
    "    BATCH_SIZE = 100\n",
    "    for i in range(0, len(records), BATCH_SIZE):\n",
    "        batch = records[i:i+BATCH_SIZE]\n",
    "        try:\n",
    "            response = requests.post(FLUME_URL, json=batch, \n",
    "                                    headers={'Content-Type': 'application/json'})\n",
    "            print(f\"Batch {i//BATCH_SIZE + 1} sent to Flume: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error sending to Flume: {e}\")\n",
    "\n",
    "def collect_daily_data():\n",
    "    \"\"\"Collect daily data for the specified stocks\"\"\"\n",
    "    for symbol in STOCK_SYMBOLS:\n",
    "        try:\n",
    "            # Get today's data\n",
    "            stock = yf.Ticker(symbol)\n",
    "            today_data = stock.history(period='1d')\n",
    "            \n",
    "            # Format data for Flume\n",
    "            if not today_data.empty:\n",
    "                record = {\n",
    "                    'symbol': symbol,\n",
    "                    'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                    'open': float(today_data['Open'].iloc[0]),\n",
    "                    'high': float(today_data['High'].iloc[0]),\n",
    "                    'low': float(today_data['Low'].iloc[0]),\n",
    "                    'close': float(today_data['Close'].iloc[0]),\n",
    "                    'volume': int(today_data['Volume'].iloc[0]),\n",
    "                    'timestamp': int(time.time())\n",
    "                }\n",
    "                \n",
    "                # Send to Flume\n",
    "                response = requests.post(\"http://localhost:8080\", \n",
    "                                       json=[record],\n",
    "                                       headers={'Content-Type': 'application/json'})\n",
    "                print(f\"Daily data for {symbol} sent to Flume: {response.status_code}\")\n",
    "            else:\n",
    "                print(f\"No data available for {symbol} today\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting data for {symbol}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--historical\":\n",
    "        # Collect historical data for initial load\n",
    "        collect_historical_data()\n",
    "    else:\n",
    "        # Default: collect daily data\n",
    "        collect_daily_data()\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('stock_data_collector.py', 'w') as f:\n",
    "        f.write(collection_script)\n",
    "    \n",
    "    print(\"Stock data collection script created: stock_data_collector.py\")\n",
    "    return 'stock_data_collector.py'\n",
    "\n",
    "###########################################\n",
    "# 3. SPARK PROCESSING\n",
    "###########################################\n",
    "\n",
    "def initialize_spark_session():\n",
    "    \"\"\"\n",
    "    Initialize SparkSession for PySpark processing\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"StockPriceForecastingPipeline\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "def data_cleaning_and_preparation(spark):\n",
    "    \"\"\"\n",
    "    Clean and prepare stock data using PySpark\n",
    "    \"\"\"\n",
    "    # Define schema for stock data\n",
    "    stock_schema = StructType([\n",
    "        StructField(\"symbol\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"open\", DoubleType(), True),\n",
    "        StructField(\"high\", DoubleType(), True),\n",
    "        StructField(\"low\", DoubleType(), True),\n",
    "        StructField(\"close\", DoubleType(), True),\n",
    "        StructField(\"volume\", DoubleType(), True),\n",
    "        StructField(\"timestamp\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Read data from HDFS\n",
    "    raw_stock_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .schema(stock_schema) \\\n",
    "        .csv(\"hdfs://localhost:9000/stock_data/*/*/*/*\")\n",
    "    \n",
    "    # Register as temp view for SQL operations\n",
    "    raw_stock_df.createOrReplaceTempView(\"raw_stock_data\")\n",
    "    \n",
    "    # Basic cleaning\n",
    "    cleaned_df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            symbol,\n",
    "            date,\n",
    "            open,\n",
    "            high,\n",
    "            low,\n",
    "            close,\n",
    "            volume,\n",
    "            timestamp\n",
    "        FROM raw_stock_data\n",
    "        WHERE\n",
    "            open IS NOT NULL\n",
    "            AND close IS NOT NULL\n",
    "            AND high IS NOT NULL\n",
    "            AND low IS NOT NULL\n",
    "            AND volume IS NOT NULL\n",
    "            AND volume > 0\n",
    "            AND open > 0\n",
    "            AND close > 0\n",
    "            AND high > 0\n",
    "            AND low > 0\n",
    "            AND high >= low\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create window spec for time-based calculations\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "    \n",
    "    # Calculate additional features\n",
    "    enhanced_df = cleaned_df \\\n",
    "        .withColumn(\"prev_close\", lag(\"close\", 1).over(window_spec)) \\\n",
    "        .withColumn(\"daily_return\", when(col(\"prev_close\").isNotNull(), \n",
    "                                      (col(\"close\") - col(\"prev_close\")) / col(\"prev_close\"))\n",
    "                  .otherwise(lit(0.0))) \\\n",
    "        .withColumn(\"price_range\", col(\"high\") - col(\"low\")) \\\n",
    "        .withColumn(\"day_of_week\", date_format(col(\"date\"), \"u\").cast(\"int\")) \\\n",
    "        .withColumn(\"month\", date_format(col(\"date\"), \"M\").cast(\"int\")) \\\n",
    "        .withColumn(\"year\", date_format(col(\"date\"), \"yyyy\").cast(\"int\"))\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    enhanced_df = enhanced_df \\\n",
    "        .withColumn(\"ma5\", expr(\"avg(close) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW)\")) \\\n",
    "        .withColumn(\"ma20\", expr(\"avg(close) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN 19 PRECEDING AND CURRENT ROW)\")) \\\n",
    "        .withColumn(\"ma50\", expr(\"avg(close) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN 49 PRECEDING AND CURRENT ROW)\")) \\\n",
    "        .withColumn(\"ma200\", expr(\"avg(close) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN 199 PRECEDING AND CURRENT ROW)\"))\n",
    "    \n",
    "    # Calculate volatility (20-day standard deviation of returns)\n",
    "    enhanced_df = enhanced_df \\\n",
    "        .withColumn(\"volatility_20d\", expr(\"stddev(daily_return) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN 19 PRECEDING AND CURRENT ROW)\"))\n",
    "    \n",
    "    # Fill null values for calculated columns\n",
    "    enhanced_df = enhanced_df \\\n",
    "        .na.fill({\n",
    "            \"prev_close\": 0.0,\n",
    "            \"daily_return\": 0.0,\n",
    "            \"ma5\": 0.0,\n",
    "            \"ma20\": 0.0, \n",
    "            \"ma50\": 0.0,\n",
    "            \"ma200\": 0.0,\n",
    "            \"volatility_20d\": 0.0\n",
    "        })\n",
    "    \n",
    "    # Save processed data back to HDFS\n",
    "    enhanced_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"symbol\", \"year\", \"month\") \\\n",
    "        .parquet(\"hdfs://localhost:9000/processed_stock_data/\")\n",
    "    \n",
    "    print(\"Data cleaning and preparation completed\")\n",
    "    return enhanced_df\n",
    "\n",
    "def perform_stock_analysis(spark, stock_df, output_path=\"hdfs://localhost:9000/stock_analysis_results/\"):\n",
    "    \"\"\"\n",
    "    Perform various stock market analyses\n",
    "    \"\"\"\n",
    "    # Register dataframe as temp view\n",
    "    stock_df.createOrReplaceTempView(\"stock_data\")\n",
    "    \n",
    "    # 1. Correlation analysis between stocks\n",
    "    correlation_df = spark.sql(\"\"\"\n",
    "        WITH daily_returns AS (\n",
    "            SELECT \n",
    "                a.date,\n",
    "                a.symbol as symbol_a,\n",
    "                b.symbol as symbol_b,\n",
    "                a.daily_return as return_a,\n",
    "                b.daily_return as return_b\n",
    "            FROM stock_data a\n",
    "            JOIN stock_data b ON a.date = b.date AND a.symbol < b.symbol\n",
    "        )\n",
    "        \n",
    "        SELECT \n",
    "            symbol_a,\n",
    "            symbol_b,\n",
    "            corr(return_a, return_b) as correlation\n",
    "        FROM daily_returns\n",
    "        GROUP BY symbol_a, symbol_b\n",
    "        ORDER BY correlation DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    # 2. Volatility analysis\n",
    "    volatility_df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            symbol,\n",
    "            year,\n",
    "            avg(volatility_20d) as avg_volatility,\n",
    "            max(volatility_20d) as max_volatility,\n",
    "            min(volatility_20d) as min_volatility\n",
    "        FROM stock_data\n",
    "        WHERE volatility_20d IS NOT NULL\n",
    "        GROUP BY symbol, year\n",
    "        ORDER BY symbol, year\n",
    "    \"\"\")\n",
    "    \n",
    "    # 3. Performance analysis\n",
    "    performance_df = spark.sql(\"\"\"\n",
    "        WITH yearly_prices AS (\n",
    "            SELECT\n",
    "                symbol,\n",
    "                year,\n",
    "                first_value(close) OVER (PARTITION BY symbol, year ORDER BY date) as start_price,\n",
    "                last_value(close) OVER (PARTITION BY symbol, year ORDER BY date \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as end_price\n",
    "            FROM stock_data\n",
    "        )\n",
    "        \n",
    "        SELECT DISTINCT\n",
    "            symbol,\n",
    "            year,\n",
    "            start_price,\n",
    "            end_price,\n",
    "            (end_price - start_price) / start_price * 100 as yearly_return_pct\n",
    "        FROM yearly_prices\n",
    "        ORDER BY year DESC, yearly_return_pct DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    # 4. Technical indicators analysis\n",
    "    indicators_df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            symbol,\n",
    "            date,\n",
    "            close,\n",
    "            ma20,\n",
    "            ma50,\n",
    "            ma200,\n",
    "            CASE \n",
    "                WHEN ma20 > ma50 AND lag(ma20, 1) OVER (PARTITION BY symbol ORDER BY date) <= lag(ma50, 1) OVER (PARTITION BY symbol ORDER BY date)\n",
    "                THEN 'GOLDEN_CROSS'\n",
    "                WHEN ma20 < ma50 AND lag(ma20, 1) OVER (PARTITION BY symbol ORDER BY date) >= lag(ma50, 1) OVER (PARTITION BY symbol ORDER BY date)\n",
    "                THEN 'DEATH_CROSS'\n",
    "                ELSE 'NONE'\n",
    "            END as ma_signal\n",
    "        FROM stock_data\n",
    "        WHERE ma20 IS NOT NULL AND ma50 IS NOT NULL AND ma200 IS NOT NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    # Write results to HDFS\n",
    "    correlation_df.write.mode(\"overwrite\").parquet(output_path + \"correlations/\")\n",
    "    volatility_df.write.mode(\"overwrite\").parquet(output_path + \"volatility/\")\n",
    "    performance_df.write.mode(\"overwrite\").parquet(output_path + \"performance/\")\n",
    "    indicators_df.write.mode(\"overwrite\").parquet(output_path + \"technical_indicators/\")\n",
    "    \n",
    "    print(\"Stock analysis completed\")\n",
    "    return {\n",
    "        \"correlation\": correlation_df,\n",
    "        \"volatility\": volatility_df,\n",
    "        \"performance\": performance_df,\n",
    "        \"indicators\": indicators_df\n",
    "    }\n",
    "\n",
    "###########################################\n",
    "# 4. TIME SERIES FORECASTING\n",
    "###########################################\n",
    "\n",
    "def build_multivariate_forecast_model():\n",
    "    \"\"\"\n",
    "    Build multivariate time series forecasting model using Prophet\n",
    "    \"\"\"\n",
    "    # Create a class for the multivariate model implementation\n",
    "    class MultiStockForecaster:\n",
    "        \"\"\"\n",
    "        Class to implement multivariate time series forecasting for multiple stocks\n",
    "        \"\"\"\n",
    "        def __init__(self, symbols, spark=None):\n",
    "            self.symbols = symbols\n",
    "            self.models = {}\n",
    "            self.metrics = {}\n",
    "            self.forecasts = {}\n",
    "            self.spark = spark\n",
    "            \n",
    "        def load_data(self):\n",
    "            \"\"\"Load data from processed HDFS location\"\"\"\n",
    "            if self.spark is None:\n",
    "                self.spark = initialize_spark_session()\n",
    "                \n",
    "            # Load all processed stock data\n",
    "            stock_data = self.spark.read.parquet(\"hdfs://localhost:9000/processed_stock_data/\")\n",
    "            \n",
    "            # Convert to pandas for Prophet\n",
    "            self.data_dict = {}\n",
    "            for symbol in self.symbols:\n",
    "                symbol_data = stock_data.filter(col(\"symbol\") == symbol).toPandas()\n",
    "                if not symbol_data.empty:\n",
    "                    # Format for Prophet (requires 'ds' for date and 'y' for target)\n",
    "                    prophet_df = symbol_data[['date', 'close']].rename(columns={'date': 'ds', 'close': 'y'})\n",
    "                    self.data_dict[symbol] = prophet_df\n",
    "                else:\n",
    "                    print(f\"No data found for symbol {symbol}\")\n",
    "            \n",
    "            return self.data_dict\n",
    "        \n",
    "        def train_models(self, periods=730):  # 730 days = 2 years\n",
    "            \"\"\"Train Prophet models for each stock\"\"\"\n",
    "            if not hasattr(self, 'data_dict'):\n",
    "                self.load_data()\n",
    "                \n",
    "            # Train a model for each stock\n",
    "            for symbol, data in self.data_dict.items():\n",
    "                print(f\"Training model for {symbol}...\")\n",
    "                \n",
    "                # Configure the Prophet model with parameters for stock forecasting\n",
    "                model = Prophet(\n",
    "                    daily_seasonality=False,\n",
    "                    weekly_seasonality=True,\n",
    "                    yearly_seasonality=True,\n",
    "                    seasonality_mode='multiplicative',\n",
    "                    interval_width=0.95,\n",
    "                    changepoint_prior_scale=0.05\n",
    "                )\n",
    "                \n",
    "                # Add additional regressors if available in the future\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(data)\n",
    "                \n",
    "                # Store the model\n",
    "                self.models[symbol] = model\n",
    "                \n",
    "                # Create future dataframe for prediction\n",
    "                future = model.make_future_dataframe(periods=periods)\n",
    "                \n",
    "                # Generate forecast\n",
    "                forecast = model.predict(future)\n",
    "                self.forecasts[symbol] = forecast\n",
    "                \n",
    "                # Evaluate the model using cross-validation\n",
    "                try:\n",
    "                    cv_results = cross_validation(\n",
    "                        model=model,\n",
    "                        initial='730 days',  # Use 2 years for initial training\n",
    "                        period='90 days',    # Test on 90-day periods\n",
    "                        horizon='180 days'   # Forecast 180 days ahead\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate performance metrics\n",
    "                    metrics = performance_metrics(cv_results)\n",
    "                    self.metrics[symbol] = metrics\n",
    "                    \n",
    "                    print(f\"Model training completed for {symbol}\")\n",
    "                    print(f\"MAPE: {metrics['mape'].mean():.2f}%\")\n",
    "                    print(f\"RMSE: {metrics['rmse'].mean():.2f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during cross-validation for {symbol}: {e}\")\n",
    "                    \n",
    "            return self.models\n",
    "        \n",
    "        def save_models(self, base_path=\"/models/\"):\n",
    "            \"\"\"Save trained models to disk\"\"\"\n",
    "            if not os.path.exists(base_path):\n",
    "                os.makedirs(base_path)\n",
    "                \n",
    "            for symbol, model in self.models.items():\n",
    "                model_path = os.path.join(base_path, f\"{symbol}_model.json\")\n",
    "                with open(model_path, 'w') as f:\n",
    "                    json.dump(model.to_json(), f)  # Prophet has to_json() method\n",
    "                    \n",
    "                # Also save the forecasts\n",
    "                forecast_path = os.path.join(base_path, f\"{symbol}_forecast.csv\")\n",
    "                self.forecasts[symbol].to_csv(forecast_path)\n",
    "                \n",
    "            print(f\"Models saved to {base_path}\")\n",
    "            return base_path\n",
    "        \n",
    "        def load_models(self, base_path=\"/models/\"):\n",
    "            \"\"\"Load trained models from disk\"\"\"\n",
    "            self.models = {}\n",
    "            for symbol in self.symbols:\n",
    "                model_path = os.path.join(base_path, f\"{symbol}_model.json\")\n",
    "                if os.path.exists(model_path):\n",
    "                    with open(model_path, 'r') as f:\n",
    "                        model_json = json.load(f)\n",
    "                        model = Prophet.from_json(model_json)\n",
    "                        self.models[symbol] = model\n",
    "                        \n",
    "                    # Also load forecasts if available\n",
    "                    forecast_path = os.path.join(base_path, f\"{symbol}_forecast.csv\") \n",
    "                    if os.path.exists(forecast_path):\n",
    "                        self.forecasts[symbol] = pd.read_csv(forecast_path)\n",
    "                        \n",
    "            print(f\"Models loaded from {base_path}\")\n",
    "            return self.models\n",
    "            \n",
    "        def get_forecast(self, symbol, days=30):\n",
    "            \"\"\"Get forecast for a specific symbol\"\"\"\n",
    "            if symbol not in self.forecasts:\n",
    "                raise ValueError(f\"No forecast available for {symbol}\")\n",
    "                \n",
    "            forecast = self.forecasts[symbol]\n",
    "            # Get most recent forecasts\n",
    "            latest_date = forecast['ds'].max()\n",
    "            start_date = latest_date - timedelta(days=30)  # Include some history\n",
    "            \n",
    "            # Filter forecast to include recent history and future predictions\n",
    "            recent_forecast = forecast[forecast['ds'] >= start_date].tail(days + 30)\n",
    "            \n",
    "            return recent_forecast\n",
    "            \n",
    "        def get_correlation_forecast(self):\n",
    "            \"\"\"Analyze correlations in forecasted values\"\"\"\n",
    "            if not self.forecasts:\n",
    "                raise ValueError(\"No forecasts available\")\n",
    "                \n",
    "            # Combine forecasts from different stocks\n",
    "            combined_forecast = pd.DataFrame({\n",
    "                'ds': next(iter(self.forecasts.values()))['ds']  # Get dates from first forecast\n",
    "            })\n",
    "            \n",
    "            # Add forecasted values for each stock\n",
    "            for symbol, forecast in self.forecasts.items():\n",
    "                combined_forecast[symbol] = forecast['yhat'].values\n",
    "                \n",
    "            # Calculate correlation matrix for future values only\n",
    "            today = datetime.now().strftime('%Y-%m-%d')\n",
    "            future_data = combined_forecast[combined_forecast['ds'] > today]\n",
    "            \n",
    "            # Drop the date column for correlation calculation\n",
    "            correlation_matrix = future_data.drop(columns=['ds']).corr()\n",
    "            \n",
    "            return correlation_matrix\n",
    "        \n",
    "    # Return the forecaster class\n",
    "    return MultiStockForecaster\n",
    "\n",
    "###########################################\n",
    "# 5. MODEL DEPLOYMENT\n",
    "###########################################\n",
    "\n",
    "def create_model_deployment():\n",
    "    \"\"\"\n",
    "    Create deployment code for the forecasting model\n",
    "    \"\"\"\n",
    "    deployment_code = \"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from prophet import Prophet\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models\n",
    "MODEL_DIR = \"/models/\"\n",
    "SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'JPM', 'V', 'JNJ']\n",
    "models = {}\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all available models\"\"\"\n",
    "    global models\n",
    "    for symbol in SYMBOLS:\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{symbol}_model.json\")\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, 'r') as f:\n",
    "                model_json = json.load(f)\n",
    "                models[symbol] = Prophet.from_json(model_json)\n",
    "                print(f\"Loaded model for {symbol}\")\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\"status\": \"healthy\", \"models_loaded\": list(models.keys())})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Endpoint for stock price prediction\n",
    "    Expected JSON input:\n",
    "    {\n",
    "        \"symbol\": \"AAPL\",\n",
    "        \"days\": 30\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        symbol = data.get('symbol')\n",
    "        days = int(data.get('days', 30))\n",
    "        \n",
    "        if symbol not in models:\n",
    "            return jsonify({\"error\": f\"Model not found for {symbol}\"}), 404\n",
    "        \n",
    "        # Create future dataframe\n",
    "        future = models[symbol].make_future_dataframe(periods=days)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = models[symbol].predict(future)\n",
    "        \n",
    "        # Format response\n",
    "        last_30_days = forecast.tail(days)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "        formatted_response = []\n",
    "        \n",
    "        for _, row in last_30_days.iterrows():\n",
    "            formatted_response.append({\n",
    "                'date': row['ds'].strftime('%Y-%m-%d'),\n",
    "                'predicted_price': float(row['yhat']),\n",
    "                'lower_bound': float(row['yhat_lower']),\n",
    "                'upper_bound': float(row['yhat_upper'])\n",
    "            })\n",
    "        \n",
    "        return jsonify({\n",
    "            'symbol': symbol,\n",
    "            'predictions': formatted_response\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/predict_all', methods=['GET'])\n",
    "def predict_all():\n",
    "    \"\"\"\n",
    "    Endpoint to predict prices for all available stocks\n",
    "    Query parameter: days (default: 7)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        days = int(request.args.get('days', 7))\n",
    "        all_predictions = {}\n",
    "        \n",
    "        for symbol, model in models.items():\n",
    "            # Create future dataframe\n",
    "            future = model.make_future_dataframe(periods=days)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # Format response for this symbol\n",
    "            last_days = forecast.tail(days)[['ds', 'yhat']]\n",
    "            symbol_predictions = []\n",
    "            \n",
    "            for _, row in last_days.iterrows():\n",
    "                symbol_predictions.append({\n",
    "                    'date': row['ds'].strftime('%Y-%m-%d'),\n",
    "                    'predicted_price': float(row['yhat'])\n",
    "                })\n",
    "            \n",
    "            all_predictions[symbol] = symbol_predictions\n",
    "        \n",
    "        return jsonify({\n",
    "            'predictions': all_predictions\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/update_model', methods=['POST'])\n",
    "def update_model():\n",
    "    \"\"\"\n",
    "    Endpoint to update a model with new data\n",
    "    Expected JSON input:\n",
    "    {\n",
    "        \"symbol\": \"AAPL\",\n",
    "        \"data\": [\n",
    "            {\"date\": \"2025-01-01\", \"close\": 200.0},\n",
    "            {\"date\": \"2025-01-02\", \"close\": 205.0},\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request_data = request.json\n",
    "        symbol = request_data.get('symbol')\n",
    "        new_data = request_data.get('data', [])\n",
    "        \n",
    "        if symbol not in models:\n",
    "            return jsonify({\"error\": f\"Model not found for {symbol}\"}), 404\n",
    "        \n",
    "        if not new_data:\n",
    "            return jsonify({\"error\": \"No data provided\"}), 400\n",
    "        \n",
    "        # Convert input data to Prophet format\n",
    "        prophet_data = pd.DataFrame([\n",
    "            {'ds': item['date'], 'y': item['close']} \n",
    "            for item in new_data\n",
    "        ])\n",
    "        \n",
    "        # Update the model with new data\n",
    "        model = models[symbol]\n",
    "        model.fit(prophet_data)\n",
    "        \n",
    "        # Save updated model\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{symbol}_model.json\")\n",
    "        with open(model_path, 'w') as f:\n",
    "            json.dump(model.to_json(), f)\n",
    "        \n",
    "        return jsonify({\"status\": \"success\", \"message\": f\"Model for {symbol} updated successfully\"})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Load models when the application starts\n",
    "load_models()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting stock prediction service...\")\n",
    "    app.run(host='0.0.0.0', port=8050)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('stock_prediction_service.py', 'w') as f:\n",
    "        f.write(deployment_code)\n",
    "    \n",
    "    print(\"Model deployment code created: stock_prediction_service.py\")\n",
    "    return 'stock_prediction_service.py'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc6ec7-d5dd-45a7-8358-98c00d0868b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 6. VERTEX AI INTEGRATION\n",
    "###########################################\n",
    "\n",
    "def create_vertex_ai_deployment_script():\n",
    "    \"\"\"\n",
    "    Create script for deploying the model on Vertex AI\n",
    "    \"\"\"\n",
    "    vertex_deployment_code = \"\"\"\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import model_monitoring\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "def deploy_to_vertex_ai(\n",
    "    project_id,\n",
    "    region,\n",
    "    model_display_name=\"stock-price-forecaster\",\n",
    "    model_dir=\"/models/\",\n",
    "    container_image=\"gcr.io/{project_id}/stock-price-forecaster:latest\"\n",
    "):\n",
    "    \"\"\"Deploy the stock forecasting model to Vertex AI\"\"\"\n",
    "    # Initialize Vertex AI client\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    # Package model artifacts\n",
    "    model_artifacts_dir = model_dir\n",
    "    serving_container_image_uri = container_image.format(project_id=project_id)\n",
    "    \n",
    "    # Upload model to Vertex AI Model Registry\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=model_artifacts_dir,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_predict_route=\"/predict\",\n",
    "        serving_container_health_route=\"/health\",\n",
    "        serving_container_environment_variables={\n",
    "            \"MODEL_DIR\": \"/models\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model uploaded to Vertex AI: {model.resource_name}\")\n",
    "    \n",
    "    # Deploy model to endpoint\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=3,\n",
    "        accelerator_type=None,\n",
    "        accelerator_count=0,\n",
    "        traffic_percentage=100,\n",
    "        deploy_request_timeout=1800,\n",
    "        display_name=f\"{model_display_name}-endpoint\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "    \n",
    "    # Set up model monitoring\n",
    "    monitoring_job = model_monitoring.ModelMonitoringJob.create(\n",
    "        display_name=f\"{model_display_name}-monitoring\",\n",
    "        endpoint=endpoint,\n",
    "        schedule_hours=24,  # Run every 24 hours\n",
    "        alerting_config=model_monitoring.AlertingConfig(\n",
    "            email_alert_config=model_monitoring.EmailAlertConfig(\n",
    "                user_emails=[\"admin@example.com\"]  # Replace with actual email\n",
    "            )\n",
    "        ),\n",
    "        sampling_rate=0.8,\n",
    "        analysis_instance_schema_uri=f\"gs://{project_id}-model-monitoring/schemas/stock_prediction_schema.yaml\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Model monitoring job created: {monitoring_job.resource_name}\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model.resource_name,\n",
    "        \"endpoint\": endpoint.resource_name,\n",
    "        \"monitoring_job\": monitoring_job.resource_name\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Deploy stock forecasting model to Vertex AI\")\n",
    "    parser.add_argument(\"--project-id\", required=True, help=\"Google Cloud Project ID\")\n",
    "    parser.add_argument(\"--region\", default=\"us-central1\", help=\"Google Cloud Region\")\n",
    "    parser.add_argument(\"--model-name\", default=\"stock-price-forecaster\", help=\"Model display name\")\n",
    "    parser.add_argument(\"--model-dir\", default=\"/models/\", help=\"Directory containing model artifacts\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    deploy_to_vertex_ai(\n",
    "        project_id=args.project_id,\n",
    "        region=args.region,\n",
    "        model_display_name=args.model_name,\n",
    "        model_dir=args.model_dir\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "    with open('vertex_ai_deployment.py', 'w') as f:\n",
    "        f.write(vertex_deployment_code)\n",
    "    \n",
    "    print(\"Vertex AI deployment script created: vertex_ai_deployment.py\")\n",
    "    return 'vertex_ai_deployment.py'\n",
    "\n",
    "def create_docker_files():\n",
    "    \"\"\"\n",
    "    Create Docker files for containerizing the model service\n",
    "    \"\"\"\n",
    "    # Dockerfile\n",
    "    dockerfile = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model serving code\n",
    "COPY stock_prediction_service.py .\n",
    "\n",
    "# Create directory for models\n",
    "RUN mkdir -p /models\n",
    "\n",
    "# Copy models (this will be done during the build process)\n",
    "# COPY ./models/ /models/\n",
    "\n",
    "# Set environment variables\n",
    "ENV MODEL_DIR=/models\n",
    "\n",
    "# Expose port for the prediction service\n",
    "EXPOSE 8080\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=30s --retries=3 \\\n",
    "  CMD curl -f http://localhost:8080/health || exit 1\n",
    "\n",
    "# Start the prediction service\n",
    "CMD [\"python\", \"stock_prediction_service.py\"]\n",
    "\"\"\"\n",
    "\n",
    "    # Requirements file\n",
    "    requirements = \"\"\"\n",
    "prophet==1.1.4\n",
    "pandas>=1.3.5\n",
    "numpy>=1.20.0\n",
    "Flask==2.2.3\n",
    "gunicorn==20.1.0\n",
    "\"\"\"\n",
    "\n",
    "    # Build script\n",
    "    build_script = \"\"\"\n",
    "#!/bin/bash\n",
    "# Script to build and push the Docker image for the stock prediction service\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "IMAGE_NAME=\"stock-price-forecaster\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "# Build the Docker image\n",
    "echo \"Building Docker image...\"\n",
    "docker build -t \"${IMAGE_NAME}:${IMAGE_TAG}\" .\n",
    "\n",
    "# Tag the image for Google Container Registry\n",
    "echo \"Tagging image for GCR...\"\n",
    "docker tag \"${IMAGE_NAME}:${IMAGE_TAG}\" \"gcr.io/${PROJECT_ID}/${IMAGE_NAME}:${IMAGE_TAG}\"\n",
    "\n",
    "# Push the image to GCR\n",
    "echo \"Pushing image to GCR...\"\n",
    "docker push \"gcr.io/${PROJECT_ID}/${IMAGE_NAME}:${IMAGE_TAG}\"\n",
    "\n",
    "echo \"Image pushed to gcr.io/${PROJECT_ID}/${IMAGE_NAME}:${IMAGE_TAG}\"\n",
    "\"\"\"\n",
    "\n",
    "    # Write files\n",
    "    with open('Dockerfile', 'w') as f:\n",
    "        f.write(dockerfile)\n",
    "    \n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements)\n",
    "    \n",
    "    with open('build_and_push.sh', 'w') as f:\n",
    "        f.write(build_script)\n",
    "    \n",
    "    # Make build script executable\n",
    "    os.chmod('build_and_push.sh', 0o755)\n",
    "    \n",
    "    print(\"Docker files created: Dockerfile, requirements.txt, build_and_push.sh\")\n",
    "    return ['Dockerfile', 'requirements.txt', 'build_and_push.sh']\n",
    "\n",
    "###########################################\n",
    "# 7. AIRFLOW DAG FOR SCHEDULING\n",
    "###########################################\n",
    "\n",
    "def create_airflow_dag():\n",
    "    \"\"\"\n",
    "    Create Airflow DAG for scheduling daily data processing\n",
    "    \"\"\"\n",
    "    airflow_dag_code = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n",
    "\n",
    "# Import functions from our pipeline\n",
    "from stock_data_processor import initialize_spark_session, data_cleaning_and_preparation, perform_stock_analysis\n",
    "from stock_forecaster import MultiStockForecaster\n",
    "\n",
    "# Define default arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['your-email@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'stock_price_forecasting_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline for stock price data processing and forecasting',\n",
    "    schedule_interval='0 0 * * *',  # Run daily at midnight\n",
    "    start_date=days_ago(1),\n",
    "    tags=['stocks', 'forecasting', 'pyspark'],\n",
    ")\n",
    "\n",
    "# Define tasks for the DAG\n",
    "def collect_daily_stock_data():\n",
    "    \"\"\"Task to collect daily stock data\"\"\"\n",
    "    import subprocess\n",
    "    result = subprocess.run(['python', 'stock_data_collector.py'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"Error collecting stock data: {result.stderr}\")\n",
    "    return \"Stock data collection completed\"\n",
    "\n",
    "def process_stock_data():\n",
    "    \"\"\"Task to process stock data with PySpark\"\"\"\n",
    "    spark = initialize_spark_session()\n",
    "    cleaned_df = data_cleaning_and_preparation(spark)\n",
    "    analysis_results = perform_stock_analysis(spark, cleaned_df)\n",
    "    spark.stop()\n",
    "    return \"Stock data processing completed\"\n",
    "\n",
    "def train_forecast_models():\n",
    "    \"\"\"Task to train forecasting models\"\"\"\n",
    "    # Define stock symbols\n",
    "    symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'JPM', 'V', 'JNJ']\n",
    "    \n",
    "    # Initialize forecaster\n",
    "    forecaster = MultiStockForecaster(symbols)\n",
    "    \n",
    "    # Load data\n",
    "    forecaster.load_data()\n",
    "    \n",
    "    # Train models\n",
    "    forecaster.train_models(periods=730)  # 2 years\n",
    "    \n",
    "    # Save models\n",
    "    forecaster.save_models(\"/models/\")\n",
    "    \n",
    "    return \"Forecast models training completed\"\n",
    "\n",
    "def update_prediction_service():\n",
    "    \"\"\"Task to update the prediction service with new models\"\"\"\n",
    "    import requests\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8050/health\")\n",
    "        print(f\"Prediction service health check: {response.json()}\")\n",
    "        \n",
    "        # No need to manually update as service loads models from /models/ directory\n",
    "        return \"Prediction service updated\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating prediction service: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Create tasks in the DAG\n",
    "start_flume_task = BashOperator(\n",
    "    task_id='start_flume',\n",
    "    bash_command='flume-ng agent --conf /etc/flume --conf-file flume_stock_config.conf --name stockAgent -Dflume.root.logger=INFO,console',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "collect_data_task = PythonOperator(\n",
    "    task_id='collect_stock_data',\n",
    "    python_callable=collect_daily_stock_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "process_data_task = PythonOperator(\n",
    "    task_id='process_stock_data',\n",
    "    python_callable=process_stock_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "train_models_task = PythonOperator(\n",
    "    task_id='train_forecast_models',\n",
    "    python_callable=train_forecast_models,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "update_service_task = PythonOperator(\n",
    "    task_id='update_prediction_service',\n",
    "    python_callable=update_prediction_service,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "start_flume_task >> collect_data_task >> process_data_task >> train_models_task >> update_service_task\n",
    "\"\"\"\n",
    "\n",
    "    with open('stock_forecasting_dag.py', 'w') as f:\n",
    "        f.write(airflow_dag_code)\n",
    "    \n",
    "    print(\"Airflow DAG created: stock_forecasting_dag.py\")\n",
    "    return 'stock_forecasting_dag.py'\n",
    "\n",
    "###########################################\n",
    "# 8. MAIN EXECUTION SCRIPT\n",
    "###########################################\n",
    "\n",
    "def create_main_script():\n",
    "    \"\"\"\n",
    "    Create main execution script for the pipeline\n",
    "    \"\"\"\n",
    "    main_script = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# Stock Price Forecasting Pipeline - Main Execution Script\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the necessary environment for the pipeline\"\"\"\n",
    "    print(\"Setting up environment...\")\n",
    "    \n",
    "    # Check if required tools are installed\n",
    "    required_tools = ['flume-ng', 'spark-submit', 'hadoop']\n",
    "    missing_tools = []\n",
    "    \n",
    "    for tool in required_tools:\n",
    "        try:\n",
    "            subprocess.run([tool, '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        except FileNotFoundError:\n",
    "            missing_tools.append(tool)\n",
    "    \n",
    "    if missing_tools:\n",
    "        print(f\"ERROR: The following required tools are missing: {', '.join(missing_tools)}\")\n",
    "        print(\"Please install them before continuing.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create necessary directories\n",
    "    dirs_to_create = ['./logs', './models', './data']\n",
    "    for dir_path in dirs_to_create:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    print(\"Environment setup completed\")\n",
    "\n",
    "def start_hadoop_flume_services():\n",
    "    \"\"\"Start Hadoop and Flume services\"\"\"\n",
    "    print(\"Starting Hadoop and Flume services...\")\n",
    "    \n",
    "    # Start Hadoop (if not already running)\n",
    "    try:\n",
    "        hadoop_status = subprocess.run(['hadoop', 'dfsadmin', '-report'], \n",
    "                                      stdout=subprocess.PIPE, \n",
    "                                      stderr=subprocess.PIPE)\n",
    "        \n",
    "        if hadoop_status.returncode != 0:\n",
    "            print(\"Starting Hadoop services...\")\n",
    "            subprocess.run(['start-dfs.sh'], check=True)\n",
    "            subprocess.run(['start-yarn.sh'], check=True)\n",
    "            time.sleep(5)  # Give Hadoop time to start\n",
    "        else:\n",
    "            print(\"Hadoop services already running\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Hadoop: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Configure and start Flume\n",
    "    print(\"Setting up Flume configuration...\")\n",
    "    from pipeline import create_flume_config, setup_flume_pipeline\n",
    "    \n",
    "    flume_config = create_flume_config()\n",
    "    flume_cmd = setup_flume_pipeline()\n",
    "    \n",
    "    print(f\"Start Flume manually with: {flume_cmd}\")\n",
    "    print(\"Hadoop and Flume services setup completed\")\n",
    "\n",
    "def historical_data_ingestion(args):\n",
    "    \"\"\"Ingest historical stock data\"\"\"\n",
    "    print(\"Starting historical data ingestion...\")\n",
    "    \n",
    "    # Generate data collection script\n",
    "    from pipeline import generate_stock_data_collection_script\n",
    "    collector_script = generate_stock_data_collection_script()\n",
    "    \n",
    "    # Execute historical data collection\n",
    "    try:\n",
    "        subprocess.run(['python', collector_script, '--historical'], check=True)\n",
    "        print(\"Historical data ingestion completed\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during historical data ingestion: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def process_data(args):\n",
    "    \"\"\"Process stock data with PySpark\"\"\"\n",
    "    print(\"Starting data processing with PySpark...\")\n",
    "    \n",
    "    # Run PySpark processing\n",
    "    try:\n",
    "        from pipeline import initialize_spark_session, data_cleaning_and_preparation, perform_stock_analysis\n",
    "        \n",
    "        spark = initialize_spark_session()\n",
    "        cleaned_df = data_cleaning_and_preparation(spark)\n",
    "        analysis_results = perform_stock_analysis(spark, cleaned_df)\n",
    "        \n",
    "        print(\"Data processing completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data processing: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def train_models(args):\n",
    "    \"\"\"Train forecasting models\"\"\"\n",
    "    print(\"Training forecasting models...\")\n",
    "    \n",
    "    try:\n",
    "        from pipeline import build_multivariate_forecast_model\n",
    "        \n",
    "        # Define stock symbols\n",
    "        symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'JPM', 'V', 'JNJ']\n",
    "        \n",
    "        # Initialize forecaster class\n",
    "        MultiStockForecaster = build_multivariate_forecast_model()\n",
    "        forecaster = MultiStockForecaster(symbols)\n",
    "        \n",
    "        # Load data\n",
    "        forecaster.load_data()\n",
    "        \n",
    "        # Train models\n",
    "        forecaster.train_models(periods=730)  # 2 years forecast\n",
    "        \n",
    "        # Save models\n",
    "        forecaster.save_models(args.model_dir)\n",
    "        \n",
    "        print(f\"Models trained and saved to {args.model_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def deploy_service(args):\n",
    "    \"\"\"Deploy prediction service\"\"\"\n",
    "    print(\"Setting up prediction service...\")\n",
    "    \n",
    "    try:\n",
    "        # Create deployment code\n",
    "        from pipeline import create_model_deployment\n",
    "        service_script = create_model_deployment()\n",
    "        \n",
    "        # Create Docker files\n",
    "        from pipeline import create_docker_files\n",
    "        docker_files = create_docker_files()\n",
    "        \n",
    "        if args.cloud_deploy:\n",
    "            # Create Vertex AI deployment script\n",
    "            from pipeline import create_vertex_ai_deployment_script\n",
    "            vertex_script = create_vertex_ai_deployment_script()\n",
    "            \n",
    "            print(f\"To deploy to Vertex AI, run: python {vertex_script} --project-id YOUR_PROJECT_ID\")\n",
    "        else:\n",
    "            # Start local prediction service\n",
    "            print(f\"To start prediction service locally, run: python {service_script}\")\n",
    "        \n",
    "        print(\"Deployment setup completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during deployment setup: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def setup_scheduling(args):\n",
    "    \"\"\"Set up scheduling with Airflow\"\"\"\n",
    "    print(\"Setting up Airflow DAG for scheduling...\")\n",
    "    \n",
    "    try:\n",
    "        from pipeline import create_airflow_dag\n",
    "        airflow_dag = create_airflow_dag()\n",
    "        \n",
    "        airflow_dag_path = os.path.join(args.airflow_dag_path, 'stock_forecasting_dag.py')\n",
    "        if args.airflow_dag_path != '.':\n",
    "            # Copy DAG to Airflow DAGs directory\n",
    "            import shutil\n",
    "            shutil.copy2(airflow_dag, airflow_dag_path)\n",
    "        \n",
    "        print(f\"Airflow DAG created at: {airflow_dag_path}\")\n",
    "        print(\"To enable the DAG in Airflow, make sure the Airflow scheduler is running\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up scheduling: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def run_full_pipeline(args):\n",
    "    \"\"\"Run the full pipeline\"\"\"\n",
    "    setup_environment()\n",
    "    start_hadoop_flume_services()\n",
    "    historical_data_ingestion(args)\n",
    "    process_data(args)\n",
    "    train_models(args)\n",
    "    deploy_service(args)\n",
    "    setup_scheduling(args)\n",
    "    print(\"Full pipeline execution completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Stock Price Forecasting Pipeline\")\n",
    "    subparsers = parser.add_subparsers(help=\"Pipeline commands\")\n",
    "    \n",
    "    # Setup environment parser\n",
    "    setup_parser = subparsers.add_parser(\"setup\", help=\"Set up the environment\")\n",
    "    setup_parser.set_defaults(func=setup_environment)\n",
    "    \n",
    "    # Services parser\n",
    "    services_parser = subparsers.add_parser(\"services\", help=\"Start Hadoop and Flume services\")\n",
    "    services_parser.set_defaults(func=start_hadoop_flume_services)\n",
    "    \n",
    "    # Ingestion parser\n",
    "    ingestion_parser = subparsers.add_parser(\"ingest\", help=\"Ingest historical stock data\")\n",
    "    ingestion_parser.set_defaults(func=historical_data_ingestion)\n",
    "    \n",
    "    # Processing parser\n",
    "    processing_parser = subparsers.add_parser(\"process\", help=\"Process stock data with PySpark\")\n",
    "    processing_parser.set_defaults(func=process_data)\n",
    "    \n",
    "    # Model training parser\n",
    "    training_parser = subparsers.add_parser(\"train\", help=\"Train forecasting models\")\n",
    "    training_parser.add_argument(\"--model-dir\", default=\"/models/\", help=\"Directory to save models\")\n",
    "    training_parser.set_defaults(func=train_models)\n",
    "    \n",
    "    # Deployment parser\n",
    "    deploy_parser = subparsers.add_parser(\"deploy\", help=\"Deploy prediction service\")\n",
    "    deploy_parser.add_argument(\"--cloud-deploy\", action=\"store_true\", help=\"Deploy to Vertex AI\")\n",
    "    deploy_parser.set_defaults(func=deploy_service)\n",
    "    \n",
    "    # Scheduling parser\n",
    "    schedule_parser = subparsers.add_parser(\"schedule\", help=\"Set up Airflow scheduling\")\n",
    "    schedule_parser.add_argument(\"--airflow-dag-path\", default=\".\", help=\"Path to Airflow DAGs directory\")\n",
    "    schedule_parser.set_defaults(func=setup_scheduling)\n",
    "    \n",
    "    # Full pipeline parser\n",
    "    full_parser = subparsers.add_parser(\"full\", help=\"Run the full pipeline\")\n",
    "    full_parser.add_argument(\"--model-dir\", default=\"/models/\", help=\"Directory to save models\")\n",
    "    full_parser.add_argument(\"--cloud-deploy\", action=\"store_true\", help=\"Deploy to Vertex AI\")\n",
    "    full_parser.add_argument(\"--airflow-dag-path\", default=\".\", help=\"Path to Airflow DAGs directory\")\n",
    "    full_parser.set_defaults(func=run_full_pipeline)\n",
    "    \n",
    "    # Parse arguments and execute functions\n",
    "    args = parser.parse_args()\n",
    "    if hasattr(args, 'func'):\n",
    "        args.func(args)\n",
    "    else:\n",
    "        parser.print_help()\n",
    "\"\"\"\n",
    "\n",
    "    with open('run_pipeline.py', 'w') as f:\n",
    "        f.write(main_script)\n",
    "    \n",
    "    # Make executable\n",
    "    os.chmod('run_pipeline.py', 0o755)\n",
    "    \n",
    "    print(\"Main execution script created: run_pipeline.py\")\n",
    "    return 'run_pipeline.py'\n",
    "\n",
    "###########################################\n",
    "# 9. PACKAGE THE PIPELINE\n",
    "###########################################\n",
    "\n",
    "def create_package_structure():\n",
    "    \"\"\"\n",
    "    Create a package structure for the pipeline\n",
    "    \"\"\"\n",
    "    # Create pipeline module\n",
    "    pipeline_module = \"\"\"\n",
    "# Stock Price Forecasting Pipeline module\n",
    "# This module contains all the functions used in the pipeline\n",
    "\n",
    "# Import all functions\n",
    "from .flume_config import create_flume_config, setup_flume_pipeline\n",
    "from .data_collection import generate_stock_data_collection_script\n",
    "from .spark_processing import initialize_spark_session, data_cleaning_and_preparation, perform_stock_analysis\n",
    "from .forecasting import build_multivariate_forecast_model\n",
    "from .deployment import create_model_deployment, create_vertex_ai_deployment_script, create_docker_files\n",
    "from .scheduling import create_airflow_dag\n",
    "\n",
    "__all__ = [\n",
    "    'create_flume_config',\n",
    "    'setup_flume_pipeline',\n",
    "    'generate_stock_data_collection_script',\n",
    "    'initialize_spark_session',\n",
    "    'data_cleaning_and_preparation',\n",
    "    'perform_stock_analysis',\n",
    "    'build_multivariate_forecast_model',\n",
    "    'create_model_deployment',\n",
    "    'create_vertex_ai_deployment_script',\n",
    "    'create_docker_files',\n",
    "    'create_airflow_dag'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "    # Create directory structure\n",
    "    os.makedirs('pipeline', exist_ok=True)\n",
    "    \n",
    "    # Write init file\n",
    "    with open('pipeline/__init__.py', 'w') as f:\n",
    "        f.write(pipeline_module)\n",
    "    \n",
    "    # Create module files\n",
    "    modules = {\n",
    "        'flume_config.py': \"\"\"\n",
    "# Flume configuration module\n",
    "def create_flume_config():\n",
    "    \\\"\\\"\\\"\n",
    "    Create Flume configuration file for stock data ingestion\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\n",
    "def setup_flume_pipeline():\n",
    "    \\\"\\\"\\\"\n",
    "    Set up the Flume pipeline for data ingestion\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\"\"\",\n",
    "        'data_collection.py': \"\"\"\n",
    "# Data collection module\n",
    "def generate_stock_data_collection_script():\n",
    "    \\\"\\\"\\\"\n",
    "    Generate a Python script to collect stock data from APIs and push to Flume\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\"\"\",\n",
    "        'spark_processing.py': \"\"\"\n",
    "# Spark processing module\n",
    "def initialize_spark_session():\n",
    "    \\\"\\\"\\\"\n",
    "    Initialize SparkSession for PySpark processing\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\n",
    "def data_cleaning_and_preparation(spark):\n",
    "    \\\"\\\"\\\"\n",
    "    Clean and prepare stock data using PySpark\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\n",
    "def perform_stock_analysis(spark, stock_df, output_path=\"hdfs://localhost:9000/stock_analysis_results/\"):\n",
    "    \\\"\\\"\\\"\n",
    "    Perform various stock market analyses\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\"\"\",\n",
    "        'forecasting.py': \"\"\"\n",
    "# Forecasting module\n",
    "def build_multivariate_forecast_model():\n",
    "    \\\"\\\"\\\"\n",
    "    Build multivariate time series forecasting model using Prophet\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\"\"\",\n",
    "        'deployment.py': \"\"\"\n",
    "# Deployment module\n",
    "def create_model_deployment():\n",
    "    \\\"\\\"\\\"\n",
    "    Create deployment code for the forecasting model\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\n",
    "def create_vertex_ai_deployment_script():\n",
    "    \\\"\\\"\\\"\n",
    "    Create script for deploying the model on Vertex AI\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\n",
    "def create_docker_files():\n",
    "    \\\"\\\"\\\"\n",
    "    Create Docker files for containerizing the model service\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\"\"\",\n",
    "        'scheduling.py': \"\"\"\n",
    "# Scheduling module\n",
    "def create_airflow_dag():\n",
    "    \\\"\\\"\\\"\n",
    "    Create Airflow DAG for scheduling daily data processing\n",
    "    \\\"\\\"\\\"\n",
    "    # Function implementation goes here\n",
    "    # This is a placeholder - the actual implementation is in the main script\n",
    "    pass\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Write module files\n",
    "    for filename, content in modules.items():\n",
    "        with open(f'pipeline/{filename}', 'w') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    # Create setup.py\n",
    "    setup_py = \"\"\"\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"stock_forecasting_pipeline\",\n",
    "    version=\"0.1.0\",\n",
    "    packages=find_packages(),\n",
    "    install_requires=[\n",
    "        \"pandas>=1.3.5\",\n",
    "        \"numpy>=1.20.0\",\n",
    "        \"matplotlib>=3.5.1\",\n",
    "        \"pyspark>=3.2.1\",\n",
    "        \"prophet>=1.1.1\",\n",
    "        \"flask>=2.2.3\",\n",
    "        \"google-cloud-aiplatform>=1.16.0\",\n",
    "        \"apache-airflow>=2.3.0\",\n",
    "        \"yfinance>=0.1.74\"\n",
    "    ],\n",
    "    author=\"Your Name\",\n",
    "    author_email=\"your.email@example.com\",\n",
    "    description=\"Stock Price Forecasting Pipeline with Flume, HDFS, and PySpark\",\n",
    "    keywords=\"stock, forecasting, pyspark, flume, hdfs\",\n",
    "    python_requires=\">=3.8\",\n",
    ")\n",
    "\"\"\"\n",
    "    \n",
    "    with open('setup.py', 'w') as f:\n",
    "        f.write(setup_py)\n",
    "    \n",
    "    # Create README.md\n",
    "    readme = \"\"\"\n",
    "# Stock Price Forecasting Pipeline\n",
    "\n",
    "A comprehensive pipeline for stock data processing and forecasting using Flume, HDFS, and PySpark on Vertex AI Workbench.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Ingests historical stock data (past 10 years) using Flume\n",
    "- Stores data in HDFS\n",
    "- Performs data cleaning and analysis with PySpark\n",
    "- Builds multivariate time series forecasting models\n",
    "- Deploys models for real-time prediction\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Apache Flume\n",
    "- Apache Hadoop (HDFS)\n",
    "- Apache Spark\n",
    "- Apache Airflow\n",
    "- Prophet\n",
    "- Google Cloud Vertex AI\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "```bash\n",
    "# Run the full pipeline\n",
    "./run_pipeline.py full\n",
    "\n",
    "# Or run individual steps\n",
    "./run_pipeline.py setup\n",
    "./run_pipeline.py services\n",
    "./run_pipeline.py ingest\n",
    "./run_pipeline.py process\n",
    "./run_pipeline.py train\n",
    "./run_pipeline.py deploy\n",
    "./run_pipeline.py schedule\n",
    "```\n",
    "\n",
    "## Deployment Options\n",
    "\n",
    "- Local deployment with Flask\n",
    "- Cloud deployment on Google Cloud Vertex AI\n",
    "\n",
    "## License\n",
    "\n",
    "MIT\n",
    "\"\"\"\n",
    "    \n",
    "    with open('README.md', 'w') as f:\n",
    "        f.write(readme)\n",
    "    \n",
    "    print(\"Package structure created\")\n",
    "    return 'setup.py'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the pipeline\n",
    "    print(\"Creating Stock Price Forecasting Pipeline...\")\n",
    "    \n",
    "    # Create Flume configuration\n",
    "    create_flume_config()\n",
    "    \n",
    "    # Create data collection script\n",
    "    generate_stock_data_collection_script()\n",
    "    \n",
    "    # Create model deployment code\n",
    "    create_model_deployment()\n",
    "    \n",
    "    # Create Vertex AI deployment script\n",
    "    create_vertex_ai_deployment_script()\n",
    "    \n",
    "    # Create Docker files\n",
    "    create_docker_files()\n",
    "    \n",
    "    # Create Airflow DAG\n",
    "    create_airflow_dag()\n",
    "    \n",
    "    # Create main script\n",
    "    create_main_script()\n",
    "    \n",
    "    # Create package structure\n",
    "    create_package_structure()\n",
    "    \n",
    "    print(\"\\nPipeline creation completed!\")\n",
    "    print(\"To run the pipeline, execute: ./run_pipeline.py full\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
