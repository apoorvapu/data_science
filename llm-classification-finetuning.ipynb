{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle: https://www.kaggle.com/competitions/llm-classification-finetuning/\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:17.572334Z","iopub.execute_input":"2025-04-26T17:57:17.572979Z","iopub.status.idle":"2025-04-26T17:57:17.577166Z","shell.execute_reply.started":"2025-04-26T17:57:17.572953Z","shell.execute_reply":"2025-04-26T17:57:17.576374Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest=pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:17.578686Z","iopub.execute_input":"2025-04-26T17:57:17.578938Z","iopub.status.idle":"2025-04-26T17:57:19.447961Z","shell.execute_reply.started":"2025-04-26T17:57:17.578921Z","shell.execute_reply":"2025-04-26T17:57:19.447329Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:19.449327Z","iopub.execute_input":"2025-04-26T17:57:19.449639Z","iopub.status.idle":"2025-04-26T17:57:19.459913Z","shell.execute_reply.started":"2025-04-26T17:57:19.449618Z","shell.execute_reply":"2025-04-26T17:57:19.459188Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:19.460640Z","iopub.execute_input":"2025-04-26T17:57:19.460835Z","iopub.status.idle":"2025-04-26T17:57:19.480380Z","shell.execute_reply.started":"2025-04-26T17:57:19.460820Z","shell.execute_reply":"2025-04-26T17:57:19.479295Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"train.drop(columns=['model_a','model_b','id'], inplace=True)\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:19.482312Z","iopub.execute_input":"2025-04-26T17:57:19.482626Z","iopub.status.idle":"2025-04-26T17:57:19.506689Z","shell.execute_reply.started":"2025-04-26T17:57:19.482605Z","shell.execute_reply":"2025-04-26T17:57:19.505870Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"train.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:19.507548Z","iopub.execute_input":"2025-04-26T17:57:19.507828Z","iopub.status.idle":"2025-04-26T17:57:19.517645Z","shell.execute_reply.started":"2025-04-26T17:57:19.507807Z","shell.execute_reply":"2025-04-26T17:57:19.516868Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"                                                  prompt  \\\n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57472</th>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"train.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:19.518469Z","iopub.execute_input":"2025-04-26T17:57:19.518777Z","iopub.status.idle":"2025-04-26T17:57:19.562601Z","shell.execute_reply.started":"2025-04-26T17:57:19.518757Z","shell.execute_reply":"2025-04-26T17:57:19.561845Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   prompt          57477 non-null  object\n 1   response_a      57477 non-null  object\n 2   response_b      57477 non-null  object\n 3   winner_model_a  57477 non-null  int64 \n 4   winner_model_b  57477 non-null  int64 \n 5   winner_tie      57477 non-null  int64 \ndtypes: int64(3), object(3)\nmemory usage: 2.6+ MB\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    text = str(text)\n    \n    # Basic cleaning\n    text = text.encode('utf-8').decode('unicode_escape', errors='ignore')\n    text = text.replace('\\n', ' ').replace('\\r', ' ')\n    text = text.replace('\\\"', '\"').replace(\"\\'\", \"'\")\n    text = text.replace('\"', '').replace('[', '').replace(']', '').replace(',', '')\n\n    # Lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n\n    # Remove multiple spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Optional: remove numbers\n    # text = re.sub(r'\\d+', '', text)\n\n    # Trim\n    text = text.strip()\n    \n    return text\n\n\n# Apply cleaning to all text columns\nfor col in ['prompt', 'response_a', 'response_b']:\n    train[col] = train[col].apply(clean_text)\n    test[col] = test[col].apply(clean_text)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:19.563532Z","iopub.execute_input":"2025-04-26T17:57:19.563836Z","iopub.status.idle":"2025-04-26T17:57:36.353999Z","shell.execute_reply.started":"2025-04-26T17:57:19.563809Z","shell.execute_reply":"2025-04-26T17:57:36.353258Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2595280604.py:9: DeprecationWarning: invalid escape sequence '\\/'\n  text = text.encode('utf-8').decode('unicode_escape', errors='ignore')\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"train.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:36.354804Z","iopub.execute_input":"2025-04-26T17:57:36.355003Z","iopub.status.idle":"2025-04-26T17:57:36.364218Z","shell.execute_reply.started":"2025-04-26T17:57:36.354987Z","shell.execute_reply":"2025-04-26T17:57:36.363426Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"                                                  prompt  \\\n57472  a simple mnemonic for π how i wish i could enu...   \n57473  in python implement a naive bayes with gaussia...   \n57474  is it unethical to work on building weapons ar...   \n57475  if a bait contains 00025 bromadiolon then how ...   \n57476  three kids eat three apples in three days how ...   \n\n                                              response_a  \\\n57472  sure let s break it down 1 how has 3 letters 2...   \n57473  here is an implementation of a naive bayes cla...   \n57474  working on weapons technology raises some ethi...   \n57475  bromadiolone is a rodenticide which is most of...   \n57476                                          27 apples   \n\n                                              response_b  winner_model_a  \\\n57472  here is how that mnemonic represents the digit...               1   \n57473  sure here s an implementation of a naive bayes...               1   \n57474  it depends on the context weapons can be used ...               1   \n57475  as an ai language model i do not promote or co...               0   \n57476  if three kids eat three apples in three days e...               1   \n\n       winner_model_b  winner_tie  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57472</th>\n      <td>a simple mnemonic for π how i wish i could enu...</td>\n      <td>sure let s break it down 1 how has 3 letters 2...</td>\n      <td>here is how that mnemonic represents the digit...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>in python implement a naive bayes with gaussia...</td>\n      <td>here is an implementation of a naive bayes cla...</td>\n      <td>sure here s an implementation of a naive bayes...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>is it unethical to work on building weapons ar...</td>\n      <td>working on weapons technology raises some ethi...</td>\n      <td>it depends on the context weapons can be used ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>if a bait contains 00025 bromadiolon then how ...</td>\n      <td>bromadiolone is a rodenticide which is most of...</td>\n      <td>as an ai language model i do not promote or co...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>three kids eat three apples in three days how ...</td>\n      <td>27 apples</td>\n      <td>if three kids eat three apples in three days e...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:36.364987Z","iopub.execute_input":"2025-04-26T17:57:36.365264Z","iopub.status.idle":"2025-04-26T17:57:36.384006Z","shell.execute_reply.started":"2025-04-26T17:57:36.365246Z","shell.execute_reply":"2025-04-26T17:57:36.383176Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  is it morally right to try to have a certain p...   \n1  what is the difference between marriage licens...   \n2  explain function calling how would you call a ...   \n3  how can i create a test set for a very rare ca...   \n4  what is the best way to travel from tel aviv t...   \n\n                                          response_a  \\\n0  the question of whether it is morally right to...   \n1  a marriage license is a legal document that al...   \n2  function calling is the process of invoking or...   \n3  creating a test set for a very rare category c...   \n4  the best way to travel from tel aviv to jerusa...   \n\n                                          response_b  winner_model_a  \\\n0  as an ai i don t have personal beliefs or opin...               1   \n1  a marriage license and a marriage certificate ...               0   \n2  function calling is the process of invoking a ...               0   \n3  when building a classifier for a very rare cat...               1   \n4  the best way to travel from tel aviv to jerusa...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>is it morally right to try to have a certain p...</td>\n      <td>the question of whether it is morally right to...</td>\n      <td>as an ai i don t have personal beliefs or opin...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what is the difference between marriage licens...</td>\n      <td>a marriage license is a legal document that al...</td>\n      <td>a marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>explain function calling how would you call a ...</td>\n      <td>function calling is the process of invoking or...</td>\n      <td>function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>how can i create a test set for a very rare ca...</td>\n      <td>creating a test set for a very rare category c...</td>\n      <td>when building a classifier for a very rare cat...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>what is the best way to travel from tel aviv t...</td>\n      <td>the best way to travel from tel aviv to jerusa...</td>\n      <td>the best way to travel from tel aviv to jerusa...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:36.386135Z","iopub.execute_input":"2025-04-26T17:57:36.386332Z","iopub.status.idle":"2025-04-26T17:57:36.403638Z","shell.execute_reply.started":"2025-04-26T17:57:36.386316Z","shell.execute_reply":"2025-04-26T17:57:36.402910Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  i have three oranges today i ate an orange yes...   \n1   211333  you are a mediator in a heated political debat...   \n2  1233961  how to initialize the classification head when...   \n\n                                          response_a  \\\n0                         you have two oranges today   \n1  thank you for sharing the details of the situa...   \n2  when you want to initialize the classification...   \n\n                                          response_b  \n0  you still have three oranges eating an orange ...  \n1  mr reddy and ms blue both have valid points in...  \n2  to initialize the classification head when per...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>i have three oranges today i ate an orange yes...</td>\n      <td>you have two oranges today</td>\n      <td>you still have three oranges eating an orange ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>you are a mediator in a heated political debat...</td>\n      <td>thank you for sharing the details of the situa...</td>\n      <td>mr reddy and ms blue both have valid points in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>how to initialize the classification head when...</td>\n      <td>when you want to initialize the classification...</td>\n      <td>to initialize the classification head when per...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"max_words = train['response_a'].apply(lambda x: len(str(x).split())).max()\nprint(\"Maximum number of words in any row:\", max_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:57:36.404325Z","iopub.execute_input":"2025-04-26T17:57:36.404605Z","iopub.status.idle":"2025-04-26T17:57:37.117373Z","shell.execute_reply.started":"2025-04-26T17:57:36.404586Z","shell.execute_reply":"2025-04-26T17:57:37.116560Z"}},"outputs":[{"name":"stdout","text":"Maximum number of words in any row: 9501\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"#  TF-IDF + Truncated SVD (Latent Semantic Analysis) + N-grams\n\n✅ Captures hidden relationships (like \"king\" ≈ \"queen\" somehow)","metadata":{}},{"cell_type":"markdown","source":"Why we combined the 3 columns for TF-IDF (Bag-of-Words model - Word importance only\t- NOT Meaningful sentence embeddings):\n\n    TF-IDF works on bag of words — it doesn't know sentence structure, order, or meaning.\n\n    TF-IDF just cares about: \"which words exist, and how frequent?\"\n\n    So if you split into separate columns, TF-IDF will treat them independently, and waste information.\n\n    Therefore, to capture full word context, we combine 'prompt', 'response_a', and 'response_b' into a single text for TF-IDF.\n    → Then TF-IDF sees all words together and computes the correct word weights across the combined context.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack  \n\n# Combine all text columns into one series for fitting\ncombined_text_train = train['prompt'] + ' ' + train['response_a'] + ' ' + train['response_b']\ncombined_text_test = test['prompt'] + ' ' + test['response_a'] + ' ' + test['response_b']\n\n'''\n# TF-IDF only\nvectorizer = TfidfVectorizer(max_features=10000)   \n\n# Fit and transform\nX_train_combined = vectorizer.fit_transform(combined_text_train)\nX_test_combined = vectorizer.transform(combined_text_test)\n\n# These are your final feature matrices\nfinal_train_features = X_train_combined\nfinal_test_features = X_test_combined\n'''\n\nfrom sklearn.decomposition import TruncatedSVD\n\n# TF-IDF + n-gram\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\nX_train_combined = vectorizer.fit_transform(combined_text_train)\nX_test_combined = vectorizer.transform(combined_text_test)\n\n# Truncated SVD\nsvd = TruncatedSVD(n_components=300, random_state=42)   \nX_train_svd = svd.fit_transform(X_train_combined)\nX_test_svd = svd.transform(X_test_combined)\n\nfinal_train_features = X_train_svd\nfinal_test_features = X_test_svd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:03:40.117941Z","iopub.execute_input":"2025-04-26T18:03:40.118236Z","iopub.status.idle":"2025-04-26T18:06:04.405348Z","shell.execute_reply.started":"2025-04-26T18:03:40.118215Z","shell.execute_reply":"2025-04-26T18:06:04.404412Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"from lightgbm import LGBMClassifier, early_stopping\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\n\n# Map the labels to numerical values (0: winner_model_a, 1: winner_model_b, 2: winner_tie)\ny = train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).map({'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2})\n\nX = final_train_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:06:04.406810Z","iopub.execute_input":"2025-04-26T18:06:04.407126Z","iopub.status.idle":"2025-04-26T18:06:04.428068Z","shell.execute_reply.started":"2025-04-26T18:06:04.407103Z","shell.execute_reply":"2025-04-26T18:06:04.427208Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:19:27.434954Z","iopub.execute_input":"2025-04-26T18:19:27.435260Z","iopub.status.idle":"2025-04-26T18:19:27.442175Z","shell.execute_reply.started":"2025-04-26T18:19:27.435236Z","shell.execute_reply":"2025-04-26T18:19:27.441357Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"array([[ 0.33207334,  0.02913317,  0.01880533, ..., -0.0445559 ,\n         0.07589928, -0.03753681],\n       [ 0.23079724,  0.00096624, -0.03016245, ...,  0.00855667,\n        -0.0114794 , -0.01806916],\n       [ 0.26256172, -0.06968897, -0.15364821, ...,  0.01698376,\n         0.02001671,  0.0183725 ],\n       ...,\n       [ 0.34269937,  0.03865303,  0.01756029, ..., -0.00663313,\n        -0.01787836,  0.01837855],\n       [ 0.28076092,  0.01727467, -0.03398818, ...,  0.00267414,\n        -0.01881258, -0.01036943],\n       [ 0.04053574,  0.00842621,  0.00371777, ..., -0.00353098,\n         0.0169585 , -0.01647963]])"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:19:31.026416Z","iopub.execute_input":"2025-04-26T18:19:31.027337Z","iopub.status.idle":"2025-04-26T18:19:31.034214Z","shell.execute_reply.started":"2025-04-26T18:19:31.027312Z","shell.execute_reply":"2025-04-26T18:19:31.033418Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"0        0\n1        1\n2        2\n3        0\n4        1\n        ..\n57472    0\n57473    0\n57474    0\n57475    1\n57476    0\nLength: 57477, dtype: int64"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"from lightgbm import LGBMClassifier, early_stopping, log_evaluation\nfrom sklearn.model_selection import ParameterSampler\n# 1. Define search space\nparam_grid = {\n    'min_child_samples': [10, 25, 50],\n    'n_estimators': [500, 1000],    # Number of boosting rounds\n    'max_depth': [10, 15],        # Limit the depth of trees\n}\n\nn_trials = 5  # How many hyperparameter combinations you want to try\nparam_sampler = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=42))\n\n# 2. Track best model\nbest_val_loss = np.inf\nbest_model = None\nbest_params = None\n\nfold = 0\ntrain_losses = []\nval_losses = []\nmodels = []\n\n# Set up cross-validation\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nfor params in param_sampler:\n    print(f\"\\n🎯 Trying parameters: {params}\")\n    \n    fold = 0\n    fold_train_losses = []\n    fold_val_losses = []\n    \n    for train_idx, val_idx in cv.split(X, y):\n        fold += 1\n        print(f\"\\n🔁 Fold {fold}\")\n\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Initialize model with sampled hyperparameters\n        model = LGBMClassifier(\n            objective='multiclass',\n            num_class=3,\n            random_state=42,\n            eval_metric='multi_logloss',\n            bagging_freq=5,\n            lambda_l2  = 0.5,\n            lambda_l1  = 0.5,\n            **params  # <<< inject sampled params here\n        )\n        \n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_train, y_train), (X_val, y_val)],\n            eval_names=['train', 'valid'],\n            callbacks=[\n                early_stopping(stopping_rounds=20),\n                log_evaluation(period=100)\n            ]\n        )\n\n        # Predict probabilities\n        train_probs = model.predict_proba(X_train)\n        val_probs = model.predict_proba(X_val)\n\n        # Compute log loss\n        train_logloss = log_loss(y_train, train_probs)\n        val_logloss = log_loss(y_val, val_probs)\n\n        print(f\"📊 Fold {fold} Log Loss - Train: {train_logloss:.4f}, Validation: {val_logloss:.4f}\")\n        fold_train_losses.append(train_logloss)\n        fold_val_losses.append(val_logloss)\n\n    avg_val_loss = np.mean(fold_val_losses)\n    print(f\"\\n🧠 Average Validation Log Loss for this parameter set: {avg_val_loss:.4f}\")\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model = model\n        best_params = params\n\n    models.append(model)\n    train_losses.append(np.mean(fold_train_losses))\n    val_losses.append(avg_val_loss)\n\n# 🔥 Best results\nprint(\"\\n✅ Hyperparameter Search Finished\")\nprint(f\"Best Validation Log Loss: {best_val_loss:.4f}\")\nprint(f\"Best Parameters: {best_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:06:04.429009Z","iopub.execute_input":"2025-04-26T18:06:04.429272Z","iopub.status.idle":"2025-04-26T18:09:09.992998Z","shell.execute_reply.started":"2025-04-26T18:06:04.429243Z","shell.execute_reply":"2025-04-26T18:09:09.991994Z"}},"outputs":[{"name":"stdout","text":"\n🎯 Trying parameters: {'n_estimators': 500, 'min_child_samples': 50, 'max_depth': 15}\n\n🔁 Fold 1\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081095 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[29]\ttrain's multi_logloss: 1.02134\tvalid's multi_logloss: 1.0848\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 1 Log Loss - Train: 1.0213, Validation: 1.0848\n\n🔁 Fold 2\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084457 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[30]\ttrain's multi_logloss: 1.02001\tvalid's multi_logloss: 1.08341\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 2 Log Loss - Train: 1.0200, Validation: 1.0834\n\n🔁 Fold 3\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087920 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073155\n[LightGBM] [Info] Start training from score -1.174436\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[35]\ttrain's multi_logloss: 1.00907\tvalid's multi_logloss: 1.08548\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 3 Log Loss - Train: 1.0091, Validation: 1.0855\n\n🧠 Average Validation Log Loss for this parameter set: 1.0846\n\n🎯 Trying parameters: {'n_estimators': 1000, 'min_child_samples': 25, 'max_depth': 15}\n\n🔁 Fold 1\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[37]\ttrain's multi_logloss: 1.00467\tvalid's multi_logloss: 1.08518\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 1 Log Loss - Train: 1.0047, Validation: 1.0852\n\n🔁 Fold 2\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[33]\ttrain's multi_logloss: 1.01383\tvalid's multi_logloss: 1.08241\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 2 Log Loss - Train: 1.0138, Validation: 1.0824\n\n🔁 Fold 3\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073155\n[LightGBM] [Info] Start training from score -1.174436\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[33]\ttrain's multi_logloss: 1.01248\tvalid's multi_logloss: 1.08462\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 3 Log Loss - Train: 1.0125, Validation: 1.0846\n\n🧠 Average Validation Log Loss for this parameter set: 1.0841\n\n🎯 Trying parameters: {'n_estimators': 500, 'min_child_samples': 10, 'max_depth': 10}\n\n🔁 Fold 1\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[31]\ttrain's multi_logloss: 1.0173\tvalid's multi_logloss: 1.085\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 1 Log Loss - Train: 1.0173, Validation: 1.0850\n\n🔁 Fold 2\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122035 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[33]\ttrain's multi_logloss: 1.01383\tvalid's multi_logloss: 1.08299\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 2 Log Loss - Train: 1.0138, Validation: 1.0830\n\n🔁 Fold 3\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073155\n[LightGBM] [Info] Start training from score -1.174436\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[29]\ttrain's multi_logloss: 1.02127\tvalid's multi_logloss: 1.08453\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 3 Log Loss - Train: 1.0213, Validation: 1.0845\n\n🧠 Average Validation Log Loss for this parameter set: 1.0842\n\n🎯 Trying parameters: {'n_estimators': 500, 'min_child_samples': 25, 'max_depth': 15}\n\n🔁 Fold 1\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088173 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[37]\ttrain's multi_logloss: 1.00467\tvalid's multi_logloss: 1.08518\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 1 Log Loss - Train: 1.0047, Validation: 1.0852\n\n🔁 Fold 2\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087901 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[33]\ttrain's multi_logloss: 1.01383\tvalid's multi_logloss: 1.08241\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 2 Log Loss - Train: 1.0138, Validation: 1.0824\n\n🔁 Fold 3\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172201 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073155\n[LightGBM] [Info] Start training from score -1.174436\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[33]\ttrain's multi_logloss: 1.01248\tvalid's multi_logloss: 1.08462\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 3 Log Loss - Train: 1.0125, Validation: 1.0846\n\n🧠 Average Validation Log Loss for this parameter set: 1.0841\n\n🎯 Trying parameters: {'n_estimators': 1000, 'min_child_samples': 50, 'max_depth': 10}\n\n🔁 Fold 1\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[40]\ttrain's multi_logloss: 1.00046\tvalid's multi_logloss: 1.08461\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 1 Log Loss - Train: 1.0005, Validation: 1.0846\n\n🔁 Fold 2\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083467 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073231\n[LightGBM] [Info] Start training from score -1.174352\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[31]\ttrain's multi_logloss: 1.01782\tvalid's multi_logloss: 1.08263\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 2 Log Loss - Train: 1.0178, Validation: 1.0826\n\n🔁 Fold 3\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086463 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 76500\n[LightGBM] [Info] Number of data points in the train set: 38318, number of used features: 300\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052458\n[LightGBM] [Info] Start training from score -1.073155\n[LightGBM] [Info] Start training from score -1.174436\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[39]\ttrain's multi_logloss: 1.00204\tvalid's multi_logloss: 1.0844\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 3 Log Loss - Train: 1.0020, Validation: 1.0844\n\n🧠 Average Validation Log Loss for this parameter set: 1.0839\n\n✅ Hyperparameter Search Finished\nBest Validation Log Loss: 1.0839\nBest Parameters: {'n_estimators': 1000, 'min_child_samples': 50, 'max_depth': 10}\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"print(train_losses)\nprint(val_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:09:09.994703Z","iopub.execute_input":"2025-04-26T18:09:09.994954Z","iopub.status.idle":"2025-04-26T18:09:09.999601Z","shell.execute_reply.started":"2025-04-26T18:09:09.994931Z","shell.execute_reply":"2025-04-26T18:09:09.998720Z"}},"outputs":[{"name":"stdout","text":"[1.0168071309667794, 1.0103297012343697, 1.0174670196476874, 1.0103297012343697, 1.0067726632650664]\n[1.084561788133965, 1.0840695212276608, 1.0841724213851165, 1.0840695212276608, 1.0838798875059048]\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"test_probs = best_model.predict_proba(final_test_features)  # test_features is the feature matrix for your test set\n\n# Predict the class labels on the test data\ntest_labels = best_model.predict(final_test_features)\n\n# Display the predicted probabilities (for each class)\nprint(\"Test Probabilities:\")\nprint(test_probs)\n\n# Display the predicted class labels (0, 1, or 2)\nprint(\"Test Class Labels:\")\nprint(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:09:10.000661Z","iopub.execute_input":"2025-04-26T18:09:10.001001Z","iopub.status.idle":"2025-04-26T18:09:10.021077Z","shell.execute_reply.started":"2025-04-26T18:09:10.000968Z","shell.execute_reply":"2025-04-26T18:09:10.020237Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\nTest Probabilities:\n[[0.26555315 0.27666264 0.45778421]\n [0.42781882 0.27943092 0.29275027]\n [0.35428346 0.3664045  0.27931204]]\nTest Class Labels:\n[2 0 1]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"prob_df = pd.DataFrame(test_probs, columns=['prompt', 'response_a', 'response_b'])\n\t\t\noutput = pd.concat([test['id'].reset_index(drop=True), prob_df], axis=1)\n#output.to_csv('submission.csv', index=False)\noutput.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:09:10.021885Z","iopub.execute_input":"2025-04-26T18:09:10.022193Z","iopub.status.idle":"2025-04-26T18:09:10.040749Z","shell.execute_reply.started":"2025-04-26T18:09:10.022155Z","shell.execute_reply":"2025-04-26T18:09:10.039823Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"        id    prompt  response_a  response_b\n0   136060  0.265553    0.276663    0.457784\n1   211333  0.427819    0.279431    0.292750\n2  1233961  0.354283    0.366404    0.279312","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.265553</td>\n      <td>0.276663</td>\n      <td>0.457784</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.427819</td>\n      <td>0.279431</td>\n      <td>0.292750</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.354283</td>\n      <td>0.366404</td>\n      <td>0.279312</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"### bag of words models are not giving very good score. Thus, meaningful emdeddings required:","metadata":{}},{"cell_type":"code","source":"import gc\ndel prob_df, test_probs, test_labels, final_train_features, X_train_svd, final_test_features, X_test_svd, X_train_combined, X_test_combined \ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:24:27.222836Z","iopub.execute_input":"2025-04-26T19:24:27.223340Z","iopub.status.idle":"2025-04-26T19:24:28.135711Z","shell.execute_reply.started":"2025-04-26T19:24:27.223316Z","shell.execute_reply":"2025-04-26T19:24:28.134976Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"636"},"metadata":{}}],"execution_count":80},{"cell_type":"markdown","source":"# trying embeddings as they also capture context \n## Con: takes too much memory that's why encoding embeddings in batches otherwise causing RAM full and crash","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('paraphrase-albert-small-v2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T18:21:41.208633Z","iopub.execute_input":"2025-04-26T18:21:41.208966Z","iopub.status.idle":"2025-04-26T18:21:45.397638Z","shell.execute_reply.started":"2025-04-26T18:21:41.208942Z","shell.execute_reply":"2025-04-26T18:21:45.396868Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4251f1ef8ac344b88ca70bcc920847c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b521d514ec9d46f9b417ff500c8d6d52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63f331609c24b298097b9a96c2e691a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff934989989740c5945e155f2387f052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa58780b52b437aa0f137cd97dce063"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd88d3d3670844e0adff43b2c74bc006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4f339e8725451f9da8cdebe5dfd086"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a55238f317764baaaa9be0ed5de98c08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12d30fca4d7848f9a22ab76b15348b79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b261319bb540589ada90641af5e37a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cb73fff50154d889d076977cd2d93c3"}},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"Why we encode separately for SentenceTransformer:\n\n    Sentence Transformers (like 'paraphrase-albert-small-v2') understand sentences and semantic meaning.\n\n    They generate one meaningful vector per input (whether it's a prompt, a response_a, or a response_b).\n\n    If you blindly combine 'prompt' + 'response_a' + 'response_b' into one text,\n    ➔ you will lose individual semantic meanings.\n    (e.g., model won't know which part is prompt and which is response)\n\nInstead:\n\n    You separately encode 'prompt', 'response_a', and 'response_b' into separate embeddings. Then concatenate the embeddings - This way you preserve the meaning of each part separately and let the model learn relationships between them!","metadata":{}},{"cell_type":"code","source":"prompt_embeds = model.encode(train['prompt'].tolist(), batch_size=2048, show_progress_bar=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:25:33.219317Z","iopub.execute_input":"2025-04-26T19:25:33.219637Z","iopub.status.idle":"2025-04-26T19:25:33.226779Z","shell.execute_reply.started":"2025-04-26T19:25:33.219616Z","shell.execute_reply":"2025-04-26T19:25:33.225973Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"array([[ 0.23200333,  0.58380276, -0.01959007, ..., -0.28125316,\n         0.5319085 , -0.49500197],\n       [ 0.13280816,  0.14608544, -0.44802836, ...,  0.90225863,\n         0.21633628,  0.3787431 ],\n       [-0.20034552, -0.6754557 ,  1.1417165 , ...,  0.31891087,\n        -0.00328754, -1.6726645 ],\n       ...,\n       [-0.28393358,  0.3422195 ,  0.5409369 , ...,  0.01849821,\n         0.33610758, -0.2706519 ],\n       [-0.02055523,  0.12156801, -0.66176605, ..., -0.46185187,\n        -0.28438604, -0.15869723],\n       [-0.3545313 ,  1.141768  , -1.1357027 , ...,  0.5116905 ,\n         0.993347  , -0.06641866]], dtype=float32)"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"print(prompt_embeds.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:26:56.795308Z","iopub.execute_input":"2025-04-26T19:26:56.795861Z","iopub.status.idle":"2025-04-26T19:26:56.801241Z","shell.execute_reply.started":"2025-04-26T19:26:56.795834Z","shell.execute_reply":"2025-04-26T19:26:56.800333Z"}},"outputs":[{"name":"stdout","text":"(57477, 768)\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"# Encode each column separately\nresponse_a_embeds = model.encode(train['response_a'].tolist(), batch_size=2048, show_progress_bar=True)\nresponse_b_embeds = model.encode(train['response_b'].tolist(), batch_size=2048, show_progress_bar=True)\n\n# Stack horizontally\nX = np.hstack([prompt_embeds, response_a_embeds, response_b_embeds])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:35:18.374846Z","iopub.execute_input":"2025-04-26T19:35:18.375159Z","iopub.status.idle":"2025-04-26T23:21:21.576238Z","shell.execute_reply.started":"2025-04-26T19:35:18.375127Z","shell.execute_reply":"2025-04-26T23:21:21.574273Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf9b68a436c4262b5dd3fdaa33171b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f139f9f4e940f3860f42bf39a04bcf"}},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"del prompt_embeds, response_a_embeds, response_b_embeds, train\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:21:21.580801Z","iopub.execute_input":"2025-04-26T23:21:21.581438Z","iopub.status.idle":"2025-04-26T23:21:23.888146Z","shell.execute_reply.started":"2025-04-26T23:21:21.581401Z","shell.execute_reply":"2025-04-26T23:21:23.887402Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"17"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"fold = 0\ntrain_losses = []\nval_losses = []\nmodels = []\n\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\nfor train_idx, val_idx in cv.split(X, y):\n    fold += 1\n    print(f\"\\n🔁 Fold {fold}\")\n\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    model_lgbm = LGBMClassifier(\n        objective='multiclass',\n        num_class=3,\n        n_estimators=1000,\n        random_state=42,\n        max_depth=10,\n        min_child_samples=50,\n        lambda_l1=0.5,\n        lambda_l2=0.5,\n        bagging_freq=5,\n        eval_metric='multi_logloss'\n    )\n\n    model_lgbm.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_val, y_val)],\n        eval_names=['train', 'valid'],\n        callbacks=[\n            early_stopping(stopping_rounds=20),\n            log_evaluation(period=100)\n        ]\n    )\n\n    models.append(model_lgbm)\n\n    # Predict probabilities\n    train_probs = model_lgbm.predict_proba(X_train)\n    val_probs = model_lgbm.predict_proba(X_val)\n\n    # Compute log loss\n    train_logloss = log_loss(y_train, train_probs)\n    val_logloss = log_loss(y_val, val_probs)\n\n    print(f\"📊 Fold {fold} Log Loss - Train: {train_logloss:.4f}, Validation: {val_logloss:.4f}\")\n    train_losses.append(train_logloss)\n    val_losses.append(val_logloss)\n\n# Summary\nprint(\"\\n✅ Cross-validated Results\")\nprint(f\"Mean Training Log Loss: {np.mean(train_losses):.4f}\")\nprint(f\"Mean Validation Log Loss: {np.mean(val_losses):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:21:50.265036Z","iopub.execute_input":"2025-04-26T23:21:50.265600Z","iopub.status.idle":"2025-04-26T23:26:40.863360Z","shell.execute_reply.started":"2025-04-26T23:21:50.265574Z","shell.execute_reply":"2025-04-26T23:26:40.862538Z"}},"outputs":[{"name":"stdout","text":"\n🔁 Fold 1\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.999549 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587520\n[LightGBM] [Info] Number of data points in the train set: 28738, number of used features: 2304\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052440\n[LightGBM] [Info] Start training from score -1.073188\n[LightGBM] [Info] Start training from score -1.174419\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[59]\ttrain's multi_logloss: 0.872411\tvalid's multi_logloss: 1.07443\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 1 Log Loss - Train: 0.8724, Validation: 1.0744\n\n🔁 Fold 2\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.117598 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587520\n[LightGBM] [Info] Number of data points in the train set: 28739, number of used features: 2304\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Info] Start training from score -1.052475\n[LightGBM] [Info] Start training from score -1.073223\n[LightGBM] [Info] Start training from score -1.174341\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[61]\ttrain's multi_logloss: 0.863744\tvalid's multi_logloss: 1.07292\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n📊 Fold 2 Log Loss - Train: 0.8637, Validation: 1.0729\n\n✅ Cross-validated Results\nMean Training Log Loss: 0.8681\nMean Validation Log Loss: 1.0737\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"## Despite very high computational cost (many times higher) - it is not doing much better than TF-IDF.","metadata":{}},{"cell_type":"code","source":"# Similarly for test\nprompt_embeds_test = model.encode(test['prompt'].tolist(), batch_size=2048, show_progress_bar=True)\nresponse_a_embeds_test = model.encode(test['response_a'].tolist(), batch_size=2048, show_progress_bar=True)\nresponse_b_embeds_test = model.encode(test['response_b'].tolist(), batch_size=2048, show_progress_bar=True)\n\ntest.drop(columns=['response_a','response_b','prompt'], inplace=True)\n\nX_test = np.hstack([prompt_embeds_test, response_a_embeds_test, response_b_embeds_test])\n\ndel prompt_embeds_test, response_a_embeds_test, response_b_embeds_test\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_probs = best_model.predict_proba(X_test)  # test_features is the feature matrix for your test set\n\n# Predict the class labels on the test data\ntest_labels = best_model.predict(X_test)\n\n# Display the predicted probabilities (for each class)\nprint(\"Test Probabilities:\")\nprint(test_probs)\n\n# Display the predicted class labels (0, 1, or 2)\nprint(\"Test Class Labels:\")\nprint(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:21:25.770912Z","iopub.status.idle":"2025-04-26T23:21:25.771152Z","shell.execute_reply.started":"2025-04-26T23:21:25.771034Z","shell.execute_reply":"2025-04-26T23:21:25.771045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prob_df = pd.DataFrame(test_probs, columns=['prompt', 'response_a', 'response_b'])\n\t\t\noutput = pd.concat([test['id'].reset_index(drop=True), prob_df], axis=1)\noutput.to_csv('submission.csv', index=False)\noutput.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:21:25.772055Z","iopub.status.idle":"2025-04-26T23:21:25.772317Z","shell.execute_reply.started":"2025-04-26T23:21:25.772193Z","shell.execute_reply":"2025-04-26T23:21:25.772204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}