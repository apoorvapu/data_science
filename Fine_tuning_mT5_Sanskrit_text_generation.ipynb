{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## takes ~2 days (CPU only) to fine-tune mT5 with 7000 sanskrit sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece\n",
    "# sentencepiece installation will require restarting kernel after installation to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "print(sentencepiece.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load mT5 Model and Tokenizer\n",
    "# ===============================\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbab18590f24ba4afd772ac3a82db17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'text': 'अनिरुद्धनगरे क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । तस्य कानिचन् चित्राणि पूर्वमेव प्रकाशितानि सन्ति । द्वौ चलचित्रौ अपि प्रकाशितौ । तस्मिन् एव क्रमेण एतत् सीतास्वयंबर इति चलचित्रं प्रकाश्यते ।'}\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Load and Clean Sanskrit Dataset\n",
    "# ===============================\n",
    "\n",
    "def clean_sanskrit_text(example):\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    # Remove zero-width characters and extra spaces\n",
    "    text = re.sub(r'[\\u200b-\\u200d]', '', text)         # Remove zero-width characters\n",
    "    text = re.sub(r'\\s+', ' ', text)                    # Collapse multiple spaces/newlines\n",
    "    text = text.strip()                                 # Trim leading/trailing whitespace\n",
    "     # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove non-Sanskrit characters (retain Devanagari script)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "   \n",
    "    # Optional: Remove grammar tables (if undesired)\n",
    "    grammar_markers = [\"लट् लकार\", \"लङ् लकार\", \"प्रथमपुरुष\", \"मध्यमपुरुष\", \"उत्तमपुरुष\"]\n",
    "    for marker in grammar_markers:\n",
    "        if marker in text:\n",
    "            text = text.split(marker)[0].strip()\n",
    "            break\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_sa', split='train[:100%]')\n",
    "\n",
    "dataset = dataset.map(clean_sanskrit_text)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7121"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'text': 'पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशनशेयरअलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति अन्याः संस्थित्यः अपि सन्ति । अधिकं ज्ञातुम् अत्र उपयोगस्य संस्थितिं पश्यतु ।'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 7121\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 46,\n",
       " 'text': 'क्रोधात् भवति सम्मोहः सम्मोहात् स्मृति विभ्रमः स्मृतिभृन्षात् बुद्धिनाशो बुद्धिनाशात् प्रनश्यते '}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9edfad7284420f8bdadf6ecff4561d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Tokenize the Dataset\n",
    "# ===============================\n",
    "max_input_length = MAX_LEN\n",
    "max_target_length = MAX_LEN\n",
    "\n",
    "def preprocess(example):\n",
    "    input_text = example[\"text\"]\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    input_ids[\"labels\"] = labels[\"input_ids\"]\n",
    "    return input_ids\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_completion(examples):\n",
    "    input_texts = [\"complete: \" + text[:100] for text in examples[\"text\"]]\n",
    "    target_texts = [text[100:200] if len(text) > 200 else text[-50:] for text in examples[\"text\"]]  # dummy completion\n",
    "    model_inputs = tokenizer(input_texts, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(target_texts, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared training arguments\n",
    "def get_training_args(output_dir):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=5e-4,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=5000,\n",
    "        save_steps=5000,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "# Trainer setup\n",
    "def train_model(preprocess_fn, output_dir, remove_cols):\n",
    "    tokenized_dataset = dataset.map(preprocess_fn, batched=True, remove_columns=remove_cols)\n",
    "    training_args = get_training_args(output_dir)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "# Task-specific training functions\n",
    "def train_completion():\n",
    "    print(\"Training for text completion...\")\n",
    "    train_model(preprocess_completion, \"./mt5-sanskrit-completion\", remove_cols=[\"text\"])\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate(task, test_input):\n",
    "    input_ids = tokenizer(test_input, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids=input_ids, max_length=512)\n",
    "    print(f\"\\n{task} Result:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for text completion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ede1b2d1d94d71a73f329d2eea52b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/var/tmp/ipykernel_19976/1713360364.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5278' max='7122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5278/7122 21:23:00 < 7:28:25, 0.07 it/s, Epoch 1.48/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.610600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint_dir = \"mt5-sanskrit-completion/checkpoint-5000/\"  # your saved checkpoint directory\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for sentences in training data, has no problem completing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति । क्रीडिता रामलीला सम्प्रति समाप्ता अस्ति ।\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"अनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति ।\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# but fine-tuning for 1 day (and 7000 sanskrit sentences) insufficient to complete sentences for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " यस्य धर्म एव मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः मुख्यः \n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"मनुष्यस्य धर्म एव मुख्यः\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " यः अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति । अस्ति ।\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " ् सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः। सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे सन्तु निरामयाः सर्वे\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"सर्वे भवन्तु सुखिनः, सर्वे सन्तु निरामयाः।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " ् । सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गोऽस्त्वकर्मणि। सङ्गो\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"कर्मण्येवाधिकारस्ते मा फलेषु कदाचन मा कर्मफलहेतुर्भूर्मा ते सङ्गोऽस्त्वकर्मणि।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -r logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
