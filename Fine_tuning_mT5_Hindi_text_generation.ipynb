{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## takes ~2 days (CPU only) to fine-tune mT5 with 7000 sanskrit sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece\n",
    "# sentencepiece installation will require restarting kernel after installation to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "print(sentencepiece.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load mT5 Model and Tokenizer\n",
    "# ===============================\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Load and Clean Sanskrit Dataset\n",
    "# ===============================\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_hi', split='train[:1%]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7702801496743d3815b0290c1f71505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7495fd6a7d724109895ca9cb790b6bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 26, 'text': 'देश के कम से कम राज्यों और दो केंद्र शासित प्रदेशों में हुए रेड अलर्टजी हां मौसम विभाग के मुताबिक सोमवार को आंधीतूफान भारी बारिश और ओलावृष्टि होने की हैं सम्भावना वही गृह मंत्रालय ने भी जानकारी देते हुए कहा हैं कि मौसम विभाग ने अलर्ट जारी किया हैं कि कश्मीर और हिमाचल प्रदेश के कुछ स्थानों पर सोमवार को आंधीतूफान और बारिश के साथ ओलावर्ष्टि हो सकती हैवहीं उत्तराखंड और पंजाब के कुछ स्थानों पर भारी बारिश और तेज हवाओं की संभावना हैं मौसम विभाग द्वारा जारी अलर्ट के बाद हरियाणा के सभी सरकारी और निजी स्कूलों को और मई को बंद रखने की घोषणा की गई हैं भारतीय मौसम विभाग के एक परामर्श का उल्लेख करते हुए गृह मंत्रालय के एक अधिकारी ने बताया कि असम मेघालय नगालैंड मणिपुर मिजोरम और त्रिपुरा के कुछ स्थानों पर भारी बारिश हो सकती है । वही अधिकारी ने यह भी बताया हैं कि जम्मूकश्मीर उत्तराखंड हिमाचल प्रदेश पंजाब हरियाणा चंडीगढ़ दिल्ली व एनसीआर और उत्तर प्रदेश के कुछ स्थानों पर भी बादल गरजने व बारिश होने और तेज हवाएं चलने की आशंका है इस रेड अलर्ट के जारी होने के बाद हम सभी की जिम्मेदारी बनती हैं कि झूटी अफवाहों से बचें और सावधानी बरतें साथ ही सक्षम लोग मौसम विभाग की वेबसाइड पर पर नज़र बनायें रखें । प्रधानमंत्री ने अपनी विज्ञान प्रौद्योगिकी तथा नवाचार सलाहकार परिषद पीएमएसटीआईएसी के सदस्यों के साथ बातचीत की पिगमेंटेशन या फिर आंखों के नीचे काले घेरे आपके पूरे चेहरे की रौनक खत्म करके उसे बेजान बना देते हैं आमतौर पर नींद की कमी सूरज की रौशनी में ज़्यादा समय'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_sanskrit_text(example):\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    # Remove zero-width characters and extra spaces\n",
    "    text = re.sub(r'[\\u200b-\\u200d]', '', text)         # Remove zero-width characters\n",
    "    text = re.sub(r'\\s+', ' ', text)                    # Collapse multiple spaces/newlines\n",
    "    text = text.strip()                                 # Trim leading/trailing whitespace\n",
    "     # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove non-Sanskrit characters (retain Devanagari script)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(clean_sanskrit_text)\n",
    "\n",
    "def filter_by_word_count(example):\n",
    "    word_count = len(example['text'].split())\n",
    "    return 150 < word_count < 350\n",
    "\n",
    "dataset = dataset.filter(filter_by_word_count)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5439"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 30,\n",
       " 'text': 'सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया भारतीय सेक्स सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया की छवियां सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया वीडियो विवरण सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया आप अश्लील वीडियो देख रहे हैं सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया। यह वीडियो सबसे बड़ी अश्लील वेबसाइट भारतीय सेक्स पर पोस्ट किया गया है। वीडियो की लंबाई के बारे में है। आप खोज बॉक्स का उपयोग करके अधिक गर्म अश्लील वीडियो खोज सकते हैं। मज़े करो सींग का नशे में चचेरे भाई बहन अंजलि ने मेरे दोस्त त्रिगुट और मीसेक्स सेक्स के साथ पार्टी के बाद बकवास किया सुंदर लवली लालसा और विकृत लड़की अपने पैर खोलने के लिए खुशी का कारण बनती है और हस्तमैथुन अपनी प्यारी मुर्गा को रगड़ती है जबकि साल में एक सेक्स खिलौना फंसने के दौरान और आरआरआर प्रोफाइल देखें और शुल्क सेक्स'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 5439\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 193,\n",
       " 'text': 'तेहरान। दक्षिणपूर्वी ईरान में शुक्रवार को तीव्रता के भूकंप के कारण एक महिला की मौत हो गई और कई गांवों में कई घर क्षतिग्रस्त हो गए। ईरान के सरकारी मीडिया ने यह जानकारी दी है। रेड क्रिसेंट सोसायटी के प्रांतीय प्रमुख रसूल रश्की ने सरकारी संवाद समिति इरना को बताया कि भूकंप के बाद कई झटके महसूस किये गए और इसके कारण सदमे से एक वर्षीय महिला की मौत हो गई। रश्की ने बताया कि स्थानीय रिपोर्ट्स के अनुसार दो अन्य लोगों की मौत हुई है जिसकी तत्काल पुष्टि नहीं हो सकी है। सरकारी टेलीविजन ने आपदा प्रबंधन के प्रांतीय प्रमुख अब्देल रहमान शाहनवाजी के हवाले से बताया कि कई गांवों में कुछ घर क्षतिग्रस्त हुए हैं। अमेरिकी भूगर्भ सर्वेक्षण संस्थान ने बताया कि भूकंप की तीव्रता रिक्टर स्केल पर मापी गई और इसका केंद्र जेहादन शहर से किलो मीटर दक्षिण पश्चिम में जमीन से किलो मीटर की गहराई पर था। हर बार पार्टी में आप एक ही तरह का हेयर स्टाइल रखकर बोर हो गई हैं और अब तो आपके पति भी आपसे कहने लग पीएम मोदी ने वाराणसी में किया देश के पहले मल्टी मॉडल टर्मिनल का उद्घाटन अब गंगा के रास्ते चलेंगे जहाज बीजेपी सांसद मनोज तिवारी का विवादित बयान अगर सोनिया गांधी ने छठ पूजा की होती तो बुद्धिमान बच्चा पैदा होता सीएम रमन सिंह ने बोला कांग्रेस अध्यक्ष पर हमला कहा लोगों के लिए एक प्रकार के मनोरंजन हैं राहुल गांधी एक बार फिर वृंदावन पहुंचे तेजप्रताप यादव बोले मुझे अपनी जिंदगी जी लेने दो भाई मैं शांति की तलाश में हूं तेलंगाना में बीजेपी ने जारी किया घोषणापत्र किया बड़ा वादा कहा हर साल मुफ्त में बाटेंगे एक लाख गाय पत्रकार जमाल खशोगी की हत्या को लेकर बड़ा खुलासा लाश को पहले तेजाब से जलाया फिर नाली में फेंक दिया नई दिल्ली। दिल्ली सर्राफा बाजार में सोमवार को सोना रुपए चमककर रुपए प्रति दस ग्राम पर पहुंच टेस्ट श्रृंखला के सभी मैचों में टास हारने वाले तीसरे भारतीय कप्तान बने विराट कोहली'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc15c8318aa04128a45d1ecfed5b4ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Tokenize the Dataset\n",
    "# ===============================\n",
    "max_input_length = MAX_LEN\n",
    "max_target_length = MAX_LEN\n",
    "\n",
    "def preprocess(example):\n",
    "    input_text = example[\"text\"]\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    input_ids[\"labels\"] = labels[\"input_ids\"]\n",
    "    return input_ids\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_completion(examples):\n",
    "    input_texts = [\"complete: \" + text[:145] for text in examples[\"text\"]]\n",
    "    target_texts = [ text[145:] for text in examples[\"text\"]]  # dummy completion\n",
    "    model_inputs = tokenizer(input_texts, padding=\"max_length\", truncation=True, max_length=max_input_length,  return_attention_mask=True, \n",
    "                             add_special_tokens=True)\n",
    "    \n",
    "    #print(input_texts[0], \" --- \", target_texts[0])\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(target_texts, padding=\"max_length\", truncation=True, max_length=max_target_length,  return_attention_mask=True, \n",
    "                             add_special_tokens=False)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared training arguments\n",
    "def get_training_args(output_dir):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=5e-4,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=5000,\n",
    "        save_steps=5000,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "# Trainer setup\n",
    "def train_model(preprocess_fn, output_dir, remove_cols):\n",
    "    tokenized_dataset = dataset.map(preprocess_fn, batched=True, remove_columns=remove_cols)\n",
    "    training_args = get_training_args(output_dir)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "# Task-specific training functions\n",
    "def train_completion():\n",
    "    print(\"Training for text completion...\")\n",
    "    train_model(preprocess_completion, \"./mt5-sanskrit-completion\", remove_cols=[\"text\"])\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate(task, test_input):\n",
    "    input_ids = tokenizer(test_input, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids=input_ids, max_length=512)\n",
    "    print(f\"\\n{task} Result:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for text completion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320eb55fdbff43bab09947da7906f2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_6740/255034336.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2720' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2720/2720 11:23:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import (\\n    MT5ForConditionalGeneration,\\n    MT5Tokenizer,\\n    Seq2SeqTrainer,\\n    Seq2SeqTrainingArguments,\\n    DataCollatorForSeq2Seq\\n)\\nfrom datasets import load_dataset, Dataset\\nimport numpy as np\\nimport torch\\nimport re\\nfrom transformers import AutoTokenizer\\n\\ncheckpoint_dir = \"mt5-sanskrit-completion/checkpoint-5000/\"  # your saved checkpoint directory\\n\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\\nmodel = MT5ForConditionalGeneration.from_pretrained(checkpoint_dir)\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint_dir = \"mt5-sanskrit-completion/checkpoint-5000/\"  # your saved checkpoint directory\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " ा यहाँ ही हमने देखा कि मैं उसकी बात और बात थी कि मैं उसकी बात और बात थी कि मैंने उसकी बात और बात ही थी कि मैंने उसकी बात और उसकी बात और उसकी बात थी कि मैं उसकी बात कर ली है कि मैंने उसकी बात और उसकी बात और बात ही जिसकी बात हमेशा से ही हमने हमने कहा कि मैंने उसकी बात और बातचीत में हमारे बारे में बात करते हुए बताया कि मैंने उसकी बात बात करते हुए उसने बताया कि मैं उसकी बात सुनते हुए हमने बताया कि मैंने हमारे बारे में बताया कि मैंने उसकी बात करते हुए मैं उसकी बात और उसकी बात और उसकी बात करते हुए हमने उसकी बात सुनकर हमने बताया कि मैंने उसकी बात की बात करते थे कि मैंने उसकी बात नही कर पाई और मैं उसकी बात बात और हमेशा हमने बात करते हुए मैं यही बात करते हुए मैंने उसकी बात करते हुए जिसमें मैं उसकी बात और उसकी बात सुनते हुए उसकी बात ही यह बात करते हुए मैं उसकी बात करतें थीं मैंने उसकी बात और मैंने ही बात करते हुए उसकी बात और बात करते हुए उसकी बात और उसकी बात और वो बात करते हुए नजर आते हुए और मैंने बात करते हुए हमने यह बात करते हुए हम बात करते हुए कहा कि मैंने यह बात हमारे बारे में यह बात बताया मैंने जब\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"आज सुबह जब मैं पार्क गया, तो मैंने देखा कि\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# but fine-tuning for 1 day (and 5000 sentences) insufficient to complete sentences for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " े ही स्कूल से लौटते हुए लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते समय स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते समय स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल के लौटते समय स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल के लौटते हुए स्कूल से लौटने लग रहे स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटने के बाद स्कूल से लौटते हुए स्कूल से लौटते हुए स्कूल से लौटते समय स्कूल से लौटते हु\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"वह बच्चा स्कूल से लौटते समय\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " कर रही थी। जबकि लोगों ने यहां से ही ही बात कर लिया है। लोगों ने यहां तक ही यह बात करने की बात थी। जबकि लोगों ने यह बात करते हुए उसने यह जान किया था। जिससे लोगों ने उसकी बातें तो यह भी कहा कि जब लोगों ने यह बताया कि हमने तक लोगों ने यह बताया कि वह अपने पास ही ही बात कर लिया था। जबकि लोगों ने यह जानना चाहते थे कि वह उसकी वजह से ही यह जानकारी दी। यह जानकारी यह है कि वह हमारे पास ही यह जानकारी दी जिससे हमारे पास थोड़ी तेज हो गई है। यहां ही लोगों ने उसने बात करते हुए कहा कि हमारे पास ही उसने यह जानकारी दी। जिससे वो यह जानकारी दी थी कि वह लोगों को यह जानना चाहते हैं कि वह लोगों को यह बात करते हुए यह जानकारी दी थी कि वह यहां से बात कराया और वह नजर आए। उसने बात करते हुए कहा कि वो उसने ही यह जानकारी दी। जिससे वह उसकी बात करते हुए यह जानकारी दी थी कि वह यहां ही ही उसने यह जानकारी दी थी कि लोगों को यह जानकारी दी। जिसकी वजह से यह बात ही बात करते हुए उसने बातचीत करते हुए यह बात कर रही थी कि यह जानकारी दी थी कि वो जिसमें उसने हमारे पास ही यह जानकारी दी है कि वह बात करते हुए थे कि हमारे पास हमारे पास ही नजर रखते हुए उन्हें यह जानते थे कि लोगों को ही उस\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"बारिश इतनी तेज हो रही थी कि लोग\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " ा रहा और अब तक ही तुम ही याद ही तुम ही याद ही वो याद ही ही तुम्हारी याद मेरे लिए याद ही हम आपके लिए हम आपको ही बताए ही वो हम आपको याद ही हम आपको याद ही हमेशा याद ही हम आपके लिए हमने हम आपको ही प्यारी याद मेरे लिए हमारे लिए हम आपको ही प्यारी ही याद रखते हैं हम आपको अपनी याद ही याद ही याद ही हम आपको याद ही हम आपको ही याद ही तुम्हारी याद रखने की सबसे प्यारी याद ही याद ही तुम्हारी याद मेरी याद ही हम आपके लिए याद ही हम आपके लिए नई बात है हम आपके लिए हम आपको याद रखता है हम आपको ही हम आपको याद किया हम आपके लिए याद ही याद रखें हम आपको ही याद रखें हम आपको हमेशा ही प्यारी याद हम आपके लिए ही याद रखें हम आपके लिए प्यारी याद रखें हम आपको ही याद रखें हम आपके लिए हम आपके लिए हम सबसे पसंद जायेगा हम आपको याद रखें हम आपको हमारे लिए हम आपको यह बात हम आपको याद रखें हम आपको पसंद है हम आपके लिए हम आपको ही प्यारी याद हम ही हम आपके लिए हम आपको याद रखता है हम आपको याद ही हम आपके लिए यही हमेशा ही याद हम आपके लिए हम आपके लिए हमेशा ही प्यारी याद हम आपको याद रखें हम आपके पास ही प्यारी न ही याद मेरे लिए हम आप\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"बचपन की सबसे प्यारी याद मेरे लिए\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Completion Result:\n",
      " चिपक रही थी और उसने उस और उसने साथ ही ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ दिया। श्रीराम ने माता और साथ साथ साथ ही साथ ही साथ साथ ही साथ ही साथ ही साथ ही श्रीराम ने माता और साथ ही ही साथ साथ साथ ही साथ ही साथ ही साथ ही साथ ही साथ साथ ही साथ ही साथ ही साथ साथ ही ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ साथ ही साथ ही साथ साथ ही साथ साथ ही साथ ही ही साथ ही साथ ही साथ ही साथ साथ ही साथ ही साथ ही साथ ही साथ ही साथ साथ ही साथ ही साथ ही साथ साथ साथ ही साथ ही साथ ही श्रीराम ने साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही ही साथ ही साथ ही साथ ही साथ साथ ही साथ साथ ही साथ ही ही साथ साथ ही साथ ही साथ ही साथ ही ही साथ ही साथ ही साथ ही साथ ही ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही ही साथ ही साथ ही साथ साथ ही साथ ही ही साथ साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ ही साथ \n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Text Completion\", \"वनवास के समय श्रीराम ने माता सीता और लक्ष्मण के साथ मिलकर\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -r logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
