{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece\n",
    "# sentencepiece installation will require restarting kernel after installation to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "print(sentencepiece.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read)))\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load mT5 Model and Tokenizer\n",
    "# ===============================\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Load and Clean Sanskrit Dataset\n",
    "# ===============================\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_hi', split='train[:5%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b753cb910624a499c8f3a917b9f4973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/95469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 6, 'text': 'आप अश्लील वीडियो देख रहे हैं देसी बड़े स्तन बंगाली गृहिणीबंगाली अश्लील। यह वीडियो  सबसे बड़ी अश्लील वेबसाइट भारतीय सेक्स पर पोस्ट किया गया है। वीडियो की लंबाई   के बारे में   है। आप खोज बॉक्स का उपयोग करके अधिक गर्म अश्लील वीडियो खोज सकते हैं। मज़े करो  '}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_sanskrit_text(example):\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    # Remove zero-width characters and extra spaces\n",
    "    text = re.sub(r'[\\u200b-\\u200d]', '', text)         # Remove zero-width characters\n",
    "    text = re.sub(r'\\s+', ' ', text)                    # Collapse multiple spaces/newlines\n",
    "    text = text.strip()                                 # Trim leading/trailing whitespace\n",
    "     # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove non-Sanskrit characters (retain Devanagari script)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(clean_sanskrit_text)\n",
    "\n",
    "def filter_by_word_count(example):\n",
    "    word_count = len(example['text'].split())\n",
    "    return 15 < word_count < 50\n",
    "\n",
    "dataset = dataset.filter(filter_by_word_count)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15673"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10,\n",
       " 'text': ' प्रथम द्वितीय तृतीय चतुर्थी पंचमी सृष्टि सप्तमी अस्टमी नवमी दसमी एकादसी द्वादसी त्रयोदसी चतुर्दसी पंचदसी पूर्णिमा अमावस्या'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 15673\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 245,\n",
       " 'text': 'कुंवर सीपी सिंह युवा टीवी पत्रकार कुंवर सीपी सिंह के बारे में सूचना मिल रही है कि उनका निधन हो गया बताया जाता है कि वे रात में सोए तो'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49315e6c8fec44a6b79c7a9152071b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 5. Tokenize the Dataset\n",
    "# ===============================\n",
    "max_input_length = MAX_LEN\n",
    "max_target_length = MAX_LEN\n",
    "\n",
    "def preprocess(example):\n",
    "    input_text = example[\"text\"]\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    input_ids[\"labels\"] = labels[\"input_ids\"]\n",
    "    return input_ids\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_completion(examples):\n",
    "    new_examples = {\"input\": [], \"target\": []}\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        words = text.strip().split()\n",
    "        # Split roughly in the middle to create a meaningful prompt-completion pair\n",
    "        split_point = len(words) // 2\n",
    "        input_part = \" \".join(words[:split_point])\n",
    "        target_part = \" \".join(words[split_point:])\n",
    "        \n",
    "        #print(input_part, \" ---- \", target_part)\n",
    "        \n",
    "        new_examples[\"input\"].append(input_part)\n",
    "        new_examples[\"target\"].append(target_part)\n",
    "\n",
    "    # Tokenize input\n",
    "    model_inputs = tokenizer(\n",
    "        new_examples[\"input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    # Tokenize target\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            new_examples[\"target\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_target_length,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments  # not TrainingArguments\n",
    "\n",
    "def get_training_args(output_dir=\"./outputs\"):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=5e-4,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=1000,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "\n",
    "# Trainer setup\n",
    "def train_model(preprocess_fn, output_dir, remove_cols):\n",
    "    tokenized_dataset = dataset.map(preprocess_fn, batched=True, remove_columns=remove_cols)\n",
    "    training_args = get_training_args(output_dir)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "# Task-specific training functions\n",
    "def train_completion():\n",
    "    print(\"Training for text completion...\")\n",
    "    train_model(preprocess_completion, \"./mt5-sanskrit-completion\", remove_cols=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for text completion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4229950149df46ac8926a50b1305aad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/var/tmp/ipykernel_6861/788856752.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [490/490 4:20:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import (\\n    MT5ForConditionalGeneration,\\n    MT5Tokenizer,\\n    Seq2SeqTrainer,\\n    Seq2SeqTrainingArguments,\\n    DataCollatorForSeq2Seq\\n)\\nfrom datasets import load_dataset, Dataset\\nimport numpy as np\\nimport torch\\nimport re\\nfrom transformers import AutoTokenizer\\n\\ncheckpoint_dir = \"mt5-sanskrit-completion/checkpoint-5000/\"  # your saved checkpoint directory\\n\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\\nmodel = MT5ForConditionalGeneration.from_pretrained(checkpoint_dir)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint_dir = \"mt5-sanskrit-completion/checkpoint-5000/\"  # your saved checkpoint directory\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def evaluate(prompt):\n",
    "    input_text = prompt\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids=input_ids, max_length=MAX_LEN)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nInput:\", prompt)\n",
    "    print(\"Output:\", result)\n",
    "    #print(outputs, outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning not sufficient - to generate meaningful sentences - will take hundreds of thousands of lines of text and many epochs, which is not achievable with current resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: प्रथम द्वितीय तृतीय चतुर्थी पंचमी सृष्टि सप्तमी\n",
      "Output: के रई हमले\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate(\"प्रथम द्वितीय तृतीय चतुर्थी पंचमी सृष्टि सप्तमी\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: दीवाली इस साल नवंबर को बुधवार को पड रही है\n",
      "Output: के और\n"
     ]
    }
   ],
   "source": [
    "#'दीवाली इस साल नवंबर को बुधवार को पड रही है। इस दिन माता लक्ष्मी भगवान गणेश व कुबेर जी की पुजा'\n",
    "evaluate(\"दीवाली इस साल नवंबर को बुधवार को पड रही है\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: आज सुबह जब मैं पार्क गया, तो मैंने देखा कि\n",
      "Output: करते हुस ही और जब जब उन्हाँ ही पास ही जाने जब और साथ पहचान जब कुछ और साथ बातों\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"आज सुबह जब मैं पार्क गया, तो मैंने देखा कि\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: वह बच्चा स्कूल से लौटते समय\n",
      "Output: में लौट लौट दैदिक दैर्ध्य रहा था। वह और साल वह लम्बी वह हालीम और लौटकर और लौते हैं।\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"वह बच्चा स्कूल से लौटते समय\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: बारिश इतनी तेज हो रही थी कि लोग\n",
      "Output: और उसकी जाने वाले ही दैवी और लिए ही थी और और थी ।\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"बारिश इतनी तेज हो रही थी कि लोग\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: बचपन की सबसे प्यारी याद मेरे लिए\n",
      "Output: <extra_id_0> वह हमेशा याद दिया है। उस थी और हम सब जाने और तुम जाने और इस बात और उस ही हम उन्हें और बाद जाने जाने और जाने जाने और\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"बचपन की सबसे प्यारी याद मेरे लिए\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: वनवास के समय श्रीराम ने माता सीता और लक्ष्मण के साथ मिलकर\n",
      "Output: ही पसली और उन्हें और और\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"वनवास के समय श्रीराम ने माता सीता और लक्ष्मण के साथ मिलकर\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
