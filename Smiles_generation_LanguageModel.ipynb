{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTmX6zvgHp6nWGxk02bANb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/Smiles_generation_LanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAm1Y1QsN68N",
        "outputId": "1f1a136c-5fc5-4ce2-8ff0-78f2917b7c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, rdkit, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 rdkit-2024.9.6 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets rdkit pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "from rdkit import Chem"
      ],
      "metadata": {
        "id": "5uOHDPJVUc6u"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========== Step 1: Load Dataset ==========\n",
        "# Example: Load SMILES strings from CSV or .smi file\n",
        "def load_smiles(file_path, max_length=100):\n",
        "    df = pd.read_csv(file_path)\n",
        "    smiles_list = df['smiles'].dropna().unique().tolist()\n",
        "    smiles_list = [s for s in smiles_list if len(s) <= max_length and Chem.MolFromSmiles(s)]\n",
        "    return smiles_list\n",
        "\n",
        "# Example dataset\n",
        "smiles = [\n",
        "    \"CC(=O)OC1=CC=CC=C1C(=O)O\",\n",
        "    \"C1=CC=C(C=C1)C=O\",\n",
        "    \"CCN(CC)CCOC(=O)C1=CC=CC=C1Cl\",\n",
        "    \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\",\n",
        "    \"CCOC(=O)C1=CC=CC=C1OC\",\n",
        "    \"CC(C)C1=CC=C(C=C1)C(C)C(=O)NC\",\n",
        "    \"COC1=CC=CC=C1OC\",\n",
        "    \"CC(C)C(=O)NC1=CC=C(C=C1)Cl\",\n",
        "    \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",\n",
        "    \"CCC(CC)COC(=O)C1=CC=CC=C1\",\n",
        "    \"CC1=CC(=O)NC(=O)N1\",\n",
        "    \"CC(C)OC(=O)C1=CC=CC=C1Cl\",\n",
        "    \"CN(C)C(=O)C1=CC=C(C=C1)Cl\",\n",
        "    \"COC1=CC=CC=C1C(=O)O\",\n",
        "    \"C1=CC=C(C=C1)N\",\n",
        "    \"CCOC(=O)C1=CC=CC=C1F\",\n",
        "    \"CN(C)C(=O)C1=CC=C(C=C1)OC\",\n",
        "    \"CCC(=O)OC1=CC=CC=C1C(=O)O\",\n",
        "    \"CC(C)NC1=CC=C(C=C1)OC\",\n",
        "    \"C1=CC(=CC=C1C=O)O\",\n",
        "    \"CC1=CC(=O)NC(=O)N1C\",\n",
        "    \"CCC(C)OC(=O)C1=CC=CC=C1\",\n",
        "    \"CCOC(=O)C1=CC=CC=C1Cl\",\n",
        "    \"CN1C=NC2=C1C(=O)NC(=O)N2\",\n",
        "    \"CC(C)OC(=O)C1=CC=CC=C1F\",\n",
        "    \"C1=CC=C2C(=C1)C=CC=C2\",\n",
        "    \"C1=CC(=CC=C1C=O)Cl\",\n",
        "    \"CCN(CC)CCOC(=O)C1=CC=CC=C1F\",\n",
        "    \"CC(C)C1=CC=C(C=C1)O\",\n",
        "    \"COC1=CC=C(C=C1)C=O\",\n",
        "    \"CCOC(=O)C1=CC=CC=C1NO\",\n",
        "    \"CC(C)OC(=O)C1=CC=CC=C1Br\",\n",
        "    \"CCC(=O)OC1=CC=CC=C1F\",\n",
        "    \"COC1=CC=CC(=C1)C=O\",\n",
        "    \"CCC(C)OC(=O)C1=CC=CC=C1Cl\",\n",
        "    \"CN1C=NC2=C1C(=O)N(C(=O)N2)C\",\n",
        "    \"CC(C)CC1=CC=CC=C1O\",\n",
        "    \"CCOC(=O)C1=CC=CC=C1Br\",\n",
        "    \"COC1=CC=C(C=C1)C(=O)O\",\n",
        "    \"CC(C)OC(=O)C1=CC=CC=C1N\",\n",
        "    \"CN(C)C(=O)C1=CC=C(C=C1)F\",\n",
        "    \"CCC(=O)OC1=CC=CC=C1NO\",\n",
        "    \"COC1=CC=CC=C1C=O\",\n",
        "    \"CCOC(=O)C1=CC=CC=C1NO2\",\n",
        "    \"CN1C=NC2=C1C(=O)NC(=O)N2C\",\n",
        "    \"CC(C)OC(=O)C1=CC=CC=C1CN\",\n",
        "    \"C1=CC=C(C=C1)C(=O)O\",\n",
        "    \"C1=CC=C(C=C1)Br\",\n",
        "    \"COC1=CC=CC=C1OC\",\n",
        "]\n",
        "\n",
        "# Save to file for training\n",
        "with open(\"smiles.txt\", \"w\") as f:\n",
        "    for s in smiles:\n",
        "        f.write(s + \"\\n\")"
      ],
      "metadata": {
        "id": "_Hnok2VSOER9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Step 2: Tokenizer ==========\n",
        "# GPT-2 uses byte-level BPE tokenizer; we adapt it to SMILES\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Needed for batching\n",
        "tokenizer.add_special_tokens({'bos_token': '<bos>', 'eos_token': '<eos>'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRguNxAsOEWG",
        "outputId": "978e1c6f-ec07-4403-dce5-9d3435c0b408"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Step 3: Prepare Dataset ==========\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"smiles.txt\",\n",
        "    block_size=128,\n",
        ")\n",
        "print(f\"Loaded {len(dataset)} samples\")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRf2rA6fOg8O",
        "outputId": "31a5ff3c-fddd-4a78-ff0a-de9d08ef0c6f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 49 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Step 4: Load GPT2 and Fine-Tune ==========\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wUketPOOkmC",
        "outputId": "635e5c08-0060-4de4-8595-b38b03ee13ac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./smiles-gpt2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=50,\n",
        "    save_steps=5000,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=5000,\n",
        "    prediction_loss_only=True,\n",
        "    learning_rate=5e-4,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ========== Step 5: Save Model ==========\n",
        "#trainer.save_model(\"./smiles-gpt2\")\n",
        "#tokenizer.save_pretrained(\"./smiles-gpt2\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "iynEmfh9N8sm",
        "outputId": "0fc91ac9-7b06-4cb0-aeb6-ba008f7bba41"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 01:59, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5, training_loss=3.1902097702026366, metrics={'train_runtime': 141.7194, 'train_samples_per_second': 1.729, 'train_steps_per_second': 0.035, 'total_flos': 3250840320000.0, 'train_loss': 3.1902097702026366, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========== Step 6: Generate Novel SMILES ==========\n",
        "\n",
        "def generate_smiles(model, tokenizer, prompt=\"C\", num_return_sequences=5, max_length=64, retry_limit=5):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    smiles_list = []\n",
        "\n",
        "    for _ in range(retry_limit):\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,  # Adjust temperature for better results\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        for i in range(num_return_sequences):\n",
        "            decoded = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "            smiles = decoded.strip().split(\"\\n\")[0].strip()\n",
        "\n",
        "            if smiles:\n",
        "                mol = Chem.MolFromSmiles(smiles)\n",
        "                if mol:\n",
        "                    smiles_list.append(smiles)\n",
        "                else:\n",
        "                    print(f\"Invalid SMILES skipped: {smiles}\")\n",
        "\n",
        "        if smiles_list:  # If at least one valid SMILES is generated, break loop\n",
        "            break\n",
        "\n",
        "    return smiles_list\n",
        "\n",
        "\n",
        "\n",
        "generated_smiles = generate_smiles(model, tokenizer, num_return_sequences=10)\n",
        "\n",
        "# ========== Step 7: Validate and Print ==========\n",
        "valid_smiles = [s for s in generated_smiles if Chem.MolFromSmiles(s)]\n",
        "print(\"Generated Valid SMILES:\")\n",
        "for s in valid_smiles:\n",
        "    print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IYZIEcOODCz",
        "outputId": "8a329a80-a23f-4911-903c-4841935f22ca"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid SMILES skipped: COC=CCCCCCCC1CCCC=CC1CC=CC=CC=CC(CC1)CC(CC1=CC)CC(CC1)1)CC1C1O(CC(CC(CC1)C)1)CC1(CC1(CCCC1\n",
            "Invalid SMILES skipped: C(((CC)CC)(O)CCCC(CC1)(CCCC1=CC)O)CC)CC(C(C)CC(CC)CC1CC1CC)CCCC)(1CC1(C1)CC1CC1CC)1(CC\n",
            "Invalid SMILES skipped: C(N1(C1)O)(1C1)1(C(1(1)C((1)N1))(CC(CC(CC)CC1(C(C(CC))CC11O)C1(1)1(CC1)\n",
            "Invalid SMILES skipped: C(C)CC1C((CC)CC1CC(CCCC)CC1CCCC(CC)CC1C(CC)(C)1C(1)CC1)CC(CC)O1(1CC)CC)C1CC(1)CC1CC1\n",
            "Invalid SMILES skipped: C()O)CC1(CC=CC(CC)CC)CC1C1CC1CC=CC1CCCC)C1C)CC1CC1(CC)1)CC1CC1CCCCCCCCCC(1)CC1CC1CC(CC)CCCC\n",
            "Invalid SMILES skipped: COC=OC(=CC1)CC1C1(CC)CCCC1CC1CC(CC1CC1C1C1(CC1)CC=CC11)CC1CC1C1O1CC(CC(=CC(O)CC)CC(11\n",
            "Invalid SMILES skipped: C(1)C((1=1=CC)O)O)CC((CC1CC)CC)CC1CC1)CC(CC)1(C1)CC1)1CC((1(CC1)C1(1CC)CC1CC)CC=\n",
            "Invalid SMILES skipped: C,=CC=CC(CC1=CC)C1(CC=CC)CC1CC=CC(CC1=CCCC(CC)O)CC1CC(CC1CC1CC1CC1CC)C(CC)C1C1(1(CC)CC(\n",
            "Invalid SMILES skipped: C1C1CC1CC=CCCC=CC(C(CC1CC)1CC)C(CC1=CC(CC)O)CC(CC1CC)CCCC(1CC)CC1C(CC(CC)(1)CC(CC)1CC1CC\n",
            "Generated Valid SMILES:\n",
            "COC theCC(CC)CC(C=1(CC)CC)CC1CC1CC1CC1CC=CCCC(CC1CC1CC1CC)CC1CC1(CCO)CC1(CCCC)CC1CC(CC1CC)CC=CC)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[20:54:48] SMILES Parse Error: extra close parentheses while parsing: COC=CCCCCCCC1CCCC=CC1CC=CC=CC=CC(CC1)CC(CC1=CC)CC(CC1)1)CC1C1O(CC(CC(CC1)C)1)CC1(CC1(CCCC1\n",
            "[20:54:48] SMILES Parse Error: check for mistakes around position 56:\n",
            "[20:54:48] 1)CC(CC1=CC)CC(CC1)1)CC1C1O(CC(CC(CC1)C)1\n",
            "[20:54:48] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'COC=CCCCCCCC1CCCC=CC1CC=CC=CC=CC(CC1)CC(CC1=CC)CC(CC1)1)CC1C1O(CC(CC(CC1)C)1)CC1(CC1(CCCC1' for input: 'COC=CCCCCCCC1CCCC=CC1CC=CC=CC=CC(CC1)CC(CC1=CC)CC(CC1)1)CC1C1O(CC(CC(CC1)C)1)CC1(CC1(CCCC1'\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C(((CC)CC)(O)CCCC(CC1)(CCCC1=CC)O)CC)CC(C(C)CC(CC)CC1CC1CC)CCCC)(1CC1(C1)CC1CC1CC)1(CC\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 3:\n",
            "[20:54:49] C(((CC)CC)(O)CCCC(CC1)(CCCC1=CC)O)CC)CC(C\n",
            "[20:54:49] ~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C(((CC)CC)(O)CCCC(CC1)(CCCC1=CC)O)CC)CC(C(C)CC(CC)CC1CC1CC)CCCC)(1CC1(C1)CC1CC1CC)1(CC' for input: 'C(((CC)CC)(O)CCCC(CC1)(CCCC1=CC)O)CC)CC(C(C)CC(CC)CC1CC1CC)CCCC)(1CC1(C1)CC1CC1CC)1(CC'\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C(N1(C1)O)(1C1)1(C(1(1)C((1)N1))(CC(CC(CC)CC1(C(C(CC))CC11O)C1(1)1(CC1)\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 12:\n",
            "[20:54:49] C(N1(C1)O)(1C1)1(C(1(1)C((1)N1))(CC(CC(CC\n",
            "[20:54:49] ~~~~~~~~~~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C(N1(C1)O)(1C1)1(C(1(1)C((1)N1))(CC(CC(CC)CC1(C(C(CC))CC11O)C1(1)1(CC1)' for input: 'C(N1(C1)O)(1C1)1(C(1(1)C((1)N1))(CC(CC(CC)CC1(C(C(CC))CC11O)C1(1)1(CC1)'\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C(C)CC1C((CC)CC1CC(CCCC)CC1CCCC(CC)CC1C(CC)(C)1C(1)CC1)CC(CC)O1(1CC)CC)C1CC(1)CC1CC1\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 10:\n",
            "[20:54:49] C(C)CC1C((CC)CC1CC(CCCC)CC1CCCC(CC)CC1C(C\n",
            "[20:54:49] ~~~~~~~~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C(C)CC1C((CC)CC1CC(CCCC)CC1CCCC(CC)CC1C(CC)(C)1C(1)CC1)CC(CC)O1(1CC)CC)C1CC(1)CC1CC1' for input: 'C(C)CC1C((CC)CC1CC(CCCC)CC1CCCC(CC)CC1C(CC)(C)1C(1)CC1)CC(CC)O1(1CC)CC)C1CC(1)CC1CC1'\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C()O)CC1(CC=CC(CC)CC)CC1C1CC1CC=CC1CCCC)C1C)CC1CC1(CC)1)CC1CC1CCCCCCCCCC(1)CC1CC1CC(CC)CCCC\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 3:\n",
            "[20:54:49] C()O)CC1(CC=CC(CC)CC)CC1C1CC1CC=CC1CCCC)C\n",
            "[20:54:49] ~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C()O)CC1(CC=CC(CC)CC)CC1C1CC1CC=CC1CCCC)C1C)CC1CC1(CC)1)CC1CC1CCCCCCCCCC(1)CC1CC1CC(CC)CCCC' for input: 'C()O)CC1(CC=CC(CC)CC)CC1C1CC1CC=CC1CCCC)C1C)CC1CC1(CC)1)CC1CC1CCCCCCCCCC(1)CC1CC1CC(CC)CCCC'\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: COC=OC(=CC1)CC1C1(CC)CCCC1CC1CC(CC1CC1C1C1(CC1)CC=CC11)CC1CC1C1O1CC(CC(=CC(O)CC)CC(11\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 84:\n",
            "[20:54:49] O1CC(CC(=CC(O)CC)CC(11\n",
            "[20:54:49] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'COC=OC(=CC1)CC1C1(CC)CCCC1CC1CC(CC1CC1C1C1(CC1)CC=CC11)CC1CC1C1O1CC(CC(=CC(O)CC)CC(11' for input: 'COC=OC(=CC1)CC1C1(CC)CCCC1CC1CC(CC1CC1C1C1(CC1)CC=CC11)CC1CC1C1O1CC(CC(=CC(O)CC)CC(11'\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C(1)C((1=1=CC)O)O)CC((CC1CC)CC)CC1CC1)CC(CC)1(C1)CC1)1CC((1(CC1)C1(1CC)CC1CC)CC=\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 3:\n",
            "[20:54:49] C(1)C((1=1=CC)O)O)CC((CC1CC)CC)CC1CC1)CC(\n",
            "[20:54:49] ~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C(1)C((1=1=CC)O)O)CC((CC1CC)CC)CC1CC1)CC(CC)1(C1)CC1)1CC((1(CC1)C1(1CC)CC1CC)CC=' for input: 'C(1)C((1=1=CC)O)O)CC((CC1CC)CC)CC1CC1)CC(CC)1(C1)CC1)1CC((1(CC1)C1(1CC)CC1CC)CC='\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C,=CC=CC(CC1=CC)C1(CC=CC)CC1CC=CC(CC1=CCCC(CC)O)CC1CC(CC1CC1CC1CC1CC)C(CC)C1C1(1(CC)CC(\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 2:\n",
            "[20:54:49] C,=CC=CC(CC1=CC)C1(CC=CC)CC1CC=CC(CC1=CCC\n",
            "[20:54:49] ~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C,=CC=CC(CC1=CC)C1(CC=CC)CC1CC=CC(CC1=CCCC(CC)O)CC1CC(CC1CC1CC1CC1CC)C(CC)C1C1(1(CC)CC(' for input: 'C,=CC=CC(CC1=CC)C1(CC=CC)CC1CC=CC(CC1=CCCC(CC)O)CC1CC(CC1CC1CC1CC1CC)C(CC)C1C1(1(CC)CC('\n",
            "[20:54:49] SMILES Parse Error: syntax error while parsing: C1C1CC1CC=CCCC=CC(C(CC1CC)1CC)C(CC1=CC(CC)O)CC(CC1CC)CCCC(1CC)CC1C(CC(CC)(1)CC(CC)1CC1CC\n",
            "[20:54:49] SMILES Parse Error: check for mistakes around position 59:\n",
            "[20:54:49] (CC)O)CC(CC1CC)CCCC(1CC)CC1C(CC(CC)(1)CC(\n",
            "[20:54:49] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[20:54:49] SMILES Parse Error: Failed parsing SMILES 'C1C1CC1CC=CCCC=CC(C(CC1CC)1CC)C(CC1=CC(CC)O)CC(CC1CC)CCCC(1CC)CC1C(CC(CC)(1)CC(CC)1CC1CC' for input: 'C1C1CC1CC=CCCC=CC(C(CC1CC)1CC)C(CC1=CC(CC)O)CC(CC1CC)CCCC(1CC)CC1C(CC(CC)(1)CC(CC)1CC1CC'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNR0QukBQ2ky"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R7nM0IFYX8JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced SMILES Generator using Chemistry-Aware Language Models\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Draw, Descriptors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Disable WANDB\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check CUDA availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "    \"\"\"Custom dataset for SMILES strings\"\"\"\n",
        "    def __init__(self, smiles_list, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.inputs = []\n",
        "\n",
        "        for smiles in tqdm(smiles_list, desc=\"Tokenizing SMILES\"):\n",
        "            # Add special tokens for better learning\n",
        "            text = f\"<smiles>{smiles}</smiles>\"\n",
        "            encodings = tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            self.inputs.append({\n",
        "                'input_ids': encodings['input_ids'][0],\n",
        "                'attention_mask': encodings['attention_mask'][0]\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx]\n",
        "\n",
        "def download_chembl_smiles(max_compounds=10000, min_atoms=5, max_atoms=50):\n",
        "    \"\"\"Download and filter ChEMBL-like drug compounds\"\"\"\n",
        "    # For demonstration, let's use a larger sample dataset\n",
        "    # In a real implementation, you'd want to download from ChEMBL or use a proper dataset\n",
        "\n",
        "    # Example drug-like SMILES (expanded from your original dataset)\n",
        "    sample_smiles = [\n",
        "        \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # Aspirin\n",
        "        \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",  # Caffeine\n",
        "        \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\",  # Ibuprofen\n",
        "        \"CN1C=NC2=C1C(=O)NC(=O)N2\",  # Theophylline\n",
        "        \"CCN(CC)CCOC(=O)C1=CC=CC=C1Cl\",  # Diethyl procainamide\n",
        "        \"CC(C)OC(=O)C1=CC=CC=C1C(=O)O\",  # Ketoprofen\n",
        "        \"CCOC(=O)C1=CC=CC=C1N\",  # Procaine\n",
        "        \"CC(=O)NC1=CC=C(C=C1)O\",  # Acetaminophen\n",
        "        \"COC1=CC=CC=C1OC\",  # Dimethoxybenzene\n",
        "        \"CN1C=NC2=C1C(=O)N(C(=O)N2)C\",  # Theobromine\n",
        "        \"CC1=CC(=O)NC(=O)N1\",  # 5-Methyluracil\n",
        "        \"CC(C)OC(=O)C1=CC=CC=C1Cl\",  # Chlorbenzoxamine\n",
        "        \"CN(C)C(=O)C1=CC=C(C=C1)Cl\",  # Chlorpheniramine\n",
        "        \"COC1=CC=CC=C1C(=O)O\",  # Methyl salicylate\n",
        "        \"C1=CC=C(C=C1)N\",  # Aniline\n",
        "        \"C1=CC=C2C(=C1)C=CC=C2\",  # Naphthalene\n",
        "        \"COC1=CC=C(CC(=O)O)C=C1\",  # 4-Methoxyphenylacetic acid\n",
        "        \"CC(C)C1=CC=C(C=C1)O\",  # 4-isopropylphenol\n",
        "        \"CC1=CC=C(C=C1)S(=O)(=O)NC(=O)NC1CCCCC1\",  # Tolbutamide\n",
        "        \"CN1CCN(CC1)C1=CC=C(Cl)C=C1\",  # Chlorpromazine derivative\n",
        "        \"OC1=CC=CC=C1C(=O)NN\",  # Salicylhydrazide\n",
        "        \"C1CC(=O)NC(=O)C1\",  # Glutarimide\n",
        "        \"CC1=CC=C(C=C1)NC(=O)C\",  # 4-Methylacetanilide\n",
        "        \"CC1=CC=CC=C1O\",  # o-Cresol\n",
        "        \"CC1=CC=CC=C1N\",  # o-Toluidine\n",
        "        \"CC(=O)OC1=CC=CC=C1\",  # Phenyl acetate\n",
        "        \"CC1=CN=C(C=C1)C(=O)N\",  # Nicotinamide derivative\n",
        "        \"C1=CC=C(C=C1)C(=O)C=O\",  # Phenylglyoxal\n",
        "        \"COC1=CC=C(C=C1)CCN\",  # 4-Methoxyphenethylamine\n",
        "        \"CC1=CC=CC=C1CC(=O)O\",  # 2-Methylphenylacetic acid\n",
        "        \"CC1=CC=CC(=C1)C(=O)O\",  # 3-Methylbenzoic acid\n",
        "        \"CC1=CC=CC=C1C(=O)O\",  # 2-Methylbenzoic acid\n",
        "        \"CC1=CC=C(C=C1)C(=O)O\",  # 4-Methylbenzoic acid\n",
        "        \"CC1=CC=C(O)C=C1\",  # p-Cresol\n",
        "        \"CC1=CC=C(C=C1)C(C)N\",  # 4-Methylamphetamine\n",
        "        \"CC1=CC=C(C=C1)S(=O)(=O)N\",  # Toluenesulfonamide\n",
        "        \"C1=CC=C(C=C1)CC(=O)O\",  # Phenylacetic acid\n",
        "        \"C1=CC=C(C=C1)CCCC(=O)O\",  # 4-Phenylbutyric acid\n",
        "        \"C1=CC=C(C=C1)C(=O)N\",  # Benzamide\n",
        "        \"CC(=O)NC1=CC=CC=C1\",  # Acetanilide\n",
        "        \"CC(C)(C)C1=CC=C(C=C1)O\",  # 4-tert-butylphenol\n",
        "        \"CC(C)(C)C1=CC=CC=C1\",  # tert-butylbenzene\n",
        "        \"CC1=CC(=CC=C1)N\",  # m-Toluidine\n",
        "    ]\n",
        "\n",
        "    # For a real application, you would download from ChEMBL:\n",
        "    # import deepchem as dc\n",
        "    # dc.molnet.load_chembl25(featurizer='ECFP', split='random')\n",
        "\n",
        "    # Validate and filter SMILES\n",
        "    valid_smiles = []\n",
        "    for smi in sample_smiles:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol:\n",
        "            # Apply filters (e.g., size, drug-likeness)\n",
        "            num_atoms = mol.GetNumAtoms()\n",
        "            if min_atoms <= num_atoms <= max_atoms:\n",
        "                # Calculate properties to ensure drug-likeness\n",
        "                logp = Descriptors.MolLogP(mol)\n",
        "                if -0.4 <= logp <= 5.6:  # Lipinski's rule of 5 range\n",
        "                    valid_smiles.append(smi)\n",
        "\n",
        "    # Add more compounds if needed (for a real implementation)\n",
        "    if len(valid_smiles) < max_compounds:\n",
        "        # In a real implementation, get more from a database\n",
        "        pass\n",
        "\n",
        "    return valid_smiles[:max_compounds]\n",
        "\n",
        "def preprocess_dataset():\n",
        "    \"\"\"Preprocess SMILES strings and prepare datasets\"\"\"\n",
        "    # Get SMILES data\n",
        "    smiles_list = download_chembl_smiles(max_compounds=2000)\n",
        "    print(f\"Total valid SMILES: {len(smiles_list)}\")\n",
        "\n",
        "    # Split into train/validation sets\n",
        "    np.random.shuffle(smiles_list)\n",
        "    split_idx = int(len(smiles_list) * 0.9)\n",
        "    train_smiles = smiles_list[:split_idx]\n",
        "    val_smiles = smiles_list[split_idx:]\n",
        "\n",
        "    return train_smiles, val_smiles\n",
        "\n",
        "def create_tokenizer_and_model(model_name=\"gpt2-medium\"):\n",
        "    \"\"\"Initialize tokenizer and model\"\"\"\n",
        "    # Use a chemistry-aware model if available\n",
        "    # For demo, we'll fine-tune GPT-2-medium which has better capacity than base GPT-2\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Add special tokens for SMILES\n",
        "    special_tokens = {\n",
        "        'pad_token': '[PAD]',\n",
        "        'bos_token': '<smiles>',\n",
        "        'eos_token': '</smiles>',\n",
        "        'additional_special_tokens': ['[C]', '[O]', '[N]', '[S]', '[Cl]', '[F]', '[Br]', '[I]']\n",
        "    }\n",
        "\n",
        "    # Add special tokens to tokenizer\n",
        "    tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "    # Initialize model\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "def train_smiles_generator(model, tokenizer, train_smiles, val_smiles, output_dir=\"./smiles-generator\"):\n",
        "    \"\"\"Train the SMILES generator model\"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = SMILESDataset(train_smiles, tokenizer)\n",
        "    val_dataset = SMILESDataset(val_smiles, tokenizer)\n",
        "\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "    # Configure training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,  # Adjust based on dataset size\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_steps=500,\n",
        "        save_steps=500,\n",
        "        warmup_steps=500,\n",
        "        prediction_loss_only=False,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=1,\n",
        "        fp16=True if torch.cuda.is_available() else False,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=5e-5,  # Lower learning rate for fine-tuning\n",
        "    )\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We're doing causal language modeling, not masked\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_novel_smiles(model, tokenizer, num_sequences=25, max_length=100, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"Generate novel SMILES strings\"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Generation parameters\n",
        "    generation_config = {\n",
        "        \"do_sample\": True,\n",
        "        \"top_p\": top_p,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_k\": 50,\n",
        "        \"max_length\": max_length,\n",
        "        \"num_return_sequences\": num_sequences,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"bos_token_id\": tokenizer.bos_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    }\n",
        "\n",
        "    # Generate sequences\n",
        "    input_ids = tokenizer(\"<smiles>\", return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Generate with guidance\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "\n",
        "    # Decode generated SMILES\n",
        "    generated_smiles = []\n",
        "    for output in outputs:\n",
        "        decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        # Extract SMILES string between tags if present\n",
        "        if \"<smiles>\" in decoded and \"</smiles>\" in decoded:\n",
        "            smiles = decoded.split(\"<smiles>\")[1].split(\"</smiles>\")[0].strip()\n",
        "        else:\n",
        "            smiles = decoded.strip()\n",
        "        generated_smiles.append(smiles)\n",
        "\n",
        "    return generated_smiles\n",
        "\n",
        "def validate_smiles(smiles_list):\n",
        "    \"\"\"Validate generated SMILES and calculate properties\"\"\"\n",
        "    valid_mols = []\n",
        "    valid_smiles = []\n",
        "    properties = []\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol:\n",
        "                valid_mols.append(mol)\n",
        "                valid_smiles.append(smiles)\n",
        "\n",
        "                # Calculate basic molecular properties\n",
        "                properties.append({\n",
        "                    'SMILES': smiles,\n",
        "                    'MolWeight': round(Descriptors.MolWt(mol), 2),\n",
        "                    'LogP': round(Descriptors.MolLogP(mol), 2),\n",
        "                    'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "                    'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
        "                    'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "                    'TPSA': round(Descriptors.TPSA(mol), 2)\n",
        "                })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"Generated {len(smiles_list)} SMILES, {len(valid_smiles)} valid ({len(valid_smiles)/len(smiles_list)*100:.1f}%)\")\n",
        "\n",
        "    return valid_mols, valid_smiles, properties\n",
        "\n",
        "def visualize_molecules(mols, n_per_row=5, max_mols=10):\n",
        "    \"\"\"Visualize generated molecules\"\"\"\n",
        "    if len(mols) > max_mols:\n",
        "        mols = mols[:max_mols]\n",
        "\n",
        "    img = Draw.MolsToGridImage(\n",
        "        mols,\n",
        "        molsPerRow=n_per_row,\n",
        "        subImgSize=(250, 250),\n",
        "        legends=[f\"Mol {i+1}\" for i in range(len(mols))]\n",
        "    )\n",
        "    return img\n",
        "\n",
        "def analyze_properties(properties):\n",
        "    \"\"\"Analyze properties of generated molecules\"\"\"\n",
        "    if not properties:\n",
        "        return \"No valid molecules to analyze\"\n",
        "\n",
        "    df = pd.DataFrame(properties)\n",
        "\n",
        "    # Check Lipinski's Rule of 5\n",
        "    df['Lipinski_Violations'] = (\n",
        "        (df['MolWeight'] > 500).astype(int) +\n",
        "        (df['LogP'] > 5).astype(int) +\n",
        "        (df['NumHDonors'] > 5).astype(int) +\n",
        "        (df['NumHAcceptors'] > 10).astype(int)\n",
        "    )\n",
        "\n",
        "    summary = {\n",
        "        'Total_Molecules': len(df),\n",
        "        'Rule_of_5_Compliant': (df['Lipinski_Violations'] <= 1).sum(),\n",
        "        'Avg_MolWeight': df['MolWeight'].mean(),\n",
        "        'Avg_LogP': df['LogP'].mean(),\n",
        "        'Avg_TPSA': df['TPSA'].mean(),\n",
        "    }\n",
        "\n",
        "    print(f\"Property Analysis Summary:\")\n",
        "    for k, v in summary.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq5nhpMLX8em",
        "outputId": "68f32f3c-5a0c-491a-bea8-00d34f5bed95"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Process dataset\n",
        "    train_smiles, val_smiles = preprocess_dataset()\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer, model = create_tokenizer_and_model()\n",
        "\n",
        "    # Train model\n",
        "    model, tokenizer = train_smiles_generator(model, tokenizer, train_smiles, val_smiles)\n",
        "\n",
        "    # Generate novel molecules\n",
        "    print(\"\\nGenerating novel SMILES...\")\n",
        "    generated_smiles = generate_novel_smiles(model, tokenizer, num_sequences=50)\n",
        "\n",
        "    # Validate and analyze generated molecules\n",
        "    mols, valid_smiles, properties = validate_smiles(generated_smiles)\n",
        "\n",
        "    # Analyze properties\n",
        "    props_df = analyze_properties(properties)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nExample valid SMILES generated:\")\n",
        "    for i, smiles in enumerate(valid_smiles[:5]):\n",
        "        print(f\"{i+1}. {smiles}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "zge4mQ-mZcvM",
        "outputId": "786f624b-4379-4ad1-93a3-25c050d94489"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total valid SMILES: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing SMILES: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:00<00:00, 1776.15it/s]\n",
            "Tokenizing SMILES: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 753.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 35\n",
            "Validation dataset size: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.STEPS",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-334340e2fe19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_smiles_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Generate novel molecules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-91ea598e6c2d>\u001b[0m in \u001b[0;36mtrain_smiles_generator\u001b[0;34m(model, tokenizer, train_smiles, val_smiles, output_dir)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Configure training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim...\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_best_model_at_end\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSaveStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1649\u001b[0m                     \u001b[0;34m\"--load_best_model_at_end requires the save and eval strategy to match, but found\\n- Evaluation \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                     \u001b[0;34mf\"strategy: {self.eval_strategy}\\n- Save strategy: {self.save_strategy}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.STEPS"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJtuL3YbZspT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}