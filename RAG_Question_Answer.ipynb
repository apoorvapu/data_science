{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/RAG_Question_Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq3zIOCofySQ"
      },
      "source": [
        "Generative AI in Research: using RAG+LLM to understand a journal paper well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_-IHuLTAh-u"
      },
      "source": [
        "leverage RAG for answering questions based on a journal paper.\n",
        "\n",
        "Download a paper and convert to .txt file in a directory named \"data\". Use this .txt file and evaluate if the RAG technique is giving good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvN1vPOiKKzp",
        "outputId": "8411283a-455a-4e41-c088-f5529fb537b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySeyRpuUKKzs",
        "outputId": "46fd54de-e3e6-431d-f096-4104cc97533a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 2210.05274\n",
            "Converted to text: data/2210.05274.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Sample arXiv paper IDs related to diffusion models\n",
        "arxiv_ids = [\n",
        "    \"2210.05274\",  # Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design\n",
        "]\n",
        "\n",
        "def download_pdf(arxiv_id, output_folder):\n",
        "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "    pdf_path = os.path.join(output_folder, f\"{arxiv_id}.pdf\")\n",
        "    response = requests.get(url)\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {arxiv_id}\")\n",
        "    return pdf_path\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"Converted to text: {txt_path}\")\n",
        "\n",
        "def main():\n",
        "    data_dir = \"data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for arxiv_id in arxiv_ids:\n",
        "        pdf_path = download_pdf(arxiv_id, data_dir)\n",
        "        txt_path = os.path.join(data_dir, f\"{arxiv_id}.txt\")\n",
        "        pdf_to_text(pdf_path, txt_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbEkf_siKKzu",
        "outputId": "9ad765a9-668a-45e9-fece-4c403e7a9a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.34)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain_community faiss-cpu sentence-transformers transformers networkx matplotlib spacy\n",
        "#!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zwXg8ta6KKzv"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "import gc\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import re\n",
        "gc.collect()\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    import re\n",
        "    # Remove inline citations like [14], [14, 27], and (author year)\n",
        "    text = re.sub(r\"\\[[0-9,\\s]+\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\(.*?\\d{4}.*?\\)\", \"\", text)  # Remove citations like (Author, 2020)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
        "\n",
        "    # Remove LaTeX math expressions\n",
        "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
        "\n",
        "    # Remove repeated words\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,;:?!\\s]\", \"\", text)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Pattern for common author name formats\n",
        "    pattern = r\"\\b([A-Z][a-z]+(?: [A-Z][a-z]+)*,? (?:[A-Z]\\.)?\\b(?: and [A-Z][a-z]+(?: [A-Z][a-z]+)*,? (?:[A-Z]\\.)?)+|\\b(?:et al\\.)\\b)\"\n",
        "    # Replace any matched author names with an empty string\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    # Remove extra spaces after author removal\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return text\n",
        "\n",
        "def clean_text_remove_references(text):\n",
        "    # Remove extra newlines and empty lines\n",
        "    text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip() != \"\"])\n",
        "\n",
        "    # Cut off everything after 'References' or 'Bibliography'\n",
        "    refs_keywords = [\"references\", \"bibliography\"]\n",
        "    for keyword in refs_keywords:\n",
        "        idx = text.lower().find(keyword)\n",
        "        if idx != -1:\n",
        "            text = text[:idx]  # Keep text before the references section\n",
        "            break\n",
        "\n",
        "    # Remove sections related to funding or acknowledgments\n",
        "    text = re.sub(r\".*grant agreement.*|.*funding.*|.*acknowledg(e)?ment.*\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "M_jzI9YmaVq7"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the long journal paper\n",
        "file_path = \"data/2210.05274.txt\"  # Path to your journal paper\n",
        "\n",
        "loader = TextLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "for doc in documents:\n",
        "    doc.page_content = clean_text(doc.page_content)\n",
        "\n",
        "raw_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "clean_text = clean_text_remove_references(raw_text)\n",
        "\n",
        "# 3. Split into manageable chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250,  # approx 1000 characters per chunk\n",
        "    chunk_overlap=20\n",
        ")"
      ],
      "metadata": {
        "id": "o3CucbVTRekA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Ask Questions\n",
        "def ask_question(question):\n",
        "    result = qa_chain.run(question)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZzDErQCpffo0"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "docs = text_splitter.create_documents([raw_text])\n",
        "\n",
        "# 4. Build vector database (FAISS)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)\n"
      ],
      "metadata": {
        "id": "3NZmY6nOmQvI"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "# Set up Language Model (use a smaller one to reduce memory consumption)\n",
        "model_name = 'google/flan-t5-large'  # Switching to a smaller version of FLAN-T5\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "text2text_gen = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=300,  # <-- important! specifies the size of outputs (answer) that LLM should give you\n",
        "    repetition_penalty=1.2,   # Important to discourage endless repeats\n",
        "    temperature=0.7,           # A bit more randomness\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        "    )\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text2text_gen)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt_template = \"\"\"\n",
        "You are a helpful academic assistant. Read the following article content carefully. Then, answer the given question completely and concisely.\n",
        "\n",
        "=== Article Content ===\n",
        "{context}\n",
        "\n",
        "=== Question ===\n",
        "{question}\n",
        "\n",
        "=== Answer ===\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=custom_prompt_template,\n",
        ")\n",
        "\n",
        "# 6. Build RetrievalQA Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}), # Retrieve top 10 chunks for question answer\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt}  # Use the custom prompt\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IkKT8Y7OWX7",
        "outputId": "b99af80e-2a28-4f49-dc1c-8f6b5f6b83f1"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q: What is the paper about?\\nA:\", ask_question(\"What is the paper about?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf9DZwMiVqJj",
        "outputId": "d648a109-fd7a-42b3-b798-134b8c51f49b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the paper about?\n",
            "A: Diffusion model for generative models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q: What are the main contributions?\\nA:\", ask_question(\"What are the main contributions?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg_nS6z4ftQI",
        "outputId": "8e34684d-6747-4536-ffc3-68b05401cc01"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What are the main contributions?\n",
            "A: a new method for the design of candidate structures from pharmacophoric hypotheses. Journal of medicinal chemistry, 3624: 38633870, 1993.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q: What are the key themes?\\nA:\", ask_question(\"What are the key themes discussed?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhvpYoYQfoq8",
        "outputId": "3435bbf6-0de0-4912-c21f-c1771d2a6013"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What are the key themes?\n",
            "A: Existing approaches are either based on syntactic pattern recognition or on autoregressive models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q: What are the key results?\\nA:\", ask_question(\"What are the key results?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO52wBSxfpDY",
        "outputId": "5f3b7ec8-bab6-4496-dcef-ebe89fe3996e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What are the key results?\n",
            "A: The following metrics assess the chemical relevance of the generated molecules. The last three metrics evaluate the standard generative properties of the methods. Method QED SA Rings Valid, Unique, Novel, GEOM and combine results removing duplicates. Overall, we obtain 41,907 molecules and 285,142 fragmentations that are randomly split in train 282,602 examples, valida tion 1,250 examples and test 1,290 examples sets. Pockets Dataset In order to assess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_question(\"What is the name of the chaperone used in the case study? Don't make up the result if you don't know - just say NO.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9o-MopAOWfH",
        "outputId": "a886b5e1-ed26-466c-9e3a-d0cad7356ac0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create a strong prompt template\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful academic assistant.\n",
        "\n",
        "Read the following article carefully.\n",
        "\n",
        "=== Article Content ===\n",
        "{context}\n",
        "\n",
        "=== Question ===\n",
        "{question}\n",
        "\n",
        "=== Answer ===\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 3. Build the RetrievalQA Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": qa_prompt},\n",
        ")"
      ],
      "metadata": {
        "id": "5GonaqcYx-NU"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain.run(\"What are the main contributions?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "VSPZVhmMyM7S",
        "outputId": "44f6eba2-656a-4a45-da48-0a8a4c55fe7a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'E n equivariant normalizing ows. arXiv preprint arXiv:2105.09016, 2021. Chunquan Sheng and Wannian Zhang. New lead structures in for helpful feedback and insightful dis cussions. Ilia Igashov has received funding from the European Unions Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No 945363. Clement Vignac would like to thank and Renxiao Wang. Comparative assessment of scoring functions: The CASF2016 update. Journal of Chemical Information and Modeling, 592:895913, November 2018. David C Thompson, R Aldrin Denny, Ramaswamy Nilakantan, Christine Humblet, Diane Joseph'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOT doing a good job understanding the paper."
      ],
      "metadata": {
        "id": "oFB1EVnCzyXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying to see if it can understand a novel story written by ChatGPT.\n",
        "\n",
        "the story does not exist anywhere on internet.\n"
      ],
      "metadata": {
        "id": "2Z0qIQY_z2_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the story\n",
        "story = \"\"\"\n",
        "The Echoes of Arlandria\n",
        "\n",
        "In the distant kingdom of Arlandria, nestled between jagged mountains and dark, mist-filled forests, there was a legend—a legend of an ancient city that had vanished without a trace. The city, once the pride of the kingdom, was said to have been built by the first rulers, whose bloodline was rumored to possess the ability to manipulate the very fabric of time itself. The city was called Rivakhar.\n",
        "\n",
        "But Rivakhar had disappeared overnight, leaving behind only a haunting silence. No one knew where it had gone, nor why it vanished. Over time, the tale faded into myth, and the people of Arlandria moved on with their lives, leaving the mystery to the realms of imagination.\n",
        "\n",
        "That is, until a young woman named Liora stumbled upon an old, tattered map in the attic of her late grandmother’s cottage. The map depicted the mountains surrounding Arlandria, with a strange, glowing mark near the heart of the forest, where no human had ventured in centuries. Liora, a curious and determined soul, had always felt a deep connection to the unknown, as if something—or someone—was calling her.\n",
        "\n",
        "Driven by an unexplainable urge, Liora set out on a journey to find the lost city. She was joined by Caden, a quiet scholar from the kingdom's library who had long been fascinated by the legend of Rivakhar, and Verrin, a skilled tracker from the village who had lost his brother to the forests long ago.\n",
        "\n",
        "The journey took them through treacherous terrain, where the air grew thick with magic, and the forest seemed to whisper secrets long forgotten. They encountered strange creatures—twisted shadows that flickered in and out of existence, glowing insects that left trails of light, and an ancient wolf who seemed to know their every step.\n",
        "\n",
        "As they ventured deeper into the forest, they uncovered forgotten ruins and ancient symbols etched into the earth. Liora began to dream of Rivakhar—vivid, surreal dreams of a city that floated above the ground, surrounded by shimmering rivers of light. In these dreams, she could hear the voices of the city's inhabitants, calling for help.\n",
        "\n",
        "One night, while camping under the stars, Liora awoke to a soft voice calling her name. It was the voice of her grandmother, long passed. The voice told her of a great choice: She was the heir of the Rivakhari bloodline, and the key to unlocking the city's return.\n",
        "\n",
        "The next day, they reached the heart of the forest, where the map’s glowing mark had led them. But as they arrived, they discovered an ancient temple, buried beneath the roots of a massive tree. Inside the temple, they found a pedestal with a crystal that pulsed with an ethereal light. The crystal, however, was cracked—its power waning.\n",
        "\n",
        "It was here that they learned the truth: Rivakhar had not disappeared; it had simply folded into another dimension, trapped between time and space. The bloodline of the rulers had the power to bring it back, but only if the heir could restore the crystal and use their blood to fuel the magic that bound the city.\n",
        "\n",
        "But there was a catch. The ruler’s bloodline had been cursed long ago. Anyone who tried to restore Rivakhar would be forced to choose: save the city or save themselves. If the city returned, its magic would drain the life of the one who called it back, trapping them in its eternity.\n",
        "\n",
        "Liora stood before the crystal, her heart heavy with the weight of the decision. She could feel the pulse of the city’s magic, calling her, but she also felt the warmth of her companions behind her—their hope, their belief in her.\n",
        "\n",
        "In a moment of clarity, Liora made her choice.\n",
        "\n",
        "She pressed her hand to the crystal, her blood mingling with its magic. A burst of light engulfed her, and the world seemed to warp around her. For a moment, everything stood still, and then…\n",
        "\n",
        "Rivakhar reappeared, rising from the earth, glowing brighter than the sun. The ancient city, with its grand towers and shining rivers, was restored.\n",
        "\n",
        "But Liora did not return with it. Her body, now made of stardust and light, stood as a sentinel at the heart of the city, her soul bound to the city forever.\n",
        "\n",
        "The people of Arlandria, hearing the legends once again, would tell stories of the brave young woman who gave everything to restore Rivakhar—the city that never truly disappeared, but had waited for the right soul to bring it back.\n",
        "\n",
        "The End.\n",
        "\"\"\"\n",
        "\n",
        "# Save the story to a text file\n",
        "with open(\"TheEchoesOfArlandria.txt\", \"w\") as file:\n",
        "    file.write(story)\n",
        "\n",
        "print(\"Story saved successfully as 'TheEchoesOfArlandria.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahcOyGZp1O9-",
        "outputId": "35a6f4cc-8b47-4c51-ad7e-4b85d4981df9"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story saved successfully as 'TheEchoesOfArlandria.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Load the long journal paper\n",
        "file_path = \"TheEchoesOfArlandria.txt\"  # Path to your journal paper\n",
        "loader = TextLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "raw_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "# 3. Split into manageable chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250,  # approx 1000 characters per chunk\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "docs = text_splitter.create_documents([raw_text])\n",
        "\n",
        "# 4. Build vector database (FAISS)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "model_name = 'google/flan-t5-large'  # Switching to a smaller version of FLAN-T5\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "text2text_gen = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=300,  # <-- important! specifies the size of outputs (answer) that LLM should give you\n",
        "    )\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text2text_gen)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt_template = \"\"\"\n",
        "You are a helpful assistant. Read the following content carefully. Then, answer the given question completely and concisely.\n",
        "\n",
        "=== Content ===\n",
        "{context}\n",
        "\n",
        "=== Question ===\n",
        "{question}\n",
        "\n",
        "=== Answer ===\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=custom_prompt_template,\n",
        ")\n",
        "\n",
        "# 6. Build RetrievalQA Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}), # Retrieve top 10 chunks for question answer\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt}  # Use the custom prompt\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9INQxQvOyO3T",
        "outputId": "4648a5f6-2c15-4c07-8478-3d6846cfb647"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_question(\"Who is the main character in the story?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP8fJZ4D3cgj",
        "outputId": "69f7b1e5-8810-4fbc-ac70-3e1e9c7ffc03"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_question(\"What was the city called in the legend?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm7lpi--3e4E",
        "outputId": "5700dbc4-340a-49cb-d95d-b817d455b246"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rivakhar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What happened to the city Rivakhar?\"\n",
        "ask_question(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q7c5jGa53q3f",
        "outputId": "17d223bc-d1f0-4173-b5f6-8a1dd6a224a9"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It had folded into another dimension.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_question(\"Write a summary of this story.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDX_cRhF35Ru",
        "outputId": "6d0289d1-c1f6-497e-a85d-60878474f0a8"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liora dreamed of the city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It understands this story and answers properly."
      ],
      "metadata": {
        "id": "qsfDsYmF5UNr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6joylgWJ4027"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}