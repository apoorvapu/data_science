{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq3zIOCofySQ"
   },
   "source": [
    "Generative AI in Research: using Large Language Models (LLMs) to enhance and streamline the academic literature review process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_-IHuLTAh-u"
   },
   "source": [
    "leverage RAG Techniques for summarizing papers, identifying connections across papers (authors, references, methods), uncovering key themes in them.\n",
    "\n",
    "Download 2 papers (related to diffusion model) and convert them to .txt files in a directory named \"data\". Use these .txt files as input papers and evaluate if the RAG technique is giving good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbRT1J6xiGMC"
   },
   "source": [
    "### download any 2 papers of diffusion model and convert their pdf to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2006.11239\n",
      "Converted to text: data/2006.11239.txt\n",
      "Downloaded 2105.05233\n",
      "Converted to text: data/2105.05233.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Sample arXiv paper IDs related to diffusion models\n",
    "arxiv_ids = [\n",
    "    \"2006.11239\",  # Denoising Diffusion Probabilistic Models\n",
    "    \"2105.05233\",  # Improved Denoising Diffusion Probabilistic Models\n",
    "]\n",
    "\n",
    "def download_pdf(arxiv_id, output_folder):\n",
    "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    pdf_path = os.path.join(output_folder, f\"{arxiv_id}.pdf\")\n",
    "    response = requests.get(url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {arxiv_id}\")\n",
    "    return pdf_path\n",
    "\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Converted to text: {txt_path}\")\n",
    "\n",
    "def main():\n",
    "    data_dir = \"data\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    for arxiv_id in arxiv_ids:\n",
    "        pdf_path = download_pdf(arxiv_id, data_dir)\n",
    "        txt_path = os.path.join(data_dir, f\"{arxiv_id}.txt\")\n",
    "        pdf_to_text(pdf_path, txt_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/*.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install langchain langchain_community faiss-cpu sentence-transformers transformers networkx matplotlib spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import gc\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in locals():\n",
    "    del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ab58afda09497b9dee194ec912409d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing: data/2105.05233.txt\n",
      "\n",
      "🔍 Summary:\n",
      "We show that diffusion models can achieve image sample quality superior to the current stateoftheart generative models. We achieve this on unconditional im age synthesis by nding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classier guid ance: a simple, computeefcient method for trading off diversity for delity using gradients from a classier. We achieve an FID of 2.97 on ImageNet 128128, 4.59 on ImageNet 256256, and 7.72 on ImageNet 512512, and we match BigGANdeep even with as few as 25 forward passes per sample. Finally, we nd that classier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256256 and 3.85 on ImageNet 512512. We release our code at 1 Introduction\n",
      "\n",
      "🔍 Connections:\n",
      "---\n",
      "\n",
      "🔍 Themes:\n",
      "Diffusion Models Beat GANs on Image Synthesis\n",
      "\n",
      "🔍 Summary:\n",
      "--- 512512. We release our code at 1 Introduction Figure 1: Selected samples from our best ImageNet 512512 model FID 3.85 Over the past few years, generative models have gained the ability to generate humanlike natural language , innite highquality synthetic images and highly diverse human speech and music . These models can be used in a variety of elds, such as generating images from text prompts or learning useful feature representations . While these models can be used in a variety of elds, there is still much room for improvement beyond the current stateoftheart, and better generative models could have wideranging impacts on graphic design, games, music production, and countless other elds. GANs currently hold the stateoftheart on most image generation tasks as measured by sample quality metrics such as FID , Inception Score and Precision . However, some of these metrics do not fully capture diversity,\n",
      "\n",
      "🔍 Connections:\n",
      "e.g., the metric FID does not fully capture the generative nature of the images, and the metric Inception Score does not fully capture the generative nature of the images.\n",
      "\n",
      "🔍 Themes:\n",
      "ImageNet 512512\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: We introduce a new class of likelihoodbased models called diffusion models. We show that these models can produce highquality images while capturing more diversity than GANs. We also show that these models can be trained to produce samples in a wallclock time comparable to GANs.\n",
      "\n",
      "🔍 Connections:\n",
      "from a distribution and then sample from it.\n",
      "\n",
      "🔍 Themes:\n",
      "---\n",
      "\n",
      "🔍 Summary:\n",
      "Diffusion models are a class of models that generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lowerbound . These models already holds the stateoftheart on CIFAR10 , but still lags behind GANs on difcult generation datasets like LSUN and ImageNet 256256 . We hypothesize that the gap between diffusion models and GANs stems from at least two factors: rst, that the model architectures used by recent GAN literature have been heavily explored and rened; second, that GANs are able to trade off diversity for delity, producing high quality samples but not covering the whole distribution. We aim to bring these benets to Diffusion models\n",
      "\n",
      "🔍 Connections:\n",
      "diffusion models by introducing a new model architecture that is able to produce samples from the full distribution.\n",
      "\n",
      "🔍 Themes:\n",
      "Diffusion models\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: We introduce a new architecture for diffusion models and a method for using gradients from a classier to guide a diffusion model during sampling. We nd that a single hyperparameter, the scale of the classier gradients, can be tuned to trade off diversity for delity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples .\n",
      "\n",
      "🔍 Connections:\n",
      "We can achieve a new stateoftheart, surpassing GANs on several different metrics and datasets.\n",
      "\n",
      "🔍 Themes:\n",
      "--- architecture --- gradients --- delity --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability --- scalability ---\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: We introduce a new architecture for diffusion models that improves the performance of the models on both unconditional and conditional image synthesis tasks.\n",
      "\n",
      "🔍 Connections:\n",
      "tuple of samples from the distribution.\n",
      "\n",
      "🔍 Themes:\n",
      "Diffusion Models for Image Synthesis\n",
      "\n",
      "🔍 Summary:\n",
      "--- Main contribution: We introduce a new method for sampling from a noise predictor xt, t.\n",
      "\n",
      "🔍 Connections:\n",
      "generating a noisy sample xt from a noisy sample x0.\n",
      "\n",
      "🔍 Themes:\n",
      "---\n",
      "\n",
      "🔍 Summary:\n",
      "Denoising Diffus sion Models for Denoising Data Analysis\n",
      "\n",
      "🔍 Connections:\n",
      "Denoising Diffusion Model\n",
      "\n",
      "🔍 Themes:\n",
      "Denoising Diffus sion Models\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: We propose a new model for reverse process denoising that uses a neural network to interpolate the variance xt, t. We also propose a new model for reverse process denoising that uses a neural network to interpolate the variance xt, t. We also propose a new model for reverse process denoising that uses a neural network to interpolate the variance xt, t.\n",
      "\n",
      "🔍 Connections:\n",
      ", , and , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "🔍 Themes:\n",
      "---\n",
      "\n",
      "🔍 Summary:\n",
      "We adopt the following metrics for comparing sample quality across models:\n",
      "\n",
      "🔍 Connections:\n",
      "we adopt them for our experiments, we also use the following metrics for evaluating the performance of models:\n",
      "\n",
      "🔍 Themes:\n",
      "--- DDPM DDIM DDIM DDPM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM DDIM\n",
      "\n",
      "📄 Processing: data/2006.11239.txt\n",
      "\n",
      "🔍 Summary:\n",
      "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a stateoftheart FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\n",
      "\n",
      "🔍 Connections:\n",
      "a long history of generating images from a latent variable model.\n",
      "\n",
      "🔍 Themes:\n",
      "Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Deno\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: A directed graphical model for generating images. Dataset used: CelebAHQ 256 256, CIFAR10, ! xT ! ! xt ! xt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1 ! ! x0 pxt1xt qxtxt1\n",
      "\n",
      "🔍 Connections:\n",
      "---\n",
      "\n",
      "🔍 Themes:\n",
      "Diffusion Probabilistic Models\n",
      "\n",
      "🔍 Summary:\n",
      "We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models Section 4.\n",
      "\n",
      "🔍 Connections:\n",
      "We also show that the equivalence is not only theoretically plausible but also empirically observable.\n",
      "\n",
      "🔍 Themes:\n",
      "Diffusion models for generative models\n",
      "\n",
      "🔍 Summary:\n",
      "--- We present a new model for lossless compression that is based on a diffusion model that is annealed during sampling. We show that the model is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with a lossy compression scheme.\n",
      "\n",
      "🔍 Connections:\n",
      "annealed Langevin dynamics\n",
      "\n",
      "🔍 Themes:\n",
      "---\n",
      "\n",
      "🔍 Summary:\n",
      "--- vastly generalizes what is normally possible with autoregressive models\n",
      "\n",
      "🔍 Connections:\n",
      "--- vastly generalizes what is normally possible with autoregressive models\n",
      "\n",
      "🔍 Themes:\n",
      "--- vastly generalizes what is normally possible with autoregressive models\n",
      "\n",
      "🔍 Summary:\n",
      "--- qxtxt1 : L 3 The forward process variances t can be learned by reparameterization or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in pxt1xt, because both processes have the same functional form when t is small . Efficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L 3 as: Eq DKLqxT x0 pxT z LT X t1 DKLqxt1xt, x0 pxt1xt z Lt1 log px0x1 z L0 5 See Appendix A for details. The labels for the terms are in Section 3. Equation 5 uses KL divergence to directly compare pxt1xt against forward process posteriors, which are tractable when conditioned on x0: qxt1xt, x0 Nxt1; txt, x0, tI, 6 where txt, x0, tI\n",
      "\n",
      "🔍 Connections:\n",
      "--- qxtxt1 : L 3 The forward process variances t can be learned by reparameterization or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in pxt1xt, because both processes have the same functional form when t are small . A notable property of the forward process is that it admits sampling xt at an arbitrary timestep t in closed form: using the notation t : 1 t and t : Qt s1 s, we have qxtx0 Nxt; tx0, 1 tI 4 2 Efcient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L 3 as: Eq DKLqxT x0 pxT z LT X t1 DKLqxt1xt, x0 pxt1xt z Lt1 log px0x1 z L0 5 See Appendix A for details. The labels on the terms are in Section 3. Equation 5 uses KL divergence to directly compare pxt1xt against forward process posteriors, which are tractable when conditioned on x0: qxt1xt, x0 Nxt1; txt, x0, tI, 6 where txt, x0\n",
      "\n",
      "🔍 Themes:\n",
      "Forward and Reverse Processes\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: We introduce a new, explicit connection between diffusion models and denoising score matching.\n",
      "\n",
      "🔍 Connections:\n",
      "---\n",
      "\n",
      "🔍 Themes:\n",
      "Denoising autoencoders\n",
      "\n",
      "🔍 Summary:\n",
      "Main contribution: We propose a new method for estimating the entropy of a reverse process. We discuss the choice of pxt1xt Nxt1; xt, t, xt, t for 1 t T.\n",
      "\n",
      "🔍 Connections:\n",
      "---\n",
      "\n",
      "🔍 Themes:\n",
      "Forward and Reverse Processes\n",
      "\n",
      "🔍 Summary:\n",
      "--- We propose a new method for predicting the t-axis of a t-spline based on the t-spline. We show that the t-spline is a t-spline with a t-spline-like structure. We also show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-s\n",
      "\n",
      "🔍 Connections:\n",
      "---\n",
      "\n",
      "🔍 Themes:\n",
      "--- txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt t\n",
      "\n",
      "🔍 Summary:\n",
      "We train a reverse process mean function approximator to predict t, or by modifying its parameterization, we can train it to predict .\n",
      "\n",
      "🔍 Connections:\n",
      "a difficult problem to solve.\n",
      "\n",
      "🔍 Themes:\n",
      "Reverse process mean function approximator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2658 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Summary:\n",
      "Main contribution: We propose a new method for predicting the t-axis of a t-spline based on the t-spline. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-spline-like structure. We show that the t-spline is a t-spline with a t-s\n",
      "\n",
      "Final Connections Across Papers:\n",
      "--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
      "\n",
      "Final Themes:\n",
      "Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Denoising Deno\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    # Remove inline citations like [14], [14, 27]\n",
    "    text = re.sub(r\"\\[[0-9,\\s]+\\]\", \"\", text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    # Remove LaTeX math expressions\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    # Remove repeated words\n",
    "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,;:?!\\s]\", \"\", text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load a document and return its content\n",
    "def load_document(file_path):\n",
    "    loader = TextLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.page_content = clean_text(doc.page_content)  # Apply cleaning here\n",
    "    return docs\n",
    "\n",
    "# Split the document into chunks ensuring each chunk is under the token limit\n",
    "def split_document(docs, chunk_size=1000, chunk_overlap=50):\n",
    "    # Make sure docs is always a list\n",
    "    if not isinstance(docs, list):\n",
    "        docs = [docs]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# Vector store (use sentence embeddings)\n",
    "def create_faiss_index(docs):\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    return FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Load HuggingFace LLM\n",
    "def load_llm():\n",
    "    model_id = \"google/flan-t5-xl\"  # Better GPU utilization, faster than flan-t5-large\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\")  # Move model to GPU\n",
    "    #pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0, max_new_tokens=512)\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "# Build RAG chain\n",
    "def build_qa_chain(llm, vectorstore):\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever(), chain_type=\"stuff\")\n",
    "\n",
    "# Analyze a single document in chunks and store results with a chunk limit\n",
    "def analyze_document(llm, docs, batch_size=1, max_chunks=10):\n",
    "    results = {\"Summary\": [], \"Connections\": [], \"Themes\": []}\n",
    "    chunks = split_document(docs)\n",
    "    chunks = chunks[:max_chunks]\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "\n",
    "        for label, q in [\n",
    "            (\"Summary\", \"Summarize the following scientific paper text in concise bullet points. Include: Main contribution, Dataset used, method, Evaluation metrics and Key results.\"),\n",
    "            (\"Connections\", \"You are reading several research papers. Based on the passage below, what connections or similarities can you identify with other papers on diffusion models? Mention common techniques, models, datasets, or evaluation strategies.\"),\n",
    "            (\"Themes\", \"What are the central *research themes* in the following paper? List them as concise topics.\")\n",
    "        ]:\n",
    "            prompts = [f\"{q}\\n\\n---\\n{chunk.page_content.strip()}\" for chunk in batch]\n",
    "            try:\n",
    "                responses = llm.pipeline(prompts)\n",
    "                for response in responses:\n",
    "                    text = response['generated_text'].strip()\n",
    "                    results[label].append(text)\n",
    "                    print(f\"\\n🔍 {label}:\\n{text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during {label} batch: {e}\")\n",
    "                results[label].extend([\"Error\"] * len(batch))\n",
    "\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main pipeline\n",
    "def process_all_documents(data_dir=\"data\", max_chunks=10):\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "    results = {\"Summary\": [], \"Connections\": [], \"Themes\": []}\n",
    "\n",
    "    llm = load_llm()\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"\\n📄 Processing: {file_path}\")\n",
    "        # Load and clean the document\n",
    "        doc = load_document(file_path)\n",
    "\n",
    "        # Process the document in chunks\n",
    "        doc_results = analyze_document(llm, docs=doc, max_chunks=max_chunks)\n",
    "\n",
    "        # Collect results\n",
    "        for label in results:\n",
    "            results[label].extend(doc_results.get(label, []))\n",
    "\n",
    "        # Free up GPU memory after processing each document\n",
    "        del doc\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()  # Run garbage collection to free memory\n",
    "\n",
    "    # Clean-up results (e.g., remove empty strings or redundant entries)\n",
    "    for label in results:\n",
    "        flat = [str(item).strip() for sublist in results[label] for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        cleaned = [s for s in flat if s and s.lower() != \"error\"]\n",
    "        combined_text = \"\\n\".join(cleaned)\n",
    "\n",
    "        if label == \"Themes\":\n",
    "          final_theme = summarize_combined_output(llm, combined_text, label) # Remove duplicates\n",
    "          themes = list(dict.fromkeys([line.strip() for line in final_theme.split(\"\\n\") if line.strip()]))\n",
    "          results[label] = \"\\n\".join(themes)\n",
    "        else:\n",
    "          results[label] = summarize_combined_output(llm, combined_text, label)\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "# Summarize combined chunk outputs into a single final output\n",
    "def summarize_combined_output(llm, text, label):\n",
    "    prompts = {\n",
    "        \"Summary\": \"You are a helpful scientific assistant. Based on the following combined summaries of a scientific paper, provide a single concise overall summary. Mention the main contribution, dataset used, method, evaluation metrics, and key results.\",\n",
    "        \"Connections\": \"You are reading several research papers. Based on the following notes, summarize the common connections or similarities across papers, focusing on shared techniques, datasets, or models.\",\n",
    "        \"Themes\": \"Summarize the central research themes mentioned in the combined text below. List them as concise, broad topics.\"\n",
    "    }\n",
    "    prompt = f\"{prompts[label]}\\n\\n{text}\"\n",
    "    try:\n",
    "        response = llm.pipeline(prompt)\n",
    "        return response[0][\"generated_text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating final {label}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "\n",
    "# Main execution\n",
    "results = process_all_documents(data_dir=\"data\", max_chunks=10)  # Limit the number of chunks for faster processing\n",
    "# Final outputs\n",
    "final_connections = results.get(\"Connections\", \"\")\n",
    "final_themes = results.get(\"Themes\", \"\")\n",
    "final_summary = results.get(\"Summary\", \"\")\n",
    "\n",
    "# Display summaries\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(final_summary)\n",
    "print(\"\\nFinal Connections Across Papers:\")\n",
    "print(final_connections)\n",
    "print(\"\\nFinal Themes:\")\n",
    "print(final_themes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
