{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":85994,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72253,"modelId":76277},{"sourceId":104623,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277},{"sourceId":104625,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72253,"modelId":76277}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook Imports and Initial Setup\n\nIn the initial cells, essential libraries and dependencies are imported, setting up the environment for the fine-tuning process. We use a custom trainer, configure GPU settings, and set up necessary libraries like Hugging Face's `transformers` and `peft` (Parameter-Efficient Fine-Tuning).","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade peft transformers bitsandbytes datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-08T23:43:18.516494Z","iopub.execute_input":"2025-04-08T23:43:18.516888Z","iopub.status.idle":"2025-04-08T23:43:38.157004Z","shell.execute_reply.started":"2025-04-08T23:43:18.516854Z","shell.execute_reply":"2025-04-08T23:43:38.155878Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting peft\n  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting datasets\n  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nCollecting huggingface_hub>=0.25.0 (from peft)\n  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nDownloading peft-0.15.1-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.51.1-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, transformers, peft, datasets, bitsandbytes\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.29.0\n    Uninstalling huggingface-hub-0.29.0:\n      Successfully uninstalled huggingface-hub-0.29.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.3.1\n    Uninstalling datasets-3.3.1:\n      Successfully uninstalled datasets-3.3.1\nSuccessfully installed bitsandbytes-0.45.5 datasets-3.5.0 huggingface_hub-0.30.2 peft-0.15.1 transformers-4.51.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import get_peft_model, LoraConfig, TaskType","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:43:38.158574Z","iopub.execute_input":"2025-04-08T23:43:38.158854Z","iopub.status.idle":"2025-04-08T23:44:02.062042Z","shell.execute_reply.started":"2025-04-08T23:43:38.158830Z","shell.execute_reply":"2025-04-08T23:44:02.061237Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:44:02.063453Z","iopub.execute_input":"2025-04-08T23:44:02.064153Z","iopub.status.idle":"2025-04-08T23:44:02.068013Z","shell.execute_reply.started":"2025-04-08T23:44:02.064099Z","shell.execute_reply":"2025-04-08T23:44:02.067041Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Load Datasets**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the Sanskrit dataset from OSCAR\ndataset = load_dataset('oscar', 'unshuffled_deduplicated_sa', split='train')","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:44:02.069857Z","iopub.execute_input":"2025-04-08T23:44:02.070265Z","iopub.status.idle":"2025-04-08T23:45:00.809848Z","shell.execute_reply.started":"2025-04-08T23:44:02.070231Z","shell.execute_reply":"2025-04-08T23:45:00.808974Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbe3b790371491db352bf0ed22a312a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ed647d318a4ba5a78973431c7911aa"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec01c24a79f4081819bb9a6c88bf12c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3618f113e88d48728336fea9617644f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad65aa6c1a34742a436e7683a118490"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Display a few samples\n\nfor i in range(3):\n    print(f\"Sample {i+1}:\\n{dataset[i]['text']}\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:00.810773Z","iopub.execute_input":"2025-04-08T23:45:00.811021Z","iopub.status.idle":"2025-04-08T23:45:00.817546Z","shell.execute_reply.started":"2025-04-08T23:45:00.811001Z","shell.execute_reply":"2025-04-08T23:45:00.816318Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sample 1:\nअनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति । तस्‍य कानिचन् चित्राणि पूर्वमेव प्रकाशितानि सन्ति । द्वौ चलचित्रौ अपि प्रकाशितौ । तस्मिन् एव क्रमेण एतत् सीतास्‍वयंबर इति चलचित्रं प्रकाश्यते ।\nलट् लकार: एकवचनम् द्विवचनम्बहुवचनम्प्रथमपुरुष:गच्‍छतिगच्‍छत:गच्‍छन्तिमध्‍यमपुरुष:गच्‍छसिगच्‍छथ:गच्‍छथउत्‍तमपुरुष:गच्‍छामिगच्‍छाव:गच्‍छाम:\n\nSample 2:\nपाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति; अन्याः संस्थित्यः अपि सन्ति । अधिकं ज्ञातुम् अत्र उपयोगस्य संस्थितिं पश्यतु ।\n\nSample 3:\nस्थिते च कवचे देहे नास्ति मृत्युश्च जीविनाम् । अस्त्रे-शस्त्रे-जले-वह्नौ सिद्धिश्चेन्नास्ति संशयः ॥ ४॥\nप्राच्यां मां पातु भूतेशः आग्नेय्यां पातु शङ्करः । दक्षिणे पातु मां रुद्रो नैॠत्यां स्थाणुरेव च ॥ ११॥\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Data Cleaning**","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_text(example):\n    text = example['text']\n    # Remove HTML tags\n    text = re.sub(r'<[^>]+>', '', text)\n    # Remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Remove non-Sanskrit characters (retain Devanagari script)\n    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n    return {'text': text}\n\n# Apply the cleaning function\ndataset = dataset.map(clean_text)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:00.818498Z","iopub.execute_input":"2025-04-08T23:45:00.818811Z","iopub.status.idle":"2025-04-08T23:45:02.805724Z","shell.execute_reply.started":"2025-04-08T23:45:00.818771Z","shell.execute_reply":"2025-04-08T23:45:02.804710Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3ed9b0dd7b4c2da9f837b87513dbd5"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def filter_short_texts(example):\n    return len(example['text'].split()) > 5  # Keep texts longer than 5 words\n\ndataset = dataset.filter(filter_short_texts)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:02.806971Z","iopub.execute_input":"2025-04-08T23:45:02.807368Z","iopub.status.idle":"2025-04-08T23:45:03.130009Z","shell.execute_reply.started":"2025-04-08T23:45:02.807330Z","shell.execute_reply":"2025-04-08T23:45:03.128941Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/7121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66131e762b114664a1ab2646ab0adc61"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"**Load Model**","metadata":{}},{"cell_type":"code","source":"model_name = '/kaggle/input/gemma-2/transformers/gemma-2-2b/1'","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:03.133018Z","iopub.execute_input":"2025-04-08T23:45:03.133299Z","iopub.status.idle":"2025-04-08T23:45:03.137322Z","shell.execute_reply.started":"2025-04-08T23:45:03.133276Z","shell.execute_reply":"2025-04-08T23:45:03.136256Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Use a multilingual tokenizer or train a new one\n# Load the tokenizer for Gemma 2 9B model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:03.139135Z","iopub.execute_input":"2025-04-08T23:45:03.139384Z","iopub.status.idle":"2025-04-08T23:45:04.845457Z","shell.execute_reply.started":"2025-04-08T23:45:03.139355Z","shell.execute_reply":"2025-04-08T23:45:04.844621Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Check whether Cuda is available**","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:04.846421Z","iopub.execute_input":"2025-04-08T23:45:04.846778Z","iopub.status.idle":"2025-04-08T23:45:04.908824Z","shell.execute_reply.started":"2025-04-08T23:45:04.846745Z","shell.execute_reply":"2025-04-08T23:45:04.907401Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**LORA Configurations**","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\n#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map='auto'\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:45:04.909893Z","iopub.execute_input":"2025-04-08T23:45:04.910253Z","iopub.status.idle":"2025-04-08T23:47:13.338917Z","shell.execute_reply.started":"2025-04-08T23:45:04.910212Z","shell.execute_reply":"2025-04-08T23:47:13.338036Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749af5493cd74375a1875b3230addb73"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=4,\n    lora_alpha=16,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type=TaskType.CAUSAL_LM,\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:13.339823Z","iopub.execute_input":"2025-04-08T23:47:13.340097Z","iopub.status.idle":"2025-04-08T23:47:13.344557Z","shell.execute_reply.started":"2025-04-08T23:47:13.340075Z","shell.execute_reply":"2025-04-08T23:47:13.343505Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:13.345623Z","iopub.execute_input":"2025-04-08T23:47:13.345906Z","iopub.status.idle":"2025-04-08T23:47:13.527963Z","shell.execute_reply.started":"2025-04-08T23:47:13.345882Z","shell.execute_reply":"2025-04-08T23:47:13.527049Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if 'lora' in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:13.528991Z","iopub.execute_input":"2025-04-08T23:47:13.529294Z","iopub.status.idle":"2025-04-08T23:47:13.537628Z","shell.execute_reply.started":"2025-04-08T23:47:13.529271Z","shell.execute_reply":"2025-04-08T23:47:13.536387Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"**Number of trainable parameters**","metadata":{}},{"cell_type":"code","source":"# Verify trainable parameters\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_params = 0\n    for _, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"Trainable params: {trainable_params} | All params: {all_params} | \"\n        f\"Trainable percentage: {100 * trainable_params / all_params:.2f}%\"\n    )\n\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:13.538839Z","iopub.execute_input":"2025-04-08T23:47:13.539269Z","iopub.status.idle":"2025-04-08T23:47:13.560947Z","shell.execute_reply.started":"2025-04-08T23:47:13.539230Z","shell.execute_reply":"2025-04-08T23:47:13.559791Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Trainable params: 1597440 | All params: 1603801344 | Trainable percentage: 0.10%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#model.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:13.562241Z","iopub.execute_input":"2025-04-08T23:47:13.562609Z","iopub.status.idle":"2025-04-08T23:47:13.577336Z","shell.execute_reply.started":"2025-04-08T23:47:13.562570Z","shell.execute_reply":"2025-04-08T23:47:13.576267Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**Prepare Tokenized Dataset**","metadata":{}},{"cell_type":"code","source":"# Prepare dataset\ndef tokenize_function(examples):\n    tokens = tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=128,\n    )\n    tokens['labels'] = tokens['input_ids'].copy()\n    return tokens\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:13.578541Z","iopub.execute_input":"2025-04-08T23:47:13.578912Z","iopub.status.idle":"2025-04-08T23:47:20.879880Z","shell.execute_reply.started":"2025-04-08T23:47:13.578875Z","shell.execute_reply":"2025-04-08T23:47:20.879028Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae2e224ef634f7e81bc90377cd79925"}},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"**Customized Data Collator**","metadata":{}},{"cell_type":"code","source":"# Custom data collator\ndef custom_data_collator(features):\n    batch = {}\n    for key in features[0].keys():\n        batch[key] = torch.stack([torch.tensor(f[key]) for f in features]).to('cuda')\n    return batch","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:20.880815Z","iopub.execute_input":"2025-04-08T23:47:20.881052Z","iopub.status.idle":"2025-04-08T23:47:20.885958Z","shell.execute_reply.started":"2025-04-08T23:47:20.881032Z","shell.execute_reply":"2025-04-08T23:47:20.884907Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Custom Trainer Initialization\n\nA `CustomTrainer` instance is initialized with the model, training arguments, and dataset. This custom trainer coordinates the training process, using the arguments and model components configured earlier.","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def prepare_inputs(self, inputs):\n        # Ensure inputs are on GPU\n        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:20.886961Z","iopub.execute_input":"2025-04-08T23:47:20.887253Z","iopub.status.idle":"2025-04-08T23:47:20.915312Z","shell.execute_reply.started":"2025-04-08T23:47:20.887231Z","shell.execute_reply":"2025-04-08T23:47:20.914236Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Get a batch of data\nbatch = tokenized_dataset[:2]  # Take the first 2 samples\n\n# Convert batch to tensors and move to GPU\ninputs = {\n    'input_ids': torch.tensor(batch['input_ids']).to('cuda'),\n    'attention_mask': torch.tensor(batch['attention_mask']).to('cuda'),\n    'labels': torch.tensor(batch['labels']).to('cuda'),\n}","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:20.916378Z","iopub.execute_input":"2025-04-08T23:47:20.916669Z","iopub.status.idle":"2025-04-08T23:47:20.938757Z","shell.execute_reply.started":"2025-04-08T23:47:20.916645Z","shell.execute_reply":"2025-04-08T23:47:20.937569Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"**Set model to evaluation mode**","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    loss = outputs.loss\n    print(f\"Loss: {loss.item()}\")\n    print(f\"Loss requires grad: {loss.requires_grad}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:20.939888Z","iopub.execute_input":"2025-04-08T23:47:20.940260Z","iopub.status.idle":"2025-04-08T23:47:21.870707Z","shell.execute_reply.started":"2025-04-08T23:47:20.940225Z","shell.execute_reply":"2025-04-08T23:47:21.869679Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loss: 5.427135944366455\nLoss requires grad: False\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Set model to training mode**","metadata":{}},{"cell_type":"code","source":"model.train()  \n\noutputs = model(**inputs)\nloss = outputs.loss\n\nprint(f\"Loss: {loss.item()}\")\nprint(f\"Loss requires grad: {loss.requires_grad}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:21.871663Z","iopub.execute_input":"2025-04-08T23:47:21.872070Z","iopub.status.idle":"2025-04-08T23:47:22.182022Z","shell.execute_reply.started":"2025-04-08T23:47:21.872023Z","shell.execute_reply":"2025-04-08T23:47:22.181043Z"},"trusted":true},"outputs":[{"name":"stderr","text":"It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n","output_type":"stream"},{"name":"stdout","text":"Loss: 5.427135944366455\nLoss requires grad: True\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"{name}: requires_grad={param.requires_grad}, device={param.device}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-08T23:47:22.185366Z","iopub.execute_input":"2025-04-08T23:47:22.185647Z","iopub.status.idle":"2025-04-08T23:47:22.238259Z","shell.execute_reply.started":"2025-04-08T23:47:22.185625Z","shell.execute_reply":"2025-04-08T23:47:22.237187Z"},"trusted":true},"outputs":[{"name":"stdout","text":"base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:22.239713Z","iopub.execute_input":"2025-04-08T23:47:22.239989Z","iopub.status.idle":"2025-04-08T23:47:22.244148Z","shell.execute_reply.started":"2025-04-08T23:47:22.239967Z","shell.execute_reply":"2025-04-08T23:47:22.242874Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Setting Training Arguments\n\nTraining arguments are configured here, including batch size, number of training epochs, learning rate, and other key settings. This configuration is essential for controlling the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='/kaggle/working/results/',\n    num_train_epochs=1,\n    per_device_train_batch_size=10,  # Adjust based on GPU memory\n    gradient_accumulation_steps=2,  # Adjust to maintain effective batch size\n    learning_rate=1e-4,\n    fp16=True,\n    save_total_limit=2,\n    save_steps=50,\n    gradient_checkpointing=False,\n    optim='adamw_bnb_8bit',\n    dataloader_pin_memory=False\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:22.245046Z","iopub.execute_input":"2025-04-08T23:47:22.245337Z","iopub.status.idle":"2025-04-08T23:47:22.290787Z","shell.execute_reply.started":"2025-04-08T23:47:22.245314Z","shell.execute_reply":"2025-04-08T23:47:22.289835Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=custom_data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:22.291714Z","iopub.execute_input":"2025-04-08T23:47:22.291952Z","iopub.status.idle":"2025-04-08T23:47:22.498350Z","shell.execute_reply.started":"2025-04-08T23:47:22.291932Z","shell.execute_reply":"2025-04-08T23:47:22.497549Z"},"trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Memory Management and Training Execution\n\nTo manage memory effectively on the GPU, we clear the CUDA cache before starting the actual training. Then, the `trainer.train()` command is used to start the fine-tuning.\n\nThe training progress, including the number of steps completed, current loss, and other details, is displayed to monitor the model's learning curve.","metadata":{}},{"cell_type":"code","source":"# Clear cache and start training\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:22.499179Z","iopub.execute_input":"2025-04-08T23:47:22.499427Z","iopub.status.idle":"2025-04-08T23:47:22.520724Z","shell.execute_reply.started":"2025-04-08T23:47:22.499406Z","shell.execute_reply":"2025-04-08T23:47:22.519860Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-04-08T23:47:22.521591Z","iopub.execute_input":"2025-04-08T23:47:22.521861Z","iopub.status.idle":"2025-04-09T00:13:08.457700Z","shell.execute_reply.started":"2025-04-08T23:47:22.521836Z","shell.execute_reply":"2025-04-09T00:13:08.456927Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [348/348 25:40, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=348, training_loss=2.879784244230424, metrics={'train_runtime': 1545.5151, 'train_samples_per_second': 4.501, 'train_steps_per_second': 0.225, 'total_flos': 1.0825485570736128e+16, 'train_loss': 2.879784244230424, 'epoch': 1.0})"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"# Saving the Trained Model\n\nOnce training is complete, we save the fine-tuned model and tokenizer to a directory for future usage in generating Sanskrit text.\n","metadata":{}},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/fine-tuned-gemma2-sanskrit-lora')\ntokenizer.save_pretrained('/kaggle/working/fine-tuned-gemma2-sanskrit-lora')","metadata":{"execution":{"iopub.status.busy":"2025-04-09T00:13:08.458565Z","iopub.execute_input":"2025-04-09T00:13:08.458909Z","iopub.status.idle":"2025-04-09T00:13:09.260407Z","shell.execute_reply.started":"2025-04-09T00:13:08.458874Z","shell.execute_reply":"2025-04-09T00:13:09.259367Z"},"trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer_config.json',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/special_tokens_map.json',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer.model',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/added_tokens.json',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer.json')"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"# Model Loading and Evaluation Setup\n\nIn this section, we load the fine-tuned model into evaluation mode for generating Sanskrit text. The `PeftModel` allows efficient loading and evaluation.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map='auto'\n)\n\nmodel = PeftModel.from_pretrained(model, '/kaggle/working/fine-tuned-gemma2-sanskrit-lora')\n\n# Set the model to evaluation mode\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2025-04-09T00:13:09.261530Z","iopub.execute_input":"2025-04-09T00:13:09.261920Z","iopub.status.idle":"2025-04-09T00:13:19.370094Z","shell.execute_reply.started":"2025-04-09T00:13:09.261882Z","shell.execute_reply":"2025-04-09T00:13:19.368943Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ff1eb2795c4b96a4fe4fc8fd655d0b"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (rotary_emb): Gemma2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"# Text Generation Function\n\nA function, `generate_text`, is defined for generating Sanskrit text based on a provided prompt. Parameters for temperature, top-p sampling, and maximum length control the creativity and diversity of the generated text.\n","metadata":{}},{"cell_type":"code","source":"def generate_text(prompt, max_length=128, num_return_sequences=1):\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    \n    # Generate output sequences\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=num_return_sequences,\n            do_sample=True,           # Use sampling for more diverse outputs\n            temperature=0.7,          # Adjust temperature for creativity\n            top_p=0.9,                # Use top-p sampling\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    # Decode the generated tokens\n    generated_texts = []\n    for output in outputs:\n        text = tokenizer.decode(output, skip_special_tokens=True)\n        generated_texts.append(text)\n    \n    return generated_texts","metadata":{"execution":{"iopub.status.busy":"2025-04-09T00:13:19.371179Z","iopub.execute_input":"2025-04-09T00:13:19.371468Z","iopub.status.idle":"2025-04-09T00:13:19.376998Z","shell.execute_reply.started":"2025-04-09T00:13:19.371445Z","shell.execute_reply":"2025-04-09T00:13:19.376015Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Example Prompt and Generated Text\n\nUsing the `generate_text` function, we provide an example Sanskrit prompt and generate text based on it to showcase the model's capability to create coherent Sanskrit sentences.","metadata":{}},{"cell_type":"code","source":"prompt = \"पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति\"\ngenerated_texts = generate_text(prompt, max_length=128)\nprint(\"Generated Text:\")\nprint(generated_texts[0])","metadata":{"execution":{"iopub.status.busy":"2025-04-09T00:13:19.378026Z","iopub.execute_input":"2025-04-09T00:13:19.378397Z","iopub.status.idle":"2025-04-09T00:13:27.440917Z","shell.execute_reply.started":"2025-04-09T00:13:19.378363Z","shell.execute_reply":"2025-04-09T00:13:27.439949Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated Text:\nपाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति। एतये दानस्य संस्करणस्य इतिहासः प्रथमः एतस्य अभिज्ञापत्रस्य संस्करणस्य इतिहासः। एतस्य संस्करणस्य तृतीय संस्करणस्य तृतीय संस्करणस्य तृतीय संस्करणस्य तृतीय संस्करणस्य तृतीय संस्करणस्य तृतीय संस्करणस्य तृतीय संस्करणस्य तृतीय संस्\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# Additional Prompt for Text Generation\n\nFurther Sanskrit prompts are used to test the model's output diversity and quality, demonstrating the ability to generate text across different contexts in Sanskrit.","metadata":{}},{"cell_type":"code","source":"prompt = \"अनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति ।\"\ngenerated_texts = generate_text(prompt, max_length=128)\n\nprint(\"Generated Text:\")\nprint(generated_texts[0])","metadata":{"execution":{"iopub.status.busy":"2025-04-09T00:13:27.441706Z","iopub.execute_input":"2025-04-09T00:13:27.441949Z","iopub.status.idle":"2025-04-09T00:13:36.542195Z","shell.execute_reply.started":"2025-04-09T00:13:27.441928Z","shell.execute_reply":"2025-04-09T00:13:36.541149Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Generated Text:\nअनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति ।  रामेण पञ्चाङ्गं यस्य जय  मन्त्रं  शक्तं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं  शक्तिं\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, we have successfully fine-tuned the Gemma-2 model using LoRA for generating coherent text in Sanskrit. This approach showcases an efficient way to create and deploy language models in low-resource languages, aiding in language preservation and linguistic research.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}