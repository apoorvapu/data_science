{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIwB7gI1FCRR63f9IB6oCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/drug_activity_prediction_belka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRZbYurSJtI1",
        "outputId": "ac8ef953-f6be-4cf4-9b0e-10e06b44dc6a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"HugeParquetProcessor\").getOrCreate()"
      ],
      "metadata": {
        "id": "qmMaM-zwJxXg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define filename\n",
        "dataset_url = \"https://huggingface.co/datasets/HoangHa/belka-smiles-train-raw/resolve/main/data/train.parquet\"\n",
        "filename = \"train.parquet\"\n",
        "!wget -O $filename $dataset_url\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNdrjJxfKRnR",
        "outputId": "efcc14fa-93b1-4305-ce42-467f7e7029ab"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-17 23:18:36--  https://huggingface.co/datasets/HoangHa/belka-smiles-train-raw/resolve/main/data/train.parquet\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.103, 18.239.50.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/4f/88/4f882ffde40c5b68f15d4d499c1455831a17d74d14834e270757fbff6f6e08f5/3330782a1855d4d18467fc84e4f2248992d5362fced0f1a2e483d545c642355d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train.parquet%3B+filename%3D%22train.parquet%22%3B&Expires=1742257116&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjI1NzExNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzRmLzg4LzRmODgyZmZkZTQwYzViNjhmMTVkNGQ0OTljMTQ1NTgzMWExN2Q3NGQxNDgzNGUyNzA3NTdmYmZmNmY2ZTA4ZjUvMzMzMDc4MmExODU1ZDRkMTg0NjdmYzg0ZTRmMjI0ODk5MmQ1MzYyZmNlZDBmMWEyZTQ4M2Q1NDVjNjQyMzU1ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=L1q0hQig5dam2RxD8zVvMUnVLCl%7EcWiGg0sFFH7mMoodsZ5FY0WIz9z7L4DFUUk5pIJPXWoOwbqK1BB%7EvoGxtsMOP804hcpKh0drP4gWNw6Eytr99GbtmyUnHqeyqlMW2A0N0eIer0lT7xEEh%7EeMk8XaR5ZLUMPzCaOol3s8vU43Sl28-2ag-xi4b814gxwLJkZB9i04iumhiqqXGanEqiow0LsyO%7ELh3KkUP18o4Fg-kVy0ADRI8pBUJkV5kuD-oE0aAtXLYCUF13KRmnvrKfoErFcBLdCt933bNxOq2AiqpkRLTRJ--d6J%7E%7Eo6mMAIfq6gXd9gi4a8Wvf5o9SoNQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-17 23:18:36--  https://cdn-lfs-us-1.hf.co/repos/4f/88/4f882ffde40c5b68f15d4d499c1455831a17d74d14834e270757fbff6f6e08f5/3330782a1855d4d18467fc84e4f2248992d5362fced0f1a2e483d545c642355d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train.parquet%3B+filename%3D%22train.parquet%22%3B&Expires=1742257116&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjI1NzExNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzRmLzg4LzRmODgyZmZkZTQwYzViNjhmMTVkNGQ0OTljMTQ1NTgzMWExN2Q3NGQxNDgzNGUyNzA3NTdmYmZmNmY2ZTA4ZjUvMzMzMDc4MmExODU1ZDRkMTg0NjdmYzg0ZTRmMjI0ODk5MmQ1MzYyZmNlZDBmMWEyZTQ4M2Q1NDVjNjQyMzU1ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=L1q0hQig5dam2RxD8zVvMUnVLCl%7EcWiGg0sFFH7mMoodsZ5FY0WIz9z7L4DFUUk5pIJPXWoOwbqK1BB%7EvoGxtsMOP804hcpKh0drP4gWNw6Eytr99GbtmyUnHqeyqlMW2A0N0eIer0lT7xEEh%7EeMk8XaR5ZLUMPzCaOol3s8vU43Sl28-2ag-xi4b814gxwLJkZB9i04iumhiqqXGanEqiow0LsyO%7ELh3KkUP18o4Fg-kVy0ADRI8pBUJkV5kuD-oE0aAtXLYCUF13KRmnvrKfoErFcBLdCt933bNxOq2AiqpkRLTRJ--d6J%7E%7Eo6mMAIfq6gXd9gi4a8Wvf5o9SoNQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 65.9.86.109, 65.9.86.32, 65.9.86.113, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|65.9.86.109|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3757779095 (3.5G) [binary/octet-stream]\n",
            "Saving to: ‘train.parquet’\n",
            "\n",
            "train.parquet       100%[===================>]   3.50G   103MB/s    in 50s     \n",
            "\n",
            "2025-03-17 23:19:26 (72.0 MB/s) - ‘train.parquet’ saved [3757779095/3757779095]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"train.parquet\")\n",
        "# Check schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "6jl4_zxtJ0dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('id','buildingblock1_smiles',\t'buildingblock2_smiles',\t'buildingblock3_smiles')\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0i1UdOHXPt4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "id": "9tqdrXylPBsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, FloatType"
      ],
      "metadata": {
        "id": "k2KPrEwSLmCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert SMILES to DenseVector\n",
        "def smiles_to_dense_fp(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=512)\n",
        "        return Vectors.dense([float(x) for x in fp])  # Convert to DenseVector\n",
        "    return Vectors.dense([0.0] * 512)  # Return zero vector for invalid SMILES\n",
        "\n",
        "# Register the UDF with VectorUDT to handle DenseVector serialization\n",
        "fp_udf = udf(smiles_to_dense_fp, VectorUDT())\n",
        "\n",
        "# Apply function to create fingerprint column\n",
        "df = df.withColumn(\"Fingerprint\", fp_udf(col(\"molecule_smiles\")))\n"
      ],
      "metadata": {
        "id": "dy-_EVBWExba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "# Convert Protein names to numerical indices\n",
        "indexer = StringIndexer(inputCol=\"protein_name\", outputCol=\"Protein_Index\")\n",
        "df = indexer.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "TwOOqXqYPSw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "iO8-OmHCSkIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=[\"Fingerprint\", \"Protein_Index\"], outputCol=\"features\")\n",
        "df = assembler.transform(df).select(\"features\", col(\"binds\").alias(\"label\"))\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "W8inePOSPkHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "pLjzOX2RU1S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "# Split into training (80%) and test (20%) sets\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "1WUdiP78PmhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest Classifier\n",
        "rf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=1)\n",
        "model = rf.fit(train_df)"
      ],
      "metadata": {
        "id": "Cq0eGfBUYgH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "predictions = model.transform(test_df)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(5)\n"
      ],
      "metadata": {
        "id": "IVEItItET8ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Test AUC: {auc:.3f}\")\n"
      ],
      "metadata": {
        "id": "V5zrf6ulPo2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdcIqWQvYpjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}