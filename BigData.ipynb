{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdOrFUQ2nzCCBSpVl48//O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvapu/data_science/blob/main/BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data\n",
        "\n",
        "1. Out-of-Core Processing (Processing Without Loading Everything in RAM)\n",
        "These tools allow you to process datasets that don't fit in memory.\n",
        "\n",
        "Pandas Alternatives for Large Datasets is Dask: Parallel computing library that extends pandas and NumPy to work on large datasets. Example: Using Dask Instead of Pandas"
      ],
      "metadata": {
        "id": "HUi3JWjVsSYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "df = dd.read_csv('large_dataset.csv')  # Lazy loading\n",
        "\n",
        "print(df.head())  # Operations happen in parallel"
      ],
      "metadata": {
        "id": "i11N9KxrszSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Distributed Computing (Scaling Beyond One Machine)\n",
        "For big data processing, distributed frameworks like Apache Spark work well.\n",
        "\n",
        "PySpark (Apache Spark in Python): Handles datasets TBs in size. Works with HDFS, S3, and cloud storage.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BigData\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"large_dataset.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "oBt-zJVWtB6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cloud & External Storage for Large Data\n",
        "If your local system has limited RAM, you can process data in cloud-based storage:\n",
        "\n",
        "Google Colab + Google Drive/BigQuery (for structured data)\n",
        "\n",
        "Hugging Face Datasets (for large ML datasets)\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "query = \"SELECT * FROM `bigquery-public-data.ml_datasets` LIMIT 1000\"\n",
        "\n",
        "df = client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "718It2P-tMci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. AI Modeling on Large Datasets:\n",
        "\n",
        "Incremental Learning (Training Models in Chunks)\n",
        "\n",
        "Scikit-learnâ€™s partial_fit() (for models like logistic regression, SVM)\n",
        "\n",
        "TensorFlow/Keras fit_generator() (for large image/text datasets)\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "clf = SGDClassifier()  # Stochastic Gradient Descent\n",
        "\n",
        "for chunk in pd.read_csv(\"large_dataset.csv\", chunksize=10000):\n",
        "    clf.partial_fit(chunk.drop(\"target\", axis=1), chunk[\"target\"], classes=[0, 1])\n"
      ],
      "metadata": {
        "id": "4JSB_UiwtmZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Deep Learning on Large Data: Use TensorFlow/Keras with Data Generators:\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator().flow_from_directory('large_dataset', batch_size=32)\n",
        "\n",
        "model.fit(datagen, epochs=10)"
      ],
      "metadata": {
        "id": "UsH3hYY5uLc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1EmkIwWyuOC5"
      }
    }
  ]
}