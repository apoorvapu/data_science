{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Imports and Initial Setup\n",
    "\n",
    "In the initial cells, essential libraries and dependencies are imported, setting up the environment for the fine-tuning process. We use a custom trainer, configure GPU settings, and set up necessary libraries like Hugging Face's `transformers` and `peft` (Parameter-Efficient Fine-Tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-01T14:39:18.966794Z",
     "iopub.status.busy": "2024-11-01T14:39:18.966334Z",
     "iopub.status.idle": "2024-11-01T14:39:48.712223Z",
     "shell.execute_reply": "2024-11-01T14:39:48.710995Z",
     "shell.execute_reply.started": "2024-11-01T14:39:18.966754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, transformers, datasets, peft\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.0.1\n",
      "    Uninstalling datasets-3.0.1:\n",
      "      Successfully uninstalled datasets-3.0.1\n",
      "Successfully installed bitsandbytes-0.44.1 datasets-3.1.0 peft-0.13.2 transformers-4.46.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade peft transformers bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:39:48.715416Z",
     "iopub.status.busy": "2024-11-01T14:39:48.714652Z",
     "iopub.status.idle": "2024-11-01T14:40:08.715237Z",
     "shell.execute_reply": "2024-11-01T14:40:08.714490Z",
     "shell.execute_reply.started": "2024-11-01T14:39:48.715368Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:08.716803Z",
     "iopub.status.busy": "2024-11-01T14:40:08.716228Z",
     "iopub.status.idle": "2024-11-01T14:40:08.721108Z",
     "shell.execute_reply": "2024-11-01T14:40:08.720235Z",
     "shell.execute_reply.started": "2024-11-01T14:40:08.716769Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:08.723455Z",
     "iopub.status.busy": "2024-11-01T14:40:08.723153Z",
     "iopub.status.idle": "2024-11-01T14:40:18.963147Z",
     "shell.execute_reply": "2024-11-01T14:40:18.962395Z",
     "shell.execute_reply.started": "2024-11-01T14:40:08.723412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32944448d9f41bb98cd1e3525994f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e8d37fb2924c9fa09a2b83e2ca3f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0613909cc96c4f478d2ac1b351e40b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50721c9e1c1a4e829a7bdc1073087f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce94ae88ceb5414988596b8cd5fd4b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Sanskrit dataset from OSCAR\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_sa', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:18.964592Z",
     "iopub.status.busy": "2024-11-01T14:40:18.964254Z",
     "iopub.status.idle": "2024-11-01T14:40:18.971886Z",
     "shell.execute_reply": "2024-11-01T14:40:18.970973Z",
     "shell.execute_reply.started": "2024-11-01T14:40:18.964557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "अनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति । तस्‍य कानिचन् चित्राणि पूर्वमेव प्रकाशितानि सन्ति । द्वौ चलचित्रौ अपि प्रकाशितौ । तस्मिन् एव क्रमेण एतत् सीतास्‍वयंबर इति चलचित्रं प्रकाश्यते ।\n",
      "लट् लकार: एकवचनम् द्विवचनम्बहुवचनम्प्रथमपुरुष:गच्‍छतिगच्‍छत:गच्‍छन्तिमध्‍यमपुरुष:गच्‍छसिगच्‍छथ:गच्‍छथउत्‍तमपुरुष:गच्‍छामिगच्‍छाव:गच्‍छाम:\n",
      "\n",
      "Sample 2:\n",
      "पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति; अन्याः संस्थित्यः अपि सन्ति । अधिकं ज्ञातुम् अत्र उपयोगस्य संस्थितिं पश्यतु ।\n",
      "\n",
      "Sample 3:\n",
      "स्थिते च कवचे देहे नास्ति मृत्युश्च जीविनाम् । अस्त्रे-शस्त्रे-जले-वह्नौ सिद्धिश्चेन्नास्ति संशयः ॥ ४॥\n",
      "प्राच्यां मां पातु भूतेशः आग्नेय्यां पातु शङ्करः । दक्षिणे पातु मां रुद्रो नैॠत्यां स्थाणुरेव च ॥ ११॥\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a few samples\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\\n{dataset[i]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:18.973633Z",
     "iopub.status.busy": "2024-11-01T14:40:18.973213Z",
     "iopub.status.idle": "2024-11-01T14:40:21.425375Z",
     "shell.execute_reply": "2024-11-01T14:40:21.424479Z",
     "shell.execute_reply.started": "2024-11-01T14:40:18.973583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b889617c9740e4918e2b61024fa131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(example):\n",
    "    text = example['text']\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove non-Sanskrit characters (retain Devanagari script)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "    return {'text': text}\n",
    "\n",
    "# Apply the cleaning function\n",
    "dataset = dataset.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:21.427480Z",
     "iopub.status.busy": "2024-11-01T14:40:21.426822Z",
     "iopub.status.idle": "2024-11-01T14:40:21.727740Z",
     "shell.execute_reply": "2024-11-01T14:40:21.726812Z",
     "shell.execute_reply.started": "2024-11-01T14:40:21.427416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f386d93133493a979148c16ee2e53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_short_texts(example):\n",
    "    return len(example['text'].split()) > 5  # Keep texts longer than 5 words\n",
    "\n",
    "dataset = dataset.filter(filter_short_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:21.729281Z",
     "iopub.status.busy": "2024-11-01T14:40:21.728980Z",
     "iopub.status.idle": "2024-11-01T14:40:21.733418Z",
     "shell.execute_reply": "2024-11-01T14:40:21.732584Z",
     "shell.execute_reply.started": "2024-11-01T14:40:21.729249Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = '/kaggle/input/gemma-2/transformers/gemma-2-2b/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:21.735689Z",
     "iopub.status.busy": "2024-11-01T14:40:21.734827Z",
     "iopub.status.idle": "2024-11-01T14:40:22.989761Z",
     "shell.execute_reply": "2024-11-01T14:40:22.988746Z",
     "shell.execute_reply.started": "2024-11-01T14:40:21.735615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a multilingual tokenizer or train a new one\n",
    "# Load the tokenizer for Gemma 2 9B model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check whether Cuda is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:22.993558Z",
     "iopub.status.busy": "2024-11-01T14:40:22.993238Z",
     "iopub.status.idle": "2024-11-01T14:40:23.026378Z",
     "shell.execute_reply": "2024-11-01T14:40:23.025460Z",
     "shell.execute_reply.started": "2024-11-01T14:40:22.993519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LORA Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:40:23.027824Z",
     "iopub.status.busy": "2024-11-01T14:40:23.027485Z",
     "iopub.status.idle": "2024-11-01T14:41:27.214310Z",
     "shell.execute_reply": "2024-11-01T14:41:27.213496Z",
     "shell.execute_reply.started": "2024-11-01T14:40:23.027788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106f38870f7a4a4e8b08126d5adaeb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:27.216029Z",
     "iopub.status.busy": "2024-11-01T14:41:27.215633Z",
     "iopub.status.idle": "2024-11-01T14:41:27.221357Z",
     "shell.execute_reply": "2024-11-01T14:41:27.220488Z",
     "shell.execute_reply.started": "2024-11-01T14:41:27.215986Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:27.222936Z",
     "iopub.status.busy": "2024-11-01T14:41:27.222611Z",
     "iopub.status.idle": "2024-11-01T14:41:27.472731Z",
     "shell.execute_reply": "2024-11-01T14:41:27.471759Z",
     "shell.execute_reply.started": "2024-11-01T14:41:27.222904Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:27.474239Z",
     "iopub.status.busy": "2024-11-01T14:41:27.473934Z",
     "iopub.status.idle": "2024-11-01T14:41:27.484048Z",
     "shell.execute_reply": "2024-11-01T14:41:27.483166Z",
     "shell.execute_reply.started": "2024-11-01T14:41:27.474208Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of trainable parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:27.485550Z",
     "iopub.status.busy": "2024-11-01T14:41:27.485178Z",
     "iopub.status.idle": "2024-11-01T14:41:27.500097Z",
     "shell.execute_reply": "2024-11-01T14:41:27.499225Z",
     "shell.execute_reply.started": "2024-11-01T14:41:27.485517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 6389760 | All params: 1608593664 | Trainable percentage: 0.40%\n"
     ]
    }
   ],
   "source": [
    "# Verify trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params} | All params: {all_params} | \"\n",
    "        f\"Trainable percentage: {100 * trainable_params / all_params:.2f}%\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:27.503596Z",
     "iopub.status.busy": "2024-11-01T14:41:27.503250Z",
     "iopub.status.idle": "2024-11-01T14:41:27.509759Z",
     "shell.execute_reply": "2024-11-01T14:41:27.508920Z",
     "shell.execute_reply.started": "2024-11-01T14:41:27.503552Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Tokenized Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:27.511525Z",
     "iopub.status.busy": "2024-11-01T14:41:27.511203Z",
     "iopub.status.idle": "2024-11-01T14:41:35.458817Z",
     "shell.execute_reply": "2024-11-01T14:41:35.457881Z",
     "shell.execute_reply.started": "2024-11-01T14:41:27.511491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec74bf3dca745c9b8b0d33500539457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customized Data Collator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:35.460810Z",
     "iopub.status.busy": "2024-11-01T14:41:35.460179Z",
     "iopub.status.idle": "2024-11-01T14:41:35.466241Z",
     "shell.execute_reply": "2024-11-01T14:41:35.465351Z",
     "shell.execute_reply.started": "2024-11-01T14:41:35.460762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom data collator\n",
    "def custom_data_collator(features):\n",
    "    batch = {}\n",
    "    for key in features[0].keys():\n",
    "        batch[key] = torch.stack([torch.tensor(f[key]) for f in features]).to('cuda')\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trainer Initialization\n",
    "\n",
    "A `CustomTrainer` instance is initialized with the model, training arguments, and dataset. This custom trainer coordinates the training process, using the arguments and model components configured earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:35.467933Z",
     "iopub.status.busy": "2024-11-01T14:41:35.467580Z",
     "iopub.status.idle": "2024-11-01T14:41:35.515447Z",
     "shell.execute_reply": "2024-11-01T14:41:35.514561Z",
     "shell.execute_reply.started": "2024-11-01T14:41:35.467892Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def prepare_inputs(self, inputs):\n",
    "        # Ensure inputs are on GPU\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:35.516951Z",
     "iopub.status.busy": "2024-11-01T14:41:35.516602Z",
     "iopub.status.idle": "2024-11-01T14:41:35.530397Z",
     "shell.execute_reply": "2024-11-01T14:41:35.529494Z",
     "shell.execute_reply.started": "2024-11-01T14:41:35.516911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "batch = tokenized_dataset[:2]  # Take the first 2 samples\n",
    "\n",
    "# Convert batch to tensors and move to GPU\n",
    "inputs = {\n",
    "    'input_ids': torch.tensor(batch['input_ids']).to('cuda'),\n",
    "    'attention_mask': torch.tensor(batch['attention_mask']).to('cuda'),\n",
    "    'labels': torch.tensor(batch['labels']).to('cuda'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set model to evaluation mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:35.531778Z",
     "iopub.status.busy": "2024-11-01T14:41:35.531462Z",
     "iopub.status.idle": "2024-11-01T14:41:37.032937Z",
     "shell.execute_reply": "2024-11-01T14:41:37.031968Z",
     "shell.execute_reply.started": "2024-11-01T14:41:35.531747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 19.825746536254883\n",
      "Loss requires grad: False\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    print(f\"Loss requires grad: {loss.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set model to training mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:37.034613Z",
     "iopub.status.busy": "2024-11-01T14:41:37.034189Z",
     "iopub.status.idle": "2024-11-01T14:41:38.002709Z",
     "shell.execute_reply": "2024-11-01T14:41:38.001622Z",
     "shell.execute_reply.started": "2024-11-01T14:41:37.034568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 19.825746536254883\n",
      "Loss requires grad: True\n"
     ]
    }
   ],
   "source": [
    "model.train()  \n",
    "\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "print(f\"Loss requires grad: {loss.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:38.004584Z",
     "iopub.status.busy": "2024-11-01T14:41:38.004152Z",
     "iopub.status.idle": "2024-11-01T14:41:38.023624Z",
     "shell.execute_reply": "2024-11-01T14:41:38.022700Z",
     "shell.execute_reply.started": "2024-11-01T14:41:38.004541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}, device={param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:38.026709Z",
     "iopub.status.busy": "2024-11-01T14:41:38.024847Z",
     "iopub.status.idle": "2024-11-01T14:41:38.031655Z",
     "shell.execute_reply": "2024-11-01T14:41:38.030781Z",
     "shell.execute_reply.started": "2024-11-01T14:41:38.026665Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Training Arguments\n",
    "\n",
    "Training arguments are configured here, including batch size, number of training epochs, learning rate, and other key settings. This configuration is essential for controlling the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:38.033288Z",
     "iopub.status.busy": "2024-11-01T14:41:38.032943Z",
     "iopub.status.idle": "2024-11-01T14:41:38.067468Z",
     "shell.execute_reply": "2024-11-01T14:41:38.066609Z",
     "shell.execute_reply.started": "2024-11-01T14:41:38.033250Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/results/',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=32,  # Adjust to maintain effective batch size\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    gradient_checkpointing=False,\n",
    "    optim='adamw_bnb_8bit',\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:38.068950Z",
     "iopub.status.busy": "2024-11-01T14:41:38.068604Z",
     "iopub.status.idle": "2024-11-01T14:41:38.874235Z",
     "shell.execute_reply": "2024-11-01T14:41:38.873270Z",
     "shell.execute_reply.started": "2024-11-01T14:41:38.068912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=custom_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Management and Training Execution\n",
    "\n",
    "To manage memory effectively on the GPU, we clear the CUDA cache before starting the actual training. Then, the `trainer.train()` command is used to start the fine-tuning.\n",
    "\n",
    "The training progress, including the number of steps completed, current loss, and other details, is displayed to monitor the model's learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:38.875949Z",
     "iopub.status.busy": "2024-11-01T14:41:38.875586Z",
     "iopub.status.idle": "2024-11-01T14:41:38.978690Z",
     "shell.execute_reply": "2024-11-01T14:41:38.977686Z",
     "shell.execute_reply.started": "2024-11-01T14:41:38.875909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear cache and start training\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T14:41:38.985006Z",
     "iopub.status.busy": "2024-11-01T14:41:38.984309Z",
     "iopub.status.idle": "2024-11-01T18:43:11.279897Z",
     "shell.execute_reply": "2024-11-01T18:43:11.278984Z",
     "shell.execute_reply.started": "2024-11-01T14:41:38.984964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='434' max='434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [434/434 4:00:58, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=434, training_loss=1.9588829778855847, metrics={'train_runtime': 14491.896, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.03, 'total_flos': 8.664651391618253e+16, 'train_loss': 1.9588829778855847, 'epoch': 1.9962627569354607})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Trained Model\n",
    "\n",
    "Once training is complete, we save the fine-tuned model and tokenizer to a directory for future usage in generating Sanskrit text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:43:11.281354Z",
     "iopub.status.busy": "2024-11-01T18:43:11.281026Z",
     "iopub.status.idle": "2024-11-01T18:43:12.101688Z",
     "shell.execute_reply": "2024-11-01T18:43:12.100707Z",
     "shell.execute_reply.started": "2024-11-01T18:43:11.281321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer_config.json',\n",
       " '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/special_tokens_map.json',\n",
       " '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer.model',\n",
       " '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/added_tokens.json',\n",
       " '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('/kaggle/working/fine-tuned-gemma2-sanskrit-lora')\n",
    "tokenizer.save_pretrained('/kaggle/working/fine-tuned-gemma2-sanskrit-lora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading and Evaluation Setup\n",
    "\n",
    "In this section, we load the fine-tuned model into evaluation mode for generating Sanskrit text. The `PeftModel` allows efficient loading and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:43:12.103816Z",
     "iopub.status.busy": "2024-11-01T18:43:12.103042Z",
     "iopub.status.idle": "2024-11-01T18:43:19.131394Z",
     "shell.execute_reply": "2024-11-01T18:43:19.130487Z",
     "shell.execute_reply.started": "2024-11-01T18:43:12.103768Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e7f9eb711c4e95afc0a9ce0f1f5166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, '/kaggle/working/fine-tuned-gemma2-sanskrit-lora')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Function\n",
    "\n",
    "A function, `generate_text`, is defined for generating Sanskrit text based on a provided prompt. Parameters for temperature, top-p sampling, and maximum length control the creativity and diversity of the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:43:19.132950Z",
     "iopub.status.busy": "2024-11-01T18:43:19.132616Z",
     "iopub.status.idle": "2024-11-01T18:43:19.141065Z",
     "shell.execute_reply": "2024-11-01T18:43:19.140083Z",
     "shell.execute_reply.started": "2024-11-01T18:43:19.132913Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=512, num_return_sequences=1):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate output sequences\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,           # Use sampling for more diverse outputs\n",
    "            temperature=0.7,          # Adjust temperature for creativity\n",
    "            top_p=0.9,                # Use top-p sampling\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Prompt and Generated Text\n",
    "\n",
    "Using the `generate_text` function, we provide an example Sanskrit prompt and generate text based on it to showcase the model's capability to create coherent Sanskrit sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:45:02.693167Z",
     "iopub.status.busy": "2024-11-01T18:45:02.692776Z",
     "iopub.status.idle": "2024-11-01T18:45:20.063526Z",
     "shell.execute_reply": "2024-11-01T18:45:20.062582Z",
     "shell.execute_reply.started": "2024-11-01T18:45:02.693131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति । पाठस्य सम्पादकः अनिल मिश्राः। पाठस्य संपादनार्थः अन्येषु इच्छामि । पाठस्य सारः । इच्छामि तस्य स्वरूपस्य विस्तृतपरिचयं करो। विषयः आध्यात्मिकविज्ञानः इतिहासः अर्थशास्त्रः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः आध्यात्मिकविज्ञानः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः आध्यात्मिकविज्ञानः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः आध्यात्मिकविज्ञानः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः\n"
     ]
    }
   ],
   "source": [
    "prompt = \"पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति\"\n",
    "generated_texts = generate_text(prompt, max_length=200)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Prompt for Text Generation\n",
    "\n",
    "Further Sanskrit prompts are used to test the model's output diversity and quality, demonstrating the ability to generate text across different contexts in Sanskrit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:43:38.901765Z",
     "iopub.status.busy": "2024-11-01T18:43:38.901400Z",
     "iopub.status.idle": "2024-11-01T18:43:57.665460Z",
     "shell.execute_reply": "2024-11-01T18:43:57.664489Z",
     "shell.execute_reply.started": "2024-11-01T18:43:38.901730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "अनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति । प्राग्गुरुगणः अन्‍यः गौरववन्तः अस्ति । अन्‍यः सुप्रसिद्धः । अन्‍यः विद्यमानः अस्ति । अन्‍यः पुष्करसमुद्रम्‍न सह एव गौरववन्तः अस्ति । अन्‍यः क्रीडा सह विद्यमानः अस्ति । अन्‍यः क्रीडापक्षम्‍न सह एव गौरववन्तः अस्ति । अन्‍यः सुप्रसिद्धः । अन्‍यः पुष्करसमुद्रम्‍न सह एव गौरववन्तः अस्ति । अन्‍यः सुप्रसिद्धः । अन्‍यः\n"
     ]
    }
   ],
   "source": [
    "prompt = \"अनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति ।\"\n",
    "generated_texts = generate_text(prompt, max_length=200)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we have successfully fine-tuned the Gemma-2 model using LoRA for generating coherent text in Sanskrit. This approach showcases an efficient way to create and deploy language models in low-resource languages, aiding in language preservation and linguistic research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9690815,
     "sourceId": 85416,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72253,
     "sourceId": 85994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72254,
     "sourceId": 104623,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72253,
     "sourceId": 104625,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
