{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":85994,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72253,"modelId":76277},{"sourceId":104623,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277},{"sourceId":104625,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72253,"modelId":76277}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook Imports and Initial Setup\n\nIn the initial cells, essential libraries and dependencies are imported, setting up the environment for the fine-tuning process. We use a custom trainer, configure GPU settings, and set up necessary libraries like Hugging Face's `transformers` and `peft` (Parameter-Efficient Fine-Tuning).","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade peft transformers bitsandbytes datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T14:39:18.966334Z","iopub.execute_input":"2024-11-01T14:39:18.966794Z","iopub.status.idle":"2024-11-01T14:39:48.712223Z","shell.execute_reply.started":"2024-11-01T14:39:18.966754Z","shell.execute_reply":"2024-11-01T14:39:48.710995Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting datasets\n  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, transformers, datasets, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.0.1\n    Uninstalling datasets-3.0.1:\n      Successfully uninstalled datasets-3.0.1\nSuccessfully installed bitsandbytes-0.44.1 datasets-3.1.0 peft-0.13.2 transformers-4.46.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import get_peft_model, LoraConfig, TaskType","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:39:48.714652Z","iopub.execute_input":"2024-11-01T14:39:48.715416Z","iopub.status.idle":"2024-11-01T14:40:08.715237Z","shell.execute_reply.started":"2024-11-01T14:39:48.715368Z","shell.execute_reply":"2024-11-01T14:40:08.714490Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:08.716228Z","iopub.execute_input":"2024-11-01T14:40:08.716803Z","iopub.status.idle":"2024-11-01T14:40:08.721108Z","shell.execute_reply.started":"2024-11-01T14:40:08.716769Z","shell.execute_reply":"2024-11-01T14:40:08.720235Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Load Datasets**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the Sanskrit dataset from OSCAR\ndataset = load_dataset('oscar', 'unshuffled_deduplicated_sa', split='train')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:08.723153Z","iopub.execute_input":"2024-11-01T14:40:08.723455Z","iopub.status.idle":"2024-11-01T14:40:18.963147Z","shell.execute_reply.started":"2024-11-01T14:40:08.723412Z","shell.execute_reply":"2024-11-01T14:40:18.962395Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a32944448d9f41bb98cd1e3525994f4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e8d37fb2924c9fa09a2b83e2ca3f14"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0613909cc96c4f478d2ac1b351e40b4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50721c9e1c1a4e829a7bdc1073087f7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce94ae88ceb5414988596b8cd5fd4b09"}},"metadata":{}}]},{"cell_type":"code","source":"# Display a few samples\n\nfor i in range(3):\n    print(f\"Sample {i+1}:\\n{dataset[i]['text']}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:18.964254Z","iopub.execute_input":"2024-11-01T14:40:18.964592Z","iopub.status.idle":"2024-11-01T14:40:18.971886Z","shell.execute_reply.started":"2024-11-01T14:40:18.964557Z","shell.execute_reply":"2024-11-01T14:40:18.970973Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Sample 1:\nअनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति । तस्‍य कानिचन् चित्राणि पूर्वमेव प्रकाशितानि सन्ति । द्वौ चलचित्रौ अपि प्रकाशितौ । तस्मिन् एव क्रमेण एतत् सीतास्‍वयंबर इति चलचित्रं प्रकाश्यते ।\nलट् लकार: एकवचनम् द्विवचनम्बहुवचनम्प्रथमपुरुष:गच्‍छतिगच्‍छत:गच्‍छन्तिमध्‍यमपुरुष:गच्‍छसिगच्‍छथ:गच्‍छथउत्‍तमपुरुष:गच्‍छामिगच्‍छाव:गच्‍छाम:\n\nSample 2:\nपाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति; अन्याः संस्थित्यः अपि सन्ति । अधिकं ज्ञातुम् अत्र उपयोगस्य संस्थितिं पश्यतु ।\n\nSample 3:\nस्थिते च कवचे देहे नास्ति मृत्युश्च जीविनाम् । अस्त्रे-शस्त्रे-जले-वह्नौ सिद्धिश्चेन्नास्ति संशयः ॥ ४॥\nप्राच्यां मां पातु भूतेशः आग्नेय्यां पातु शङ्करः । दक्षिणे पातु मां रुद्रो नैॠत्यां स्थाणुरेव च ॥ ११॥\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Data Cleaning**","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_text(example):\n    text = example['text']\n    # Remove HTML tags\n    text = re.sub(r'<[^>]+>', '', text)\n    # Remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Remove non-Sanskrit characters (retain Devanagari script)\n    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n    return {'text': text}\n\n# Apply the cleaning function\ndataset = dataset.map(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:18.973213Z","iopub.execute_input":"2024-11-01T14:40:18.973633Z","iopub.status.idle":"2024-11-01T14:40:21.425375Z","shell.execute_reply.started":"2024-11-01T14:40:18.973583Z","shell.execute_reply":"2024-11-01T14:40:21.424479Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b889617c9740e4918e2b61024fa131"}},"metadata":{}}]},{"cell_type":"code","source":"def filter_short_texts(example):\n    return len(example['text'].split()) > 5  # Keep texts longer than 5 words\n\ndataset = dataset.filter(filter_short_texts)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:21.426822Z","iopub.execute_input":"2024-11-01T14:40:21.427480Z","iopub.status.idle":"2024-11-01T14:40:21.727740Z","shell.execute_reply.started":"2024-11-01T14:40:21.427416Z","shell.execute_reply":"2024-11-01T14:40:21.726812Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/7121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f386d93133493a979148c16ee2e53f"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Load Model**","metadata":{}},{"cell_type":"code","source":"model_name = '/kaggle/input/gemma-2/transformers/gemma-2-2b/1'","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:21.728980Z","iopub.execute_input":"2024-11-01T14:40:21.729281Z","iopub.status.idle":"2024-11-01T14:40:21.733418Z","shell.execute_reply.started":"2024-11-01T14:40:21.729249Z","shell.execute_reply":"2024-11-01T14:40:21.732584Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Use a multilingual tokenizer or train a new one\n# Load the tokenizer for Gemma 2 9B model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:21.734827Z","iopub.execute_input":"2024-11-01T14:40:21.735689Z","iopub.status.idle":"2024-11-01T14:40:22.989761Z","shell.execute_reply.started":"2024-11-01T14:40:21.735615Z","shell.execute_reply":"2024-11-01T14:40:22.988746Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Check whether Cuda is available**","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:22.993238Z","iopub.execute_input":"2024-11-01T14:40:22.993558Z","iopub.status.idle":"2024-11-01T14:40:23.026378Z","shell.execute_reply.started":"2024-11-01T14:40:22.993519Z","shell.execute_reply":"2024-11-01T14:40:23.025460Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**LORA Configurations**","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\n#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map='auto'\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:40:23.027485Z","iopub.execute_input":"2024-11-01T14:40:23.027824Z","iopub.status.idle":"2024-11-01T14:41:27.214310Z","shell.execute_reply.started":"2024-11-01T14:40:23.027788Z","shell.execute_reply":"2024-11-01T14:41:27.213496Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"106f38870f7a4a4e8b08126d5adaeb44"}},"metadata":{}}]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type=TaskType.CAUSAL_LM,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:27.215633Z","iopub.execute_input":"2024-11-01T14:41:27.216029Z","iopub.status.idle":"2024-11-01T14:41:27.221357Z","shell.execute_reply.started":"2024-11-01T14:41:27.215986Z","shell.execute_reply":"2024-11-01T14:41:27.220488Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:27.222611Z","iopub.execute_input":"2024-11-01T14:41:27.222936Z","iopub.status.idle":"2024-11-01T14:41:27.472731Z","shell.execute_reply.started":"2024-11-01T14:41:27.222904Z","shell.execute_reply":"2024-11-01T14:41:27.471759Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if 'lora' in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:27.473934Z","iopub.execute_input":"2024-11-01T14:41:27.474239Z","iopub.status.idle":"2024-11-01T14:41:27.484048Z","shell.execute_reply.started":"2024-11-01T14:41:27.474208Z","shell.execute_reply":"2024-11-01T14:41:27.483166Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Number of trainable parameters**","metadata":{}},{"cell_type":"code","source":"# Verify trainable parameters\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_params = 0\n    for _, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"Trainable params: {trainable_params} | All params: {all_params} | \"\n        f\"Trainable percentage: {100 * trainable_params / all_params:.2f}%\"\n    )\n\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:27.485178Z","iopub.execute_input":"2024-11-01T14:41:27.485550Z","iopub.status.idle":"2024-11-01T14:41:27.500097Z","shell.execute_reply.started":"2024-11-01T14:41:27.485517Z","shell.execute_reply":"2024-11-01T14:41:27.499225Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Trainable params: 6389760 | All params: 1608593664 | Trainable percentage: 0.40%\n","output_type":"stream"}]},{"cell_type":"code","source":"#model.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:27.503250Z","iopub.execute_input":"2024-11-01T14:41:27.503596Z","iopub.status.idle":"2024-11-01T14:41:27.509759Z","shell.execute_reply.started":"2024-11-01T14:41:27.503552Z","shell.execute_reply":"2024-11-01T14:41:27.508920Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Prepare Tokenized Dataset**","metadata":{}},{"cell_type":"code","source":"# Prepare dataset\ndef tokenize_function(examples):\n    tokens = tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n    )\n    tokens['labels'] = tokens['input_ids'].copy()\n    return tokens\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:27.511203Z","iopub.execute_input":"2024-11-01T14:41:27.511525Z","iopub.status.idle":"2024-11-01T14:41:35.458817Z","shell.execute_reply.started":"2024-11-01T14:41:27.511491Z","shell.execute_reply":"2024-11-01T14:41:35.457881Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec74bf3dca745c9b8b0d33500539457"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Customized Data Collator**","metadata":{}},{"cell_type":"code","source":"# Custom data collator\ndef custom_data_collator(features):\n    batch = {}\n    for key in features[0].keys():\n        batch[key] = torch.stack([torch.tensor(f[key]) for f in features]).to('cuda')\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:35.460179Z","iopub.execute_input":"2024-11-01T14:41:35.460810Z","iopub.status.idle":"2024-11-01T14:41:35.466241Z","shell.execute_reply.started":"2024-11-01T14:41:35.460762Z","shell.execute_reply":"2024-11-01T14:41:35.465351Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Custom Trainer Initialization\n\nA `CustomTrainer` instance is initialized with the model, training arguments, and dataset. This custom trainer coordinates the training process, using the arguments and model components configured earlier.","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def prepare_inputs(self, inputs):\n        # Ensure inputs are on GPU\n        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:35.467580Z","iopub.execute_input":"2024-11-01T14:41:35.467933Z","iopub.status.idle":"2024-11-01T14:41:35.515447Z","shell.execute_reply.started":"2024-11-01T14:41:35.467892Z","shell.execute_reply":"2024-11-01T14:41:35.514561Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Get a batch of data\nbatch = tokenized_dataset[:2]  # Take the first 2 samples\n\n# Convert batch to tensors and move to GPU\ninputs = {\n    'input_ids': torch.tensor(batch['input_ids']).to('cuda'),\n    'attention_mask': torch.tensor(batch['attention_mask']).to('cuda'),\n    'labels': torch.tensor(batch['labels']).to('cuda'),\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:35.516602Z","iopub.execute_input":"2024-11-01T14:41:35.516951Z","iopub.status.idle":"2024-11-01T14:41:35.530397Z","shell.execute_reply.started":"2024-11-01T14:41:35.516911Z","shell.execute_reply":"2024-11-01T14:41:35.529494Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Set model to evaluation mode**","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    loss = outputs.loss\n    print(f\"Loss: {loss.item()}\")\n    print(f\"Loss requires grad: {loss.requires_grad}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:35.531462Z","iopub.execute_input":"2024-11-01T14:41:35.531778Z","iopub.status.idle":"2024-11-01T14:41:37.032937Z","shell.execute_reply.started":"2024-11-01T14:41:35.531747Z","shell.execute_reply":"2024-11-01T14:41:37.031968Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Loss: 19.825746536254883\nLoss requires grad: False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Set model to training mode**","metadata":{}},{"cell_type":"code","source":"model.train()  \n\noutputs = model(**inputs)\nloss = outputs.loss\n\nprint(f\"Loss: {loss.item()}\")\nprint(f\"Loss requires grad: {loss.requires_grad}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:37.034189Z","iopub.execute_input":"2024-11-01T14:41:37.034613Z","iopub.status.idle":"2024-11-01T14:41:38.002709Z","shell.execute_reply.started":"2024-11-01T14:41:37.034568Z","shell.execute_reply":"2024-11-01T14:41:38.001622Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Loss: 19.825746536254883\nLoss requires grad: True\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"{name}: requires_grad={param.requires_grad}, device={param.device}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-01T14:41:38.004152Z","iopub.execute_input":"2024-11-01T14:41:38.004584Z","iopub.status.idle":"2024-11-01T14:41:38.023624Z","shell.execute_reply.started":"2024-11-01T14:41:38.004541Z","shell.execute_reply":"2024-11-01T14:41:38.022700Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: requires_grad=True, device=cuda:0\nbase_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: requires_grad=True, device=cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:38.024847Z","iopub.execute_input":"2024-11-01T14:41:38.026709Z","iopub.status.idle":"2024-11-01T14:41:38.031655Z","shell.execute_reply.started":"2024-11-01T14:41:38.026665Z","shell.execute_reply":"2024-11-01T14:41:38.030781Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Setting Training Arguments\n\nTraining arguments are configured here, including batch size, number of training epochs, learning rate, and other key settings. This configuration is essential for controlling the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='/kaggle/working/results/',\n    num_train_epochs=2,\n    per_device_train_batch_size=1,  # Adjust based on GPU memory\n    gradient_accumulation_steps=32,  # Adjust to maintain effective batch size\n    learning_rate=1e-4,\n    fp16=True,\n    save_total_limit=2,\n    save_steps=500,\n    gradient_checkpointing=False,\n    optim='adamw_bnb_8bit',\n    dataloader_pin_memory=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:38.032943Z","iopub.execute_input":"2024-11-01T14:41:38.033288Z","iopub.status.idle":"2024-11-01T14:41:38.067468Z","shell.execute_reply.started":"2024-11-01T14:41:38.033250Z","shell.execute_reply":"2024-11-01T14:41:38.066609Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=custom_data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:38.068604Z","iopub.execute_input":"2024-11-01T14:41:38.068950Z","iopub.status.idle":"2024-11-01T14:41:38.874235Z","shell.execute_reply.started":"2024-11-01T14:41:38.068912Z","shell.execute_reply":"2024-11-01T14:41:38.873270Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Memory Management and Training Execution\n\nTo manage memory effectively on the GPU, we clear the CUDA cache before starting the actual training. Then, the `trainer.train()` command is used to start the fine-tuning.\n\nThe training progress, including the number of steps completed, current loss, and other details, is displayed to monitor the model's learning curve.","metadata":{}},{"cell_type":"code","source":"# Clear cache and start training\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:38.875586Z","iopub.execute_input":"2024-11-01T14:41:38.875949Z","iopub.status.idle":"2024-11-01T14:41:38.978690Z","shell.execute_reply.started":"2024-11-01T14:41:38.875909Z","shell.execute_reply":"2024-11-01T14:41:38.977686Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:41:38.984309Z","iopub.execute_input":"2024-11-01T14:41:38.985006Z","iopub.status.idle":"2024-11-01T18:43:11.279897Z","shell.execute_reply.started":"2024-11-01T14:41:38.984964Z","shell.execute_reply":"2024-11-01T18:43:11.278984Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='434' max='434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [434/434 4:00:58, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=434, training_loss=1.9588829778855847, metrics={'train_runtime': 14491.896, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.03, 'total_flos': 8.664651391618253e+16, 'train_loss': 1.9588829778855847, 'epoch': 1.9962627569354607})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Saving the Trained Model\n\nOnce training is complete, we save the fine-tuned model and tokenizer to a directory for future usage in generating Sanskrit text.\n","metadata":{}},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/fine-tuned-gemma2-sanskrit-lora')\ntokenizer.save_pretrained('/kaggle/working/fine-tuned-gemma2-sanskrit-lora')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:43:11.281026Z","iopub.execute_input":"2024-11-01T18:43:11.281354Z","iopub.status.idle":"2024-11-01T18:43:12.101688Z","shell.execute_reply.started":"2024-11-01T18:43:11.281321Z","shell.execute_reply":"2024-11-01T18:43:12.100707Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer_config.json',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/special_tokens_map.json',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer.model',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/added_tokens.json',\n '/kaggle/working/fine-tuned-gemma2-sanskrit-lora/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Loading and Evaluation Setup\n\nIn this section, we load the fine-tuned model into evaluation mode for generating Sanskrit text. The `PeftModel` allows efficient loading and evaluation.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map='auto'\n)\n\nmodel = PeftModel.from_pretrained(model, '/kaggle/working/fine-tuned-gemma2-sanskrit-lora')\n\n# Set the model to evaluation mode\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:43:12.103042Z","iopub.execute_input":"2024-11-01T18:43:12.103816Z","iopub.status.idle":"2024-11-01T18:43:19.131394Z","shell.execute_reply.started":"2024-11-01T18:43:12.103768Z","shell.execute_reply":"2024-11-01T18:43:19.130487Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e7f9eb711c4e95afc0a9ce0f1f5166"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Text Generation Function\n\nA function, `generate_text`, is defined for generating Sanskrit text based on a provided prompt. Parameters for temperature, top-p sampling, and maximum length control the creativity and diversity of the generated text.\n","metadata":{}},{"cell_type":"code","source":"def generate_text(prompt, max_length=512, num_return_sequences=1):\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    \n    # Generate output sequences\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=num_return_sequences,\n            do_sample=True,           # Use sampling for more diverse outputs\n            temperature=0.7,          # Adjust temperature for creativity\n            top_p=0.9,                # Use top-p sampling\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    # Decode the generated tokens\n    generated_texts = []\n    for output in outputs:\n        text = tokenizer.decode(output, skip_special_tokens=True)\n        generated_texts.append(text)\n    \n    return generated_texts","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:43:19.132616Z","iopub.execute_input":"2024-11-01T18:43:19.132950Z","iopub.status.idle":"2024-11-01T18:43:19.141065Z","shell.execute_reply.started":"2024-11-01T18:43:19.132913Z","shell.execute_reply":"2024-11-01T18:43:19.140083Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Example Prompt and Generated Text\n\nUsing the `generate_text` function, we provide an example Sanskrit prompt and generate text based on it to showcase the model's capability to create coherent Sanskrit sentences.","metadata":{}},{"cell_type":"code","source":"prompt = \"पाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति\"\ngenerated_texts = generate_text(prompt, max_length=200)\nprint(\"Generated Text:\")\nprint(generated_texts[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:45:02.692776Z","iopub.execute_input":"2024-11-01T18:45:02.693167Z","iopub.status.idle":"2024-11-01T18:45:20.063526Z","shell.execute_reply.started":"2024-11-01T18:45:02.693131Z","shell.execute_reply":"2024-11-01T18:45:20.062582Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Generated Text:\nपाठः क्रियेटिव कॉमन्स ऐट्रिब्यूशन/शेयर-अलाइक अभिज्ञापत्रस्य अन्तर्गततया उपलब्धः अस्ति । पाठस्य सम्पादकः अनिल मिश्राः। पाठस्य संपादनार्थः अन्येषु इच्छामि । पाठस्य सारः । इच्छामि तस्य स्वरूपस्य विस्तृतपरिचयं करो। विषयः आध्यात्मिकविज्ञानः इतिहासः अर्थशास्त्रः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः आध्यात्मिकविज्ञानः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः आध्यात्मिकविज्ञानः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः आध्यात्मिकविज्ञानः विज्ञानशास्त्रः अर्थशास्त्रः इतिहासः\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Additional Prompt for Text Generation\n\nFurther Sanskrit prompts are used to test the model's output diversity and quality, demonstrating the ability to generate text across different contexts in Sanskrit.","metadata":{}},{"cell_type":"code","source":"prompt = \"अनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति ।\"\ngenerated_texts = generate_text(prompt, max_length=200)\n\nprint(\"Generated Text:\")\nprint(generated_texts[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:43:38.901400Z","iopub.execute_input":"2024-11-01T18:43:38.901765Z","iopub.status.idle":"2024-11-01T18:43:57.665460Z","shell.execute_reply.started":"2024-11-01T18:43:38.901730Z","shell.execute_reply":"2024-11-01T18:43:57.664489Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Generated Text:\nअनिरुद्धनगरे क्रीडिता रामलीला सम्‍प्रति समाप्‍ता अस्ति । प्राग्गुरुगणः अन्‍यः गौरववन्तः अस्ति । अन्‍यः सुप्रसिद्धः । अन्‍यः विद्यमानः अस्ति । अन्‍यः पुष्करसमुद्रम्‍न सह एव गौरववन्तः अस्ति । अन्‍यः क्रीडा सह विद्यमानः अस्ति । अन्‍यः क्रीडापक्षम्‍न सह एव गौरववन्तः अस्ति । अन्‍यः सुप्रसिद्धः । अन्‍यः पुष्करसमुद्रम्‍न सह एव गौरववन्तः अस्ति । अन्‍यः सुप्रसिद्धः । अन्‍यः\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, we have successfully fine-tuned the Gemma-2 model using LoRA for generating coherent text in Sanskrit. This approach showcases an efficient way to create and deploy language models in low-resource languages, aiding in language preservation and linguistic research.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}