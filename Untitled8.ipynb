{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_images(x):\n",
    "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
    "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
    "    grid = torchvision.utils.make_grid(x)\n",
    "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
    "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
    "    return grid_im\n",
    "\n",
    "\n",
    "def make_grid(images, size=64):\n",
    "    \"\"\"Given a list of PIL images, stack them together into a line for easy viewing\"\"\"\n",
    "    output_im = Image.new(\"RGB\", (size * len(images), size))\n",
    "    for i, im in enumerate(images):\n",
    "        output_im.paste(im.resize((size, size)), (i * size, 0))\n",
    "    return output_im\n",
    "\n",
    "\n",
    "# Mac users may need device = 'mps' (untested)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llTHTURvsH1z"
   },
   "source": [
    "# create images using pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY6FWaTOmLJd"
   },
   "source": [
    "Pipelines are great for end-users, but if you're here for this course we assume you want to know what is going on under the hood! So, over the rest of this notebook we're going to build our own pipeline capable of generating small butterfly pictures. Here's the final result in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "# Load the butterfly pipeline\n",
    "butterfly_pipeline = DDPMPipeline.from_pretrained(\n",
    "    \"johnowhitaker/ddpm-butterflies-32px\"\n",
    ").to(device)\n",
    "\n",
    "# Create 8 images\n",
    "images = butterfly_pipeline(batch_size=8).images\n",
    "\n",
    "# View the result\n",
    "make_grid(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlCzBVDjsNqZ"
   },
   "source": [
    "# Training diffusion model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiVi_sS3mdh9"
   },
   "source": [
    "Not as impressive as the DreamBooth example perhaps, but then we're training from scratch with ~0.0001% of the data used to train Stable Diffusion. Speaking of training, recall from the introduction to this unit that training a diffusion model looks something like this:\n",
    "\n",
    "Load in some images from the training data\n",
    "Add noise, in different amounts.\n",
    "Feed the noisy versions of the inputs into the model\n",
    "Evaluate how well the model does at denoising these inputs\n",
    "Use this information to update the model weights, and repeat\n",
    "We'll explore these steps one by one in the next few sections until we have a complete training loop working, and then we'll explore how to sample from the trained model and how to package everything up into a pipeline for easy sharing. Let's begin with the data...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "# Or load images from a local folder\n",
    "# dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/folder\")\n",
    "\n",
    "# We'll train on 16-pixel square images, but you can try larger sizes too\n",
    "image_size = 32\n",
    "# You can lower your batch size if you're running out of GPU memory\n",
    "batch_size = 64\n",
    "\n",
    "# Define data augmentations\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDa7my-vnAzc"
   },
   "source": [
    "view some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = next(iter(train_dataloader))[\"images\"].to(device)[:8]\n",
    "print(\"X shape:\", xb.shape)\n",
    "show_images(xb).resize((8 * 64, 64), resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtJvSjoNnDUY"
   },
   "source": [
    "noise schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q_6R4RZnRG5"
   },
   "source": [
    "The DDPM paper describes a corruption process that adds a small amount of noise for every 'timestep'. Given $x_{t-1}$ for some timestep, we can get the next (slightly more noisy) version $x_t$ with:<br><br>\n",
    "\n",
    "$q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})$<br><br>\n",
    "\n",
    "\n",
    "That is, we take $x_{t-1}$, scale it by $\\sqrt{1 - \\beta_t}$ and add noise scaled by $\\beta_t$. This $\\beta$ is defined for every t according to some schedule, and determines how much noise is added per timestep. Now, we don't necessarily want to do this operation 500 times to get $x_{500}$ so we have another formula to get $x_t$ for any t given $x_0$: <br><br>\n",
    "\n",
    "$\\begin{aligned}\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, {(1 - \\bar{\\alpha}_t)} \\mathbf{I})\n",
    "\\end{aligned}$ where $\\bar{\\alpha}_t = \\prod_{i=1}^T \\alpha_i$ and $\\alpha_i = 1-\\beta_i$<br><br>\n",
    "\n",
    "The maths notation always looks scary! Luckily the scheduler handles all that for us. We can plot $\\sqrt{\\bar{\\alpha}_t}$ (labelled as `sqrt_alpha_prod`) and $\\sqrt{(1 - \\bar{\\alpha}_t)}$ (labelled as `sqrt_one_minus_alpha_prod`) to view how the input (x) and the noise are scaled and mixed across different timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(noise_scheduler.alphas_cumprod.cpu() ** 0.5, label=r\"${\\sqrt{\\bar{\\alpha}_t}}$\")\n",
    "plt.plot((1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5, label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$\")\n",
    "plt.legend(fontsize=\"x-large\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "131KNjWvne14"
   },
   "source": [
    "Exercise: You can explore how this plot changes with different settings for beta_start, beta_end and beta_schedule by swapping in one of the commented-out options here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One with too little noise added:\n",
    "# noise_scheduler = DDPMScheduler(num_train_timesteps=100, beta_start=0.001, beta_end=0.004)\n",
    "# The 'cosine' schedule, which may be better for small image sizes:\n",
    "# noise_scheduler = DDPMScheduler(num_train_timesteps=100, beta_schedule='squaredcos_cap_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu3e54ehno5p"
   },
   "source": [
    "Whichever scheduler you've chosen, we can now use it to add noise in different amounts using the noise_scheduler.add_noise function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = torch.linspace(0, 999, 8).long().to(device)\n",
    "noise = torch.randn_like(xb)\n",
    "noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)\n",
    "print(\"Noisy X shape\", noisy_xb.shape)\n",
    "show_images(noisy_xb).resize((8 * 64, 64), resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S64RDq1lnx_N"
   },
   "source": [
    "Define the Model\n",
    "\n",
    "Now we come to the core component: the model itself.\n",
    "\n",
    "Most diffusion models use architectures that are some variant of a [U-net](https://arxiv.org/abs/1505.04597) and that's what we'll use here.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)\n",
    "\n",
    "In a nutshell:\n",
    "- the model has the input image go through several blocks of ResNet layers, each of which halves the image size by 2\n",
    "- then through the same number of blocks that upsample it again.\n",
    "- there are skip connections linking the features on the downsample path to the corresponding layers in the upsample path.\n",
    "\n",
    "A key feature of this model is that it predicts images of the same size as the input, which is exactly what we need here.\n",
    "\n",
    "Diffusers provides us a handy `UNet2DModel` class which creates the desired architecture in PyTorch.\n",
    "\n",
    "Let's create a U-net for our desired image size.\n",
    "Note that `down_block_types` correspond to the downsampling blocks (green on the diagram above), and `up_block_types` are the upsampling blocks (red on the diagram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "# Create a model\n",
    "model = UNet2DModel(\n",
    "    sample_size=image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0rb8YR2oJHO"
   },
   "source": [
    "We can check that passing in a batch of data and some random timesteps produces an output the same shape as the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_prediction = model(noisy_xb, timesteps).sample\n",
    "model_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TT1okf_okWX"
   },
   "source": [
    "Time to train! Below is a typical optimization loop in PyTorch, where we run through the data batch by batch and update the parameters of our model each step using an optimizer - in this case the AdamW optimizer with a learning rate of 0.0004.\n",
    "\n",
    "For each batch of data, we\n",
    "- Sample some random timesteps\n",
    "- Noise the data accordingly\n",
    "- Feed the noisy data through the model\n",
    "- Compare the model predictions with the target (i.e. the noise in this case) using mean squared error as our loss function\n",
    "- Update the model parameters via `loss.backward()` and `optimizer.step()`\n",
    "\n",
    "During this process we also log the losses over time for later plotting.\n",
    "\n",
    "NB: This code takes nearly 10 minutes to run - feel free to skip these two cells and use the pretrained model if you are in a hurry. Alternatively, you can explore how reducing the number of channels in each layer via the model definition above can speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the noise scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\"\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update the model parameters with the optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
    "        print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCFO1W9kpRUj"
   },
   "source": [
    "# How do we get images with trained model? either by creating a pipeline or by writing a sampling loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH7Yy2DlpZhg"
   },
   "source": [
    "### Option 1: Creating a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_output = image_pipe(num_inference_steps=1000)\n",
    "pipeline_output.images[0].resize((1 * 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSNlfioNp2jj"
   },
   "source": [
    "### Option 2: Writing a Sampling Loop\n",
    "\n",
    "We begin with random noise, and run through the scheduler timesteps from most to least noisy, removing a small amount of noise each step based on the model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random starting point (8 random images):\n",
    "sample = torch.randn(8, 3, 32, 32).to(device)\n",
    "\n",
    "for i, t in enumerate(noise_scheduler.timesteps):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = model(sample, t).sample\n",
    "\n",
    "    # Update sample with step\n",
    "    sample = noise_scheduler.step(residual, t, sample).prev_sample\n",
    "\n",
    "show_images(sample).resize((8 * 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7CnHQfPqNRr"
   },
   "source": [
    "The `noise_scheduler.step()` function does the maths required to update `sample` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --clear-output --inplace Untitled8.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
